---
title: "Data Analysis for Business - Midterm Project"
author: Edoardo Cocciò, Alexandra Tabarani, Marta Torella, Simone Filosofi
output:
  html_document: default
---

# Standard Deviated Group {.tabset .tabset-fade}

Group members:

-   Edoardo Cocciò - 282401
-   Alexandra Tabarani - 282091
-   Marta Torella - 284091
-   Simone Filosofi - 284531

## Task 1 {.tabset .tabset-fade}

The dataset we are going to analyze is the "Telecom Churn" dataset, which contains information about US Telecom. For this first task, our objective is to predict customer churn, enabling the company to implement preventative measures to retain clients; as a matter of fact, by identifying customers at high risk of churn, the company can proactively offer targeted promotions to improve retention rates.

```{r message=FALSE, warning=FALSE, include=FALSE}
library(ggplot2)
library(corrplot)
library(mapproj)
library(dplyr)
library(tidyr)
library(skimr)
library(treemapify)
library(stringr)
library(glue)
library(dummy)
library(caret)
library(car)
library(pROC)
library(glmnet)
library(rpart)
library(rpart.plot) 
library(randomForest)
library(pROC)
library(cluster)
library(factoextra)
library(sf)
library(latex2exp)
library(gridExtra)
library(tree)
library(randomForest)
library(xgboost)
library(dendextend)
library(purrr)
library(ROSE)

data <- read.csv("TelecomChurn.csv")
```

### EDA

We start by performing an exploratory data analysis (EDA) to understand the structure of the dataset and the relationships between the variables. Starting with a general overview of the dataset, we can see that it contains 3333 observations and 20 variables. The variables are a mix of numerical and categorical variable; we proceed by factoring the categorical variables in order to better deal with them later.

```{r echo=FALSE}
names(data)

data$State = as.factor(data$State)
data$International.plan = as.factor(data$International.plan)
data$Voice.mail.plan = as.factor(data$Voice.mail.plan)
data$Churn = as.factor(data$Churn)

continuous_vars <- data[, sapply(data, is.numeric)]
categorical_vars <- data[, sapply(data, is.factor) & !names(data) %in% "Churn"]
```

#### Missing Values and Duplicates

A crucial step in the data analysis process is to check for missing values and duplicates in the dataset to ensure that the data is clean and ready for analysis; in our case we have no missing values and no duplicates. After ensuring dataset integrity, we proceed with the analysis.

```{r echo=FALSE, results='hide'}
sum(is.na(data))
anyDuplicated(data)
```

#### Univariate Analysis 

We proceed by conducting an univariate analysis of the variables in our dataset: our first step is to check the distribution of the target variable 'Churn'; indeed, we identified the variable 'Churn' as the target of our analysis since, as previously stated, the goal of our first task is to build a model to predict customer churn. 
As shown by the below pie chart,the distribution of the target variable is unbalanced, with 85.5% of the customers not churning and 14.5% churning.

```{r echo=FALSE, fig.align='center', out.width="50%"}
churn_percent <- data %>%
  group_by(Churn) %>%
  summarise(Count = n(), .groups = "drop") %>%
  mutate(Percent = Count / sum(Count) * 100)


ggplot(churn_percent, aes(x = "", y = Percent, fill = Churn)) +
  geom_col(width = 1, color = 1) +
  coord_polar("y", start = 0) +
  theme_void() +
  scale_fill_manual(values = c("False" = "#dab0ff", "True" = "#ADD8E6"), labels = c("False" = "No", "True" = "Yes")) +
  labs(title = "Percentage of Churn", fill = "Churn") +
  geom_text(aes(label = paste0(round(Percent, 1), "%")), position = position_stack(vjust = 0.5)) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 18))

```

We now proceed with the univariate analysis of the numerical variables in the dataset. We will plot the distribution of the numerical variables by the target variable 'Churn' to see if there are any differences in the distribution of the numerical variables between the two levels of the target variable.

```{r, echo=FALSE, fig.align='center', fig.width=10, fig.height=10}
continuous <- data[, sapply(data, is.numeric) | names(data) == "Churn"]

continuous %>%
  pivot_longer(cols = -Churn, names_to = "metric", values_to = "value") %>%
  ggplot(aes(value, color = as.factor(Churn))) +
  geom_density(alpha = 0.3) +
  facet_wrap(vars(metric), scales = "free") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  scale_color_manual(name = "Churn", labels = c("No", "Yes"), values = c("#dab0ff", "#ADD8E6")) +
  labs(title = "Numeric Features", subtitle = "Univariate Distribution by Churn") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 18, colour = "gray5"),
    plot.subtitle = element_text(hjust = 0.5, colour = "gray5"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    legend.position = "top"
  )
```

The above univariate analysis of the numerical variables shows that the variables distribution differs remarkably for the two levels of the target variable "Churn" in a couple of cases. Indeed, there's a noticeable difference in the distribution of **customer service calls**: customers who churned seem to make more customer service calls, which could indicate a relationship between customer satisfaction and churn. It also appears that customers who churned tend to have slightly higher **total day minutes** and **total day charges**; this might suggest that customers with higher usage during the day are more likely to churn. The other variables do not show a significant difference in distribution between the two levels of the target variable.

Another remarkable imbalance can be found in the distribution of both **International.plan** and **Voice.mail.plan**. The majority of customers do not have an international plan nor a voice mail plan; this might suggest that these services are not very popular among customers.

```{r echo=FALSE, fig.show ="hold", out.width="50%"}
long_data <- categorical_vars %>%
  pivot_longer(cols = c("International.plan", "Voice.mail.plan"), names_to = "Category", values_to = "Value")

ggplot(long_data, aes(x = Value, fill = Value)) +
  geom_bar(color = "black") +
  facet_wrap(~ Category, scales = "free_x", nrow = 2, ncol = 2) +
  labs(title = "Plan Distribution", x = NULL, y = "Count") +
  theme_minimal() +
  scale_fill_manual(values = c("No" = "#ADD8E6", "Yes" = "#dab0ff"), labels = c("False" = "No", "True" = "Yes")) +
  theme(
    strip.text = element_text(size = 12),
    axis.text.x = element_text(angle = 90, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    legend.position = "none"
  ) 

```

To conclude the analysis of the categorical variables, we also analysed the distribution of the **State** variable; the below plot shows the distribution of customers across different states in the US. 

```{r message=FALSE, warning=FALSE}
us_map <- map_data("state") %>% 
  as_tibble()

us_map %>%
  ggplot(aes(long, lat, map_id = region)) + 
  geom_map(
    map =us_map,
    color = "gray80",
    fill = "gray30",
    size = 0.3
  ) + 
  coord_map("ortho", orientation = c(39, -98, 0)) + 
  labs(
    title = "US Map",
    subtitle = "Based on Latitude and Longitude"
  ) + theme(plot.title = element_text(hjust=0.5))

state_names <- c(KS = "kansas", OH = "ohio", NJ = "new jersey", OK = "oklahoma",
                 AL = "alabama", MA = "massachusetts", MO = "missouri", LA = "louisiana",
                 WV = "west virginia", IN = "indiana", RI = "rhode island", IA = "iowa",
                 MT = "montana", NY = "new york", ID = "idaho", VT = "vermont",
                 VA = "virginia", TX = "texas", FL = "florida", CO = "colorado",
                 AZ = "arizona", SC = "south carolina", NE = "nebraska", WY = "wyoming",
                 HI = "hawaii", IL = "illinois", NH = "new hampshire", GA = "georgia",
                 AK = "alaska", MD = "maryland", AR = "arkansas", WI = "wisconsin",
                 OR = "oregon", MI = "michigan", DE = "delaware", UT = "utah",
                 CA = "california", MN = "minnesota", SD = "south dakota", NC = "north carolina",
                 WA = "washington", NM = "new mexico", NV = "nevada", DC = "district of columbia",
                 KY = "kentucky", ME = "maine", MS = "mississippi", TN = "tennessee",
                 PA = "pennsylvania", CT = "connecticut", ND = "north dakota")

churn.tbl <- data %>% select(State, Churn) %>%
  group_by(State) %>%
  summarize(
    total_customers = n(),
    churned_customers = sum(Churn == "True"),
    churn_rate = (churned_customers / total_customers),
    churn_rate_txt = scales::percent(churn_rate)
  ) %>%
  mutate(full_state = state_names[as.character(State)]) %>%
  ungroup() %>% 
  left_join(us_map, by=c("full_state" = "region"))

us_map_sf <- st_as_sf(us_map, coords = c("long", "lat"), crs = 4326, agr = "constant")

centroids <- us_map_sf %>%
  group_by(region) %>%
  summarise(geometry = st_centroid(st_union(geometry)))

centroids_df <- as.data.frame(st_coordinates(centroids))
centroids_df$region <- centroids$region

churn.tbl <- churn.tbl %>%
  left_join(centroids_df, by = c("full_state" = "region"))

churn.tbl %>%
  ggplot(aes(long, lat)) + 
  geom_map(
    aes(map_id = full_state),
    map = us_map,
    color = "gray86",
    fill = "gray30",
    size = 0.3
  ) + coord_map("ortho", orientation = c(39, -98, 0)) + geom_polygon(aes(group = group, fill = churn_rate), color="black") +
  scale_fill_gradient2("",low = "#18BC9C", mid = "white", high = "#E31A1C", midpoint = 0.10, labels = scales::percent) + 
  geom_text(aes(x = X, y = Y, label = State), stat = "unique", size = 3, inherit.aes = FALSE, fontface = "bold") +
  theme_void() + 
  theme(
    plot.title = element_text(size=18, face="bold", color = "#2C3E50"),
    legend.position = "right"
  ) + 
  labs(
    title = "Churn Rate Across US States",
    x = "",
    y = ""
  ) + 
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
  ) 
```

#### Hypothesis testing

#### Correlation analysis

```{r, echo=FALSE, fig.align='center'}
cor_mat <-
  data %>% 
  select(where(is.numeric)) %>% 
  cor()

corrplot(
  main = "\n\nCorrelation Matrix",
  cor_mat,
  method = "color",
  order = "alphabet",
  type = "lower",
  diag = FALSE,
  number.cex = 0.6,
  tl.cex = 0.6,
  tl.srt = 45,
  cl.pos = "b",
  addgrid.col = "white",
  addCoef.col = "white",
  col = COL1("Purples"),
  bg="gray",
  tl.col = "grey50"
)
```

The correlation matrix shows that there is a **perfect correlation between the charge variables and minutes variables** so we can drop all the minutes one **we drop Total.day.minutes, Total.eve.minutes, Total.night.minutes, Total.intl.minutes**.


### Lower-dimensional models

Before proceeding with the full model we need to focus on some lower dimensional model in order to investigate some interesting relationships between the variables. We only include customers who have made a service call:

```{r}
service_calls <- data %>%
  filter(Customer.service.calls > 0)
```
This is the proportion of customers who churned after making a service call:
```{r, echo=FALSE, fig.align='center'}
churn_percent <- service_calls %>%
  group_by(Customer.service.calls, Churn) %>%
  summarise(Count = n(), .groups = "drop") %>%
  mutate(Percent = Count / sum(Count) * 100)

ggplot(churn_percent, aes(x = Customer.service.calls, y = Percent, fill = Churn)) +
  geom_col(width = 0.7, color = 1) +
  labs(title = "Churn by Customer Service Calls", x = "Customer Service Calls", y = "Percent", fill = "Churn") +
  theme_minimal() +
  scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "bottom"
  )
```

We can see that the churn rate increases as the number of customer service calls increases. This suggests that customers who make more service calls are more likely to churn.




(decidere se gli vogliamo mettere tutti o meno)




**N.B.** We computed more analysis, using plots and hypothesis testing, to understand the relationship between the variables and the target variable "Churn", but for limited space restrictions we will not show them here. In the above paragraph ('Conclusions up to now') we will report the main conclusions arising from this analysis.

**CONCLUSION UP TO NOW:** 

- we dropped the *minutes variables* because they are *perfectly correlated with the charge variables*; 

- we saw that the *churn rate* is *higher* for customers that have an *international plan*; 
- we saw that the churn rate is *higher* for customers that do *not have a voice mail plan*; 

- we saw that the churn rate is *higher* for customers that *have a higher number of customer service calls*; 

- we saw that the churn rate is *higher* for customers that *have higher total day minutes and total day charge*; 

- we saw that the churn rate is *higher* for customers that *have higher total eve charge, night charge, and intl charge*; 

- we saw that *State* also *influences the churn rate*;


### Preprocessing and feature engineering

We should first deal with the 51 states: we can group them by region (north, south, east, west).

```{r, echo=FALSE}
map_region <- function(state) {
  west.regions <- c("WA", "OR", "ID", "MT", "WY", "CA", "NV", "UT", "CO", "AK", "HI")
  southwest.regions <- c("AZ", "NM", "TX", "OK")
  midwest.regions <- c("ND", "SD", "NE", "KS", "MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH")
  southeast.regions <- c("AR", "LA", "MS", "AL", "TN", "KY", "WV", "VA", "NC", "SC", "GA", "FL")
  northeast.regions <- c("ME", "NH", "VT", "MA", "RI", "CT", "NY", "PA", "NJ", "DE", "MD", "DC")
  
  if (state %in% west.regions) {
    return("West")
  } else if (state %in% midwest.regions) {
    return("Mid West")
  } else if (state %in% southeast.regions) {
    return("South East")
  } else if (state %in% northeast.regions) {
    return("North East")
  } else {
    return("South West")  # in case there are any states not covered
  }
}

data$Region <- sapply(data$State, map_region)
data$Region <- as.factor(data$Region)
```

We'll use the regions column for the first models to avoid having high-cardinality encoded columns: we store the variable in order to use it later with more robust models (tree-based).

```{r}
state.column <- data$State
data$State <- NULL
```

Now let's re-do the chi square:

```{r}
contingency.table <- table(data$Region, data$Churn)
chisq.test(contingency.table)
```

H0: Region and Churn are independent, H1: Region and Churn are dependent. Since p-value \> 0.05, we fail to reject the null hypothesis, therefore the region does not influence the churn rate. Let's see the distribution of churn by region:

```{r, echo=FALSE, fig.align='center'}
ggplot(data, aes(x = Region, fill = Churn)) + 
  geom_bar(width = 0.7, color = 1) +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust = 0, colour = "gray29", size = 10)) +
  labs(title = "Churn by Region", x = "Region", y = "Count", fill = "Churn", subtitle = "Is Churn rate influenced by the Region?") +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank(),
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5)
  ) +
  scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) 
```

We can see that the churn rate is not influenced by the region.

#### Splitting the data and scaling

```{r}
set.seed(1)
ids.train <- sample(1:nrow(data), size = 0.75 * nrow(data), replace = FALSE)
data.train <- data[ids.train,]
data.val <- data[-ids.train,] 
```

We oversample the minority class to balance the dataset, now we have 4260 total observations.

```{r}
data.train.balanced <- ovun.sample(Churn ~ ., data = data.train, method = "over", N=4260)$data
table(data.train.balanced$Churn)
```
In order to to prevent certain columns from dominating the model, we scale it using the training data's mean and standard deviation:
```{r}
cols_to_scale <- c("Account.length", "Area.code", "Number.vmail.messages", "Total.day.calls", "Total.day.charge", "Total.eve.calls", "Total.eve.charge", "Total.night.calls", "Total.night.charge", "Total.intl.calls", "Total.intl.charge", "Customer.service.calls")

train.mean <- apply(data.train.balanced[, cols_to_scale], MARGIN = 2, FUN = mean)
train.sd <- apply(data.train.balanced[, cols_to_scale], MARGIN = 2, FUN = sd)

data.train.balanced[, cols_to_scale] <- scale(data.train.balanced[, cols_to_scale], center = train.mean, scale = train.sd)
data.val[, cols_to_scale] <- scale(data.val[, cols_to_scale], center = train.mean, scale = train.sd)
```
#### Baseline Logistic Regression Model
Our starting point is a logistic regression model. We will use this model as a baseline to compare the performance of more complex models:
```{r}
logistic.baseline <- glm(Churn ~ ., data = data.train.balanced, family = "binomial")
```
The baseline model shows decent accuracy at 83.33%, however, precision at 57.89% and recall at 42.04% indicate areas for potential improvement. This is normal since we still have to perform feature selection and regularization to improve the model.
```{r, echo=FALSE}
baseline.pred <- ifelse(predict(logistic.baseline, newdata = data.val) > 0.5, 1, 0)
(baseline.cm <- table(baseline.pred, data.val$Churn))

get.metrics<- function(conf.mat) {
  true.positives <- conf.mat[2,2]
  true.negatives <- conf.mat[1,1]
  false.positives <- conf.mat[1,2]
  false.negatives <- conf.mat[2,1]
  num.observations <- true.positives + true.negatives + false.positives + false.negatives
  
  accuracy <- (true.positives + true.negatives) / num.observations
  precision <- (true.positives) / (true.positives + false.positives)
  recall <- true.positives / (true.positives + false.negatives)
  f1 <- 2 * ((precision * recall) / (precision + recall))
  
  metrics <- data.frame(t(c(accuracy, precision, recall, f1)))
  columns <- c("Accuracy", "Precision", "Recall", "F1")
  colnames(metrics) <- columns
  
  return(metrics)
}

(baseline.metrics <- get.metrics(baseline.cm))
```

##### AIC and BIC Model Selection
We performed the **Akaike Information Criterion** (AIC) stepwise selection  to identify the most important predictors in the model:
```{r, results='hide'}
akaike.fw <- step(glm(Churn ~ 1, family = "binomial", data = data.train.balanced), scope = formula(logistic.baseline), direction = "forward")
akaike.back <- step(logistic.baseline, direction = "backward")
akaike.both <- step(logistic.baseline, direction = "both")
```
Across all selection methods, certain predictors consistently appear as significant: "International.plan", "Customer.service.calls", "Total.day.charge" and "Voice.mail.plan".

**Bayesian Information criterion** (BIC) is another criterion to select the best model. We will use the BIC to compare the models selected by the AIC:
```{r, results='hide'}
bayesian.fw <- step(glm(Churn ~ 1, family = "binomial", data = data.train.balanced), scope = formula(logistic.baseline), direction = "forward", k = log(nrow(data.train.balanced)))
bayesian.back <- step(logistic.baseline, direction = "backward", k = log(nrow(data.train.balanced)))
bayesian.both <- step(logistic.baseline, direction = "both", k = log(nrow(data.train.balanced)))
```
Considering bidirectional elimination of both methods, BIC is more strict as it removes all the covariates representing the regions.

After that we compare the validation set results:

The results show that the model selected by the AIC method performs slightly better than the one selected by the BIC method. The AIC model has a higher accuracy, precision, recall, and F1 score: maybe dropping all the region features as BIC does is too big of a loss of informations.

**AIC**:
```{r, echo=FALSE}
akaike.preds <- ifelse(predict(akaike.both, data.val) > 0.5, 1, 0)
(akaike.cm <- table(akaike.preds, data.val$Churn))
(akaike.metrics <- get.metrics(akaike.cm))
```
**BIC**:
```{r, echo=FALSE}
bayesian.preds <- ifelse(predict(bayesian.both, data.val) > 0.5, 1, 0)
(bayesian.cm <- table(bayesian.preds, data.val$Churn))
(bayesian.metrics <- get.metrics(bayesian.cm))
```

#### ROC Curves
```{r, echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow = c(2, 2))

roc_full <- roc(data.val$Churn, as.numeric(baseline.pred), 
                plot = TRUE, main = "ROC Curve Full Model", col = "purple", lwd = 3, 
                auc.polygon = TRUE, print.auc = TRUE)

roc_akaike <- roc(data.val$Churn, predict(akaike.both, newdata = data.val, type = "response"), 
                  plot = TRUE, main = "AIC Model", col = "blue", lwd = 3, 
                  auc.polygon = TRUE, print.auc = TRUE)

roc_bayesian <- roc(data.val$Churn, predict(bayesian.both, newdata = data.val, type = "response"), 
                    plot = TRUE, main = "BIC Model", col = "red", lwd = 3, 
                    auc.polygon = TRUE, print.auc = TRUE)

par(mfrow = c(1, 1))
```

**AUC**:
In order to insight into the models' ability to evaluate their predictive performance in discerning churn, we're calculating the Area Under the Curve (AUC) for Akaike, Bayesian, and Baseline models.
```{r, results='hide', message=FALSE, warning=FALSE}
auc_akaike <- as.numeric(auc(roc(data.val$Churn, predict(akaike.both, newdata = data.val, type = "response"))))
auc_bayesian <- as.numeric(auc(roc(data.val$Churn, predict(bayesian.both, newdata = data.val, type = "response"))))
auc_full <- as.numeric(auc(roc(data.val$Churn, baseline.pred)))

```

```{r, echo=FALSE, results='hide'}
# comparison (for now) between full, aic, bic 
akaike_df <- data.frame(Model = "Akaike",
                        Accuracy = akaike.metrics$Accuracy,
                        Precision = akaike.metrics$Precision,
                        Recall = akaike.metrics$Recall,
                        F1_Score = akaike.metrics$F1,
                        AUC = auc_akaike)

bayesian_df <- data.frame(Model = "Bayesian",
                          Accuracy = bayesian.metrics$Accuracy,
                          Precision = bayesian.metrics$Precision,
                          Recall = bayesian.metrics$Recall,
                          F1_Score = bayesian.metrics$F1,
                          AUC = auc_bayesian)

full_df <- data.frame(Model = "Full Logistic",
                      Accuracy = baseline.metrics$Accuracy,
                      Precision = baseline.metrics$Precision,
                      Recall = baseline.metrics$Recall,
                      F1_Score = baseline.metrics$F1,
                      AUC = auc_full)

comparison_df <- bind_rows(akaike_df, bayesian_df, full_df)
comparison_df
```
##### LASSO and RIDGE REGULARIZATION
We will use LASSO and Ridge regularization to improve the logistic regression model. We will use:

-`glmnet` package to perform the regularization;

- `cv.glmnet` function to perform cross-validation to find the optimal value of the regularization parameter lambda;

- `glmnet` function to fit the model with the optimal lambda value.

**LASSO**:
```{r}
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)
lasso <- train(Churn ~ ., data = data.train.balanced, method = "glmnet", metric = "Accuracy", trControl = ctrl, tuneGrid = expand.grid(alpha = 1, lambda = seq(0, 0.15, length = 30)))
max(lasso$results$Accuracy)
lasso$bestTune

lasso.predict <- predict(lasso, data.val)
lasso.cm <- table(lasso.predict, data.val$Churn)
lasso.metrics <- get.metrics(lasso.cm)
```
```{r, echo=FALSE}
lasso.plot <- lasso %>% 
  ggplot(aes(x = lambda, y = Accuracy)) + 
  geom_line() + 
  geom_point() +
  geom_text(aes(label = sprintf("%.3f", Accuracy)), check_overlap = TRUE, vjust = -0.5, size = 2.5) + 
  scale_x_continuous(limits = c(0, 0.10)) + 
  labs(x = TeX("Lambda ($\\lambda$)"), y = "Accuracy", title = "Accuracy vs. Lambda for Lasso Regularization") +
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold")
  )
```
**RIDGE**:
```{r}
set.seed(1)
ridge <- train(Churn ~ ., data = data.train.balanced, method = "glmnet", metric = "Accuracy", trControl = ctrl, tuneGrid = expand.grid(alpha = 0, lambda = seq(0, 0.15, length = 30)))
max(ridge$results$Accuracy)
ridge$bestTune

ridge.predict <- predict(ridge, data.val)
ridge.cm <- table(ridge.predict, data.val$Churn)
ridge.metrics <- get.metrics(ridge.cm)
```
```{r, echo=FALSE, message=FALSE, warning=FALSE}

ridge.plot <- ridge %>% 
  ggplot(aes(x = lambda, y = Accuracy)) + 
  geom_line() + 
  geom_point() +
  geom_text(aes(label = sprintf("%.3f", Accuracy)), check_overlap = TRUE, vjust = -0.5, size = 2.5) +
  labs(x = TeX("Lambda ($\\lambda$)"), y = "Accuracy", title = "Accuracy vs. Lambda for Ridge Regularization") +
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold")
  )
```
```{r, echo=FALSE, warning=FALSE, message=FALSE}
par(mfrow = c(1,2))

auc.lasso <- roc(data.val$Churn, as.numeric(lasso.predict), 
                 plot = TRUE, main = "ROC Curve Lasso Model", col = "purple", lwd = 3, 
                 auc.polygon = TRUE, print.auc = TRUE)

auc.ridge <- roc(data.val$Churn, as.numeric(ridge.predict), 
                 plot = TRUE, main = "ROC Curve Ridge Model", col = "black", lwd = 3, 
                 auc.polygon = TRUE, print.auc = TRUE)
```
```{r, echo=FALSE, warning=FALSE, message=FALSE}
grid.arrange(lasso.plot, ridge.plot, top = "Penalized Approaches for Logistic Regression")

lasso_df <- data.frame(Model = "Logistic Lasso",
                       Accuracy = lasso.metrics$Accuracy,
                       Precision = lasso.metrics$Precision,
                       Recall = lasso.metrics$Recall,
                       F1_Score = lasso.metrics$F1,
                       AUC = as.numeric(auc.lasso$auc))

ridge_df <- data.frame(Model = "Logistic Ridge",
                       Accuracy = ridge.metrics$Accuracy,
                       Precision = ridge.metrics$Precision,
                       Recall = ridge.metrics$Recall,
                       F1_Score = ridge.metrics$F1,
                       AUC = as.numeric(auc.ridge$auc))
```



Now let's compare the metrics obtained with the different models:
```{r, echo=FALSE}
comparison_df <- rbind(comparison_df, ridge_df, lasso_df)
comparison_df
```
Our baseline logistic regression model demonstrates moderate performance across various metrics:

- following analyses under different approaches, accuracy ranges from 78.54% to 83.69%;

- precision consistently hovers around 76%, indicating a high percentage of correctly predicted positive cases;

- recall is lower, ranging from 36% to 43%, suggesting room for improvement in capturing all positive cases;

- the F1 score, a balanced metric, varies from 0.49 to 0.50. AUC values, indicating model discrimination, range from 0.73 to 0.83;

Overall, the model performs reasonably well but could benefit from increased recall to capture more positive cases.

#### Other models

From now on we are studying other models and comparing the metrics with the logistic regression we chose as baseline model:

- **Decision trees**

- **Random Forest**

- **Gradient Boosting**


#### Decision Trees
First of all we split again to undo the scaling and to make trees more interpretable for us:
```{r}
set.seed(1)
ids.train <- sample(1:nrow(data), size = 0.75 * nrow(data), replace = FALSE)
data.train <- data[ids.train,]
data.val <- data[-ids.train,] 
```
Then we oversampled the minority class in order to have a total of 4260 observations as in the previous case:
```{r results='hide'}
data.train.balanced <- ovun.sample(Churn ~ ., data = data.train, method = "over", N=4260)$data
table(data.train.balanced$Churn)
```
```{r echo=FALSE}
set.seed(1) # otherwise results might be inconsistent due to ties
tree.full <- tree(Churn ~ ., data = data.train.balanced)
par(mar=c(5,5,2,2))  # Adjust margin sizes to avoid the "Error in plot.new() : figure margins too large"
plot(tree.full)
text(tree.full, pretty = 0, cex = 0.7)
```

```{r echo=FALSE}
tree.pred <- predict(tree.full, data.val, type = "class")
table(tree.pred, data.val$Churn)
get.metrics(table(tree.pred, data.val$Churn))
```

##### Pruning the tree
In order to improve the performance and generalization ability of decision trees (to prevent overfitting) we adopted a Pruning approach with cross-validation:


```{r echo=FALSE}
cv.trees <- cv.tree(tree.full, FUN = prune.misclass)
#(optimal.size <- cv.trees$size[which.min(cv.trees$dev)])
par(mfrow = c(1,2))
plot(cv.trees$size, cv.trees$dev/nrow(data.val), type = "b", xlab = "Size", ylab = "Error")
plot(cv.trees$k, cv.trees$dev/nrow(data.val), type = "b", col = 3, xlab = "K", ylab = "Error")
```

After pruning, the resuting tree is the following:

```{r echo=FALSE}
pruned.tree <- prune.misclass(tree.full, k = 0, best = optimal.size)
plot(pruned.tree)
text(pruned.tree, pretty = 0, cex = 0.7)
```

The predictive performance of the pruned tree is evaluated on the validation dataset, giving as contingency table the below:
```{r echo=FALSE, message=FALSE, warning=FALSE}
pruned.pred <- predict(pruned.tree, data.val, type = "class")
table(pruned.pred, data.val$Churn)
trees.metrics <- get.metrics(table(pruned.pred, data.val$Churn))
trees.auc <- roc(data.val$Churn, as.numeric(pruned.pred))
```

```{r echo=FALSE}
cv_trees_df <- data.frame(Model = "CV Decision Trees",
                          Accuracy = trees.metrics$Accuracy,
                          Precision = trees.metrics$Precision,
                          Recall = trees.metrics$Recall,
                          F1_Score = trees.metrics$F1,
                          AUC = as.numeric(trees.auc$auc))
```
#### Random Forest

Now its the turn of the Random Forest model. Using again a seed set for reproducibility, we train the model on a balanced training dataset, configuring parameters such as the number of trees and variable selection for each split (500).
```{r}
set.seed(1)
fit.bag <- randomForest(Churn ~ ., data = data.train.balanced, mtry = ncol(data.train.balanced) - 1, ntree = 500, importance = T) 
```

```{r echo=FALSE, fig.align='center'}
layout(matrix(c(1,2),nrow=1),
       width=c(4,1)) 
par(mar=c(5,4,4,0)) # No margin right side
plot(fit.bag, log="y")
par(mar=c(5,0,4,2)) # No margin left side
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("bottom", colnames(fit.bag$err.rate),col=1:4,cex=0.8,fill=1:4)
```


In this section, we evaluate our bagged model's performance on the validation dataset: 
```{r echo=FALSE, fig.align='center'}
varImpPlot(fit.bag)
mean(fit.bag$err.rate[,1])
validation.preds <- predict(fit.bag, newdata = data.val)
(bag.missclassification <- mean(validation.preds != data.val$Churn)) 
table(validation.preds, data.val$Churn)
```
```{r echo=FALSE}
rf.metrics <- get.metrics(table(validation.preds, data.val$Churn))
rf.auc <- roc(data.val$Churn, as.numeric(validation.preds))
```

We utilized two metrics for error estimation: Out-of-Bag (OOB) test error, which is approximately 1.33%, and true test error, which is approximately 3.72%. By comparing these figures, we observe that with a larger number of trees, the OOB test error estimation could potentially approach the true test error more closely.
Additionally, we examine the contingency table of predicted versus actual outcomes, which illustrates the model's performance. For instance, out of 720 instances predicted as 'False', 709 are actually 'False', while out of 114 instances predicted as 'True', 94 are indeed 'True'.

```{r echo=FALSE}
rf_df <- data.frame(Model = "Random Forests",
                    Accuracy = rf.metrics$Accuracy,
                    Precision = rf.metrics$Precision,
                    Recall = rf.metrics$Recall,
                    F1_Score = rf.metrics$F1,
                    AUC = as.numeric(rf.auc$auc))
```

#### eXtreme Gradient Boosting

We're now using XGBoost to build a churn prediction model.
First, we'll prepare our data by encoding categorical variables and splitting it into training and validation sets, then, we'll train the XGBoost model with specified parameters.

```{r results='hide', message=FALSE, warning=FALSE}
X.tr <- model.matrix(Churn ~ ., data = data.train.balanced)[, -1]
y.tr <- as.numeric(data.train.balanced$Churn)-1
X.val <- model.matrix(Churn ~ ., data = data.val)[, -1]
y.val <- as.numeric(data.val$Churn)-1
fit.xg <- xgboost(as.matrix(X.tr), label = y.tr, nrounds = 50, objective = "binary:logistic", eval_metric = "error")
```
As shown from the results below, the **XGBoost** model achieves **strong performance** with a 97% of accuracy, a recall of 93.20% for 'Churn' instances and a precision of 84.21%. It exhibits low rates of false negatives (18 out of 114) and false positives (7 out of 731).
```{r echo=FALSE, message=FALSE, warning=FALSE}
xg.pred <- ifelse(predict(fit.xg, X.val)> 0.5, 1, 0)
table(xg.pred, data.val$Churn)
get.metrics(table(xg.pred, data.val$Churn))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
train.errors <- fit.xg$evaluation_log$train_error
val.errors <- numeric(50)

for (i in 1:50) {
  pred_i = ifelse(predict(fit.xg, X.val, ntreelimit = i) > 0.5, 1, 0)
  val.errors[i] = mean(pred_i != y.val)
} 

plot(1:50, val.errors, type="b", xlab="number of trees", ylab="error", col=3, ylim = c(0,0.3))
points(1:50, train.errors, type="b")
legend("topright", legend = c("Train", "Test"), col=c(1, 3), lty=1, lwd=2, cex = 1)
#which.min(val.errors)
abline(v = which.min(val.errors))
```
```{r echo=FALSE, results='hide', warning=FALSE, message=FALSE}
(feature.importance <- xgb.importance(model = fit.xg))
```
##### Feature Importance
```{r, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
xgb.plot.importance(importance_matrix = feature.importance)
```
Notably, **total day minutes, customer service calls, and the presence of an international plan emerge as the top features**, indicating their significant influence on customer churn. Other factors such as evening and international call durations also contribute to the model's predictive power, albeit to a lesser extent. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
fitControl = trainControl(
  method = "cv",
  number = 10, 
  search="random")

tune_grid = 
  expand.grid(
    nrounds = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100),
    eta = 0.3,
    max_depth=5,
    subsample = 1,
    colsample_bytree = 1,
    min_child_weight = 5,
    gamma = c(0.1, 0.2, 0.5, 0.75, 1)
  )

set.seed(1)
fit_xg_cv = train(Churn ~ ., data = data.train.balanced, 
                  method = "xgbTree", 
                  trControl = fitControl,
                  verbose = FALSE, 
                  tuneGrid = tune_grid,
                  objective = "binary:logistic", 
                  eval_metric = "error")
```
 Here we can see from the confusion matrix that the model has a good performance, but if compared to the matrix shown at the beginning of the section, we can see that the model improved in the prediction of true positives (96 before, 98 now), but has a slightly worse performance in predicting true negatives (713 before, 707 now).
```{r echo=FALSE, warning=FALSE, message=FALSE}
pred_xg_cv = predict(fit_xg_cv, data.val, type="raw")
#mean(pred_xg_cv != data.val$Churn)
table(pred_xg_cv, data.val$Churn)
xgb.metrics <- get.metrics(table(pred_xg_cv, data.val$Churn))

auc.xgb <- roc(data.val$Churn, as.numeric(pred_xg_cv))

xgb_df <- data.frame(Model = "CV XGB",
                     Accuracy = xgb.metrics$Accuracy,
                     Precision = xgb.metrics$Precision,
                     Recall = xgb.metrics$Recall,
                     F1_Score = xgb.metrics$F1,
                     AUC = as.numeric(auc.xgb$auc))
```



### Main conclusions

#### Main conclusions

To sum up, we have performed a comprehensive analysis of the churn dataset, including data exploration, preprocessing, and feature engineering. We have built a logistic regression model as a baseline and compared its performance with other models, such as decision trees, random forests, and gradient boosting. 

```{r, echo=FALSE}
comparison_df <- rbind(comparison_df, cv_trees_df)
comparison_df <- rbind(comparison_df, rf_df)
comparison_df <- rbind(comparison_df, xgb_df)
comparison_df
```

Now to draw the main conclusions from our analysis let's compare the metrics obtained with the different models' final data:

-  **Best classification model**: Random Forests demonstrate the best overall performance, with an accuracy of 96.64%, precision of 81.58%, recall of 93.00%, F1-score of 86.92%, and area under the curve (AUC) of 90.30%. This indicates that Random Forests is the most effective model in predicting churn.

-  **Boosting models**: CV XGBoost shows an accuracy of 96.52%, precision of 85.96%, recall of 88.29%, F1-score of 87.11%, and AUC of 92.08%, demonstrating performance similar to Random Forests and superior to other classification models.

-  **Linear models**: Linear models (Akaike, Bayesian, Full Logistic, Logistic Ridge, Logistic Lasso) exhibit lower performance compared to boosting models and decision trees, with AUC ranging from 72.63% to 83.44%. However, Logistic Ridge and Logistic Lasso show notably higher precision compared to other linear models.

-  **Decision trees**: CV Decision Trees demonstrate good performance with an accuracy of 90.05%, precision of 87.72%, recall of 59.17%, F1-score of 70.67%, and AUC of 89.07%, but are surpassed by boosting models and Random Forests in terms of AUC.

In conclusion, **Random Forests and CV XGBoost emerge as the most effective models** for churn prediction, with Random Forests showing slightly better performance in terms of AUC.


## Task 2 {.tabset}

Cluster customers according to their behavior

### Feature Engineering

### Kmeans

### Hierarchical Clustering

