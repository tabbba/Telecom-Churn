---
title: "Data Analysis for Business - Midterm Project"
author: Edoardo Cocciò, Alexandra Tabarani, Marta Torella, Simone Filosofi
output:
  html_document: default
---

# Standard Deviated Group {.tabset .tabset-fade}

Group members:

-   Edoardo Cocciò - 282401
-   Alexandra Tabarani - 282091
-   Marta Torella - 284091
-   Simone Filosofi - 284531

## Task 1 {.tabset .tabset-fade}

The dataset we are going to analyze is the "Telecom Churn" dataset, which is a dataset that contains information about a telecommunications company that is trying to understand why its customers are leaving the company. Our role is to build a classification model to help Telecom in targeting the willing-to-churn customers before it is too late.

```{r message=FALSE, warning=FALSE, include=FALSE}
library(ggplot2)
library(corrplot)
library(dplyr)
library(tidyr)
library(skimr)
library(treemapify)
library(stringr)
library(glue)
library(dummy)
library(caret)
library(car)
library(pROC)
library(glmnet)
library(rpart)
library(rpart.plot) 
library(randomForest)
library(pROC)
library(cluster)
library(factoextra)
library(sf)
library(latex2exp)
library(gridExtra)
library(tree)
library(randomForest)
library(xgboost)
library(dendextend)
library(purrr)
library(ROSE)

data <- read.csv("TelecomChurn.csv")
```

### EDA

```{r}
names(data)

data$State = as.factor(data$State)
data$International.plan = as.factor(data$International.plan)
data$Voice.mail.plan = as.factor(data$Voice.mail.plan)
data$Churn = as.factor(data$Churn)
```

#### Missing values and duplicates

```{r}
sum(is.na(data))
anyDuplicated(data)
```

We checked for missing values in the dataset and found that there are no missing values in the dataset. We also checked for duplicates and found that there are no duplicates either.

```{r}
continuous_vars <- data[, sapply(data, is.numeric)]
categorical_vars <- data[, sapply(data, is.factor) & !names(data) %in% "Churn"]
```

#### Univariate analysis for numerical variables

```{r, echo=FALSE, fig.align='center', fig.width=10, fig.height=10}
continuous <- data[, sapply(data, is.numeric) | names(data) == "Churn"]

continuous %>%
  pivot_longer(cols = -Churn, names_to = "metric", values_to = "value") %>%
  ggplot(aes(value, color = as.factor(Churn))) +
  geom_density(alpha = 0.3) +
  facet_wrap(vars(metric), scales = "free") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  scale_color_manual(name = "Churn", labels = c("No", "Yes"), values = c("#15aabf", "#ff8787")) +
  labs(title = "Numeric Features", subtitle = "Univariate Distribution by Churn") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 18, colour = "gray5"),
    plot.subtitle = element_text(hjust = 0.5, colour = "gray5"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    legend.position = "top"
  )
```

The univariate analysis of the numerical variables shows that the distribution of the numerical variables is different for the two levels of the target variable "Churn". Foe example there's a noticeable difference in the distribution of **customer service calls**: customers who churned seem to make more customer service calls, which could indicate a relationship between customer satisfaction and churn. It appears that customers who churned tend to have slightly higher day minutes and charges. This might suggest that customers with higher usage during the day are more likely to churn. Something similar happens with **total.day.charge** also. The distribution of the other variables is not as clear-cut as the ones mentioned above, but there are some differences in the distributions of the variables between the two levels of the target variable.

```{r include=FALSE}
continuous_vars <- data[, sapply(data, is.numeric)]
categorical_vars <- data[, sapply(data, is.factor) & !names(data) %in% "Churn"]
```

#### Distribution of Target and Categorical Variables

```{r echo=FALSE, fig.show ="hold", out.width="50%"}
churn_percent <- data %>%
  group_by(Churn) %>%
  summarise(Count = n(), .groups = "drop") %>%
  mutate(Percent = Count / sum(Count) * 100)

ggplot(churn_percent, aes(x = "", y = Percent, fill = Churn)) +
  geom_col(width = 1, color = 1) +
  coord_polar("y", start = 0) +
  theme_void() +
  scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
  labs(title = "Percentage of Churn", fill = "Churn") +
  geom_text(aes(label = paste0(round(Percent, 1), "%")), position = position_stack(vjust = 0.5)) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 18))

long_data <- categorical_vars %>%
  pivot_longer(cols = c("International.plan", "Voice.mail.plan"), names_to = "Category", values_to = "Value")

ggplot(long_data, aes(x = Value, fill = Value)) +
  geom_bar(color = "black") +
  facet_wrap(~ Category, scales = "free_x", nrow = 2, ncol = 2) +
  labs(title = "Distribution of Categorical Variables", x = NULL, y = "Count") +
  theme_minimal() +
  scale_fill_manual(values = c("No" = "#ffe8cc", "Yes" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
  theme(
    strip.text = element_text(size = 12),
    axis.text.x = element_text(angle = 90, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    legend.position = "none"
  ) 

```

```{r message=FALSE, warning=FALSE, include=FALSE}
us_map <- map_data("state") %>% 
  as_tibble()

us_map %>%
  ggplot(aes(long, lat, map_id = region)) + 
  geom_map(
    map =us_map,
    color = "gray80",
    fill = "gray30",
    size = 0.3
  ) + 
  coord_map("ortho", orientation = c(39, -98, 0)) + 
  labs(
    title = "US Map",
    subtitle = "Based on Latitude and Longitude"
  ) + theme(plot.title = element_text(hjust=0.5))
```

```{r echo=FALSE, fig.align="center", fig.height=9, fig.width=9, message=FALSE, warning=FALSE}
state_names <- c(KS = "kansas", OH = "ohio", NJ = "new jersey", OK = "oklahoma",
                 AL = "alabama", MA = "massachusetts", MO = "missouri", LA = "louisiana",
                 WV = "west virginia", IN = "indiana", RI = "rhode island", IA = "iowa",
                 MT = "montana", NY = "new york", ID = "idaho", VT = "vermont",
                 VA = "virginia", TX = "texas", FL = "florida", CO = "colorado",
                 AZ = "arizona", SC = "south carolina", NE = "nebraska", WY = "wyoming",
                 HI = "hawaii", IL = "illinois", NH = "new hampshire", GA = "georgia",
                 AK = "alaska", MD = "maryland", AR = "arkansas", WI = "wisconsin",
                 OR = "oregon", MI = "michigan", DE = "delaware", UT = "utah",
                 CA = "california", MN = "minnesota", SD = "south dakota", NC = "north carolina",
                 WA = "washington", NM = "new mexico", NV = "nevada", DC = "district of columbia",
                 KY = "kentucky", ME = "maine", MS = "mississippi", TN = "tennessee",
                 PA = "pennsylvania", CT = "connecticut", ND = "north dakota")

churn.tbl <- data %>% select(State, Churn) %>%
  group_by(State) %>%
  summarize(
    total_customers = n(),
    churned_customers = sum(Churn == "True"),
    churn_rate = (churned_customers / total_customers),
    churn_rate_txt = scales::percent(churn_rate)
  ) %>%
  mutate(full_state = state_names[as.character(State)]) %>%
  ungroup() %>% 
  left_join(us_map, by=c("full_state" = "region"))

us_map_sf <- st_as_sf(us_map, coords = c("long", "lat"), crs = 4326, agr = "constant")

centroids <- us_map_sf %>%
  group_by(region) %>%
  summarise(geometry = st_centroid(st_union(geometry)))

centroids_df <- as.data.frame(st_coordinates(centroids))
centroids_df$region <- centroids$region

churn.tbl <- churn.tbl %>%
  left_join(centroids_df, by = c("full_state" = "region"))

churn.tbl %>%
  ggplot(aes(long, lat)) + 
  geom_map(
    aes(map_id = full_state),
    map = us_map,
    color = "gray86",
    fill = "gray30",
    size = 0.3
  ) + coord_map("ortho", orientation = c(39, -98, 0)) + geom_polygon(aes(group = group, fill = churn_rate), color="black") +
  scale_fill_gradient2("",low = "#18BC9C", mid = "white", high = "#E31A1C", midpoint = 0.10, labels = scales::percent) + 
  geom_text(aes(x = X, y = Y, label = State), stat = "unique", size = 3, inherit.aes = FALSE, fontface = "bold") +
  theme_void() + 
  theme(
    plot.title = element_text(size=18, face="bold", color = "#2C3E50"),
    legend.position = "right"
  ) + 
  labs(
    title = "Churn Rate Across US States",
    x = "",
    y = ""
  ) + 
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
  ) 
```

#### Hypothesis testing

#### Correlation analysis

```{r, echo=FALSE, fig.align='center'}
cor_mat <-
  data %>% 
  select(where(is.numeric)) %>% 
  cor()

corrplot(
  main = "\n\nCorrelation Matrix",
  cor_mat,
  method = "color",
  order = "alphabet",
  type = "lower",
  diag = FALSE,
  number.cex = 0.6,
  tl.cex = 0.6,
  tl.srt = 45,
  cl.pos = "b",
  addgrid.col = "white",
  addCoef.col = "white",
  col = COL1("Purples"),
  bg="gray",
  tl.col = "grey50"
)
```

The correlation matrix shows that there is a **perfect correlation between the charge variables and minutes variables** so we can drop all the minutes one **we drop Total.day.minutes, Total.eve.minutes, Total.night.minutes, Total.intl.minutes**.


### Lower-dimensional models

Before proceeding with the full model we need to focus on some lower dimensional model in order to investigate some interesting relationships between the variables. We only include customers who have made a service call:

```{r}
service_calls <- data %>%
  filter(Customer.service.calls > 0)
```
This is the proportion of customers who churned after making a service call:
```{r, echo=FALSE, fig.align='center'}
churn_percent <- service_calls %>%
  group_by(Customer.service.calls, Churn) %>%
  summarise(Count = n(), .groups = "drop") %>%
  mutate(Percent = Count / sum(Count) * 100)

ggplot(churn_percent, aes(x = Customer.service.calls, y = Percent, fill = Churn)) +
  geom_col(width = 0.7, color = 1) +
  labs(title = "Churn by Customer Service Calls", x = "Customer Service Calls", y = "Percent", fill = "Churn") +
  theme_minimal() +
  scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "bottom"
  )
```

We can see that the churn rate increases as the number of customer service calls increases. This suggests that customers who make more service calls are more likely to churn.




(decidere se gli vogliamo mettere tutti o meno)




**N.B.** We computed more analysis, using plots and hypothesis testing, to understand the relationship between the variables and the target variable "Churn", but for limited space restrictions we will not show them here. In the above paragraph ('Conclusions up to now') we will report the main conclusions arising from this analysis.

**CONCLUSION UP TO NOW:** 
- we dropped the *minutes variables* because they are *perfectly correlated with the charge variables*; 
- we saw that the *churn rate* is *higher* for customers that have an *international plan*; 
- we saw that the churn rate is *higher* for customers that do *not have a voice mail plan*; 
- we saw that the churn rate is *higher* for customers that *have a higher number of customer service calls*; 
- we saw that the churn rate is *higher* for customers that *have higher total day minutes and total day charge*; 
- we saw that the churn rate is *higher* for customers that *have higher total eve charge, night charge, and intl charge*; 
- we saw that *State* also *influences the churn rate*;


### Preprocessing and feature engineering

We should first deal with the 51 states: we can group them by region (north, south, east, west).

```{r, echo=FALSE}
map_region <- function(state) {
  west.regions <- c("WA", "OR", "ID", "MT", "WY", "CA", "NV", "UT", "CO", "AK", "HI")
  southwest.regions <- c("AZ", "NM", "TX", "OK")
  midwest.regions <- c("ND", "SD", "NE", "KS", "MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH")
  southeast.regions <- c("AR", "LA", "MS", "AL", "TN", "KY", "WV", "VA", "NC", "SC", "GA", "FL")
  northeast.regions <- c("ME", "NH", "VT", "MA", "RI", "CT", "NY", "PA", "NJ", "DE", "MD", "DC")
  
  if (state %in% west.regions) {
    return("West")
  } else if (state %in% midwest.regions) {
    return("Mid West")
  } else if (state %in% southeast.regions) {
    return("South East")
  } else if (state %in% northeast.regions) {
    return("North East")
  } else {
    return("South West")  # in case there are any states not covered
  }
}

data$Region <- sapply(data$State, map_region)
data$Region <- as.factor(data$Region)
```

We'll use the regions column for the first models to avoid having high-cardinality encoded columns: we store the variable in order to use it later with more robust models (tree-based).

```{r}
state.column <- data$State
data$State <- NULL
```

Now let's re-do the chi square:

```{r}
contingency.table <- table(data$Region, data$Churn)
chisq.test(contingency.table)
```

H0: Region and Churn are independent, H1: Region and Churn are dependent. Since p-value \> 0.05, we fail to reject the null hypothesis, therefore the region does not influence the churn rate. Let's see the distribution of churn by region:

```{r, echo=FALSE}
ggplot(data, aes(x = Region, fill = Churn)) + 
  geom_bar(width = 0.7, color = 1) +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust = 0, colour = "gray29", size = 10)) +
  labs(title = "Churn by Region", x = "Region", y = "Count", fill = "Churn", subtitle = "Is Churn rate influenced by the Region?") +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank(),
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5)
  ) +
  scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) 
```

We can see that the churn rate is not influenced by the region.

#### Splitting the data and scaling

```{r}
set.seed(1)
ids.train <- sample(1:nrow(data), size = 0.75 * nrow(data), replace = FALSE)
data.train <- data[ids.train,]
data.val <- data[-ids.train,] 
```

We oversample the minority class to balance the dataset, now we have 4260 total observations.

```{r}
data.train.balanced <- ovun.sample(Churn ~ ., data = data.train, method = "over", N=4260)$data
table(data.train.balanced$Churn)
```
In order to to prevent certain columns from dominating the model, we scale it using the training data's mean and standard deviation:
```{r}
cols_to_scale <- c("Account.length", "Area.code", "Number.vmail.messages", "Total.day.calls", "Total.day.charge", "Total.eve.calls", "Total.eve.charge", "Total.night.calls", "Total.night.charge", "Total.intl.calls", "Total.intl.charge", "Customer.service.calls")

train.mean <- apply(data.train.balanced[, cols_to_scale], MARGIN = 2, FUN = mean)
train.sd <- apply(data.train.balanced[, cols_to_scale], MARGIN = 2, FUN = sd)

data.train.balanced[, cols_to_scale] <- scale(data.train.balanced[, cols_to_scale], center = train.mean, scale = train.sd)
# Scale validation data using training data's parameters
data.val[, cols_to_scale] <- scale(data.val[, cols_to_scale], center = train.mean, scale = train.sd)
```
#### Baseline Logistic Regression Model
Our starting point is a logistic regression model. We will use this model as a baseline to compare the performance of more complex models:
```{r}
logistic.baseline <- glm(Churn ~ ., data = data.train.balanced, family = "binomial")
#summary(logistic.baseline)
```
The confusion matrix and the metrics we obtained at the beginning are:
```{r, echo=FALSE}
baseline.pred <- ifelse(predict(logistic.baseline, newdata = data.val) > 0.5, 1, 0)
(baseline.cm <- table(baseline.pred, data.val$Churn))

get.metrics<- function(conf.mat) {
  true.positives <- conf.mat[2,2]
  true.negatives <- conf.mat[1,1]
  false.positives <- conf.mat[1,2]
  false.negatives <- conf.mat[2,1]
  num.observations <- true.positives + true.negatives + false.positives + false.negatives
  
  accuracy <- (true.positives + true.negatives) / num.observations
  precision <- (true.positives) / (true.positives + false.positives)
  recall <- true.positives / (true.positives + false.negatives)
  f1 <- 2 * ((precision * recall) / (precision + recall))
  
  metrics <- data.frame(t(c(accuracy, precision, recall, f1)))
  columns <- c("Accuracy", "Precision", "Recall", "F1")
  colnames(metrics) <- columns
  
  return(metrics)
}

(baseline.metrics <- get.metrics(baseline.cm))
```
We performed the **Akaike Information Criterion** (AIC) stepwise selection  to identify the most important predictors in the model:
```{r, results='hide'}
akaike.fw <- step(glm(Churn ~ 1, family = "binomial", data = data.train.balanced), scope = formula(logistic.baseline), direction = "forward")
akaike.back <- step(logistic.baseline, direction = "backward")
akaike.both <- step(logistic.baseline, direction = "both")
```
Across all selection methods, certain predictors consistently appear as significant: "International.plan", "Customer.service.calls", "Total.day.charge" and "Voice.mail.plan".

**Bayesian Information criterion** (BIC) is another criterion to select the best model. We will use the BIC to compare the models selected by the AIC:
```{r, results='hide'}
bayesian.fw <- step(glm(Churn ~ 1, family = "binomial", data = data.train.balanced), scope = formula(logistic.baseline), direction = "forward", k = log(nrow(data.train.balanced)))
bayesian.back <- step(logistic.baseline, direction = "backward", k = log(nrow(data.train.balanced)))
bayesian.both <- step(logistic.baseline, direction = "both", k = log(nrow(data.train.balanced)))
```
Considering bidirectional elimination of both methods, BIC is more strict as it removes all the covariates representing the regions.

After that we compare the validation set results:
**AIC**:
```{r, echo=FALSE}
akaike.preds <- ifelse(predict(akaike.both, data.val) > 0.5, 1, 0)
(akaike.cm <- table(akaike.preds, data.val$Churn))
(akaike.metrics <- get.metrics(akaike.cm))
```
**BIC**:
```{r, echo=FALSE}
bayesian.preds <- ifelse(predict(bayesian.both, data.val) > 0.5, 1, 0)
(bayesian.cm <- table(bayesian.preds, data.val$Churn))
(bayesian.metrics <- get.metrics(bayesian.cm))
```
The results show that the model selected by the AIC method performs slightly better than the one selected by the BIC method. The AIC model has a higher accuracy, precision, recall, and F1 score: maybe dropping all the region features as BIC does is too big of a loss of informations.


#### ROC Curves
```{r, echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow = c(2, 2))

roc_full <- roc(data.val$Churn, as.numeric(baseline.pred), 
                plot = TRUE, main = "ROC Curve Full Model", col = "purple", lwd = 3, 
                auc.polygon = TRUE, print.auc = TRUE)

roc_akaike <- roc(data.val$Churn, predict(akaike.both, newdata = data.val, type = "response"), 
                  plot = TRUE, main = "AIC Model", col = "blue", lwd = 3, 
                  auc.polygon = TRUE, print.auc = TRUE)

roc_bayesian <- roc(data.val$Churn, predict(bayesian.both, newdata = data.val, type = "response"), 
                    plot = TRUE, main = "BIC Model", col = "red", lwd = 3, 
                    auc.polygon = TRUE, print.auc = TRUE)

par(mfrow = c(1, 1))
```

**AUC**:
```{r, results='hide', message=FALSE, warning=FALSE}
auc_akaike <- as.numeric(auc(roc(data.val$Churn, predict(akaike.both, newdata = data.val, type = "response"))))
auc_bayesian <- as.numeric(auc(roc(data.val$Churn, predict(bayesian.both, newdata = data.val, type = "response"))))
auc_full <- as.numeric(auc(roc(data.val$Churn, baseline.pred)))

```

```{r, echo=FALSE, results='hide'}
# comparison (for now) between full, aic, bic 
akaike_df <- data.frame(Model = "Akaike",
                        Accuracy = akaike.metrics$Accuracy,
                        Precision = akaike.metrics$Precision,
                        Recall = akaike.metrics$Recall,
                        F1_Score = akaike.metrics$F1,
                        AUC = auc_akaike)

bayesian_df <- data.frame(Model = "Bayesian",
                          Accuracy = bayesian.metrics$Accuracy,
                          Precision = bayesian.metrics$Precision,
                          Recall = bayesian.metrics$Recall,
                          F1_Score = bayesian.metrics$F1,
                          AUC = auc_bayesian)

full_df <- data.frame(Model = "Full Logistic",
                      Accuracy = baseline.metrics$Accuracy,
                      Precision = baseline.metrics$Precision,
                      Recall = baseline.metrics$Recall,
                      F1_Score = baseline.metrics$F1,
                      AUC = auc_full)

comparison_df <- bind_rows(akaike_df, bayesian_df, full_df)
comparison_df
```
##### LASSO and RIDGE REGULARIZATION
We will use LASSO and Ridge regularization to improve the logistic regression model. We will use:
-`glmnet` package to perform the regularization;
- `cv.glmnet` function to perform cross-validation to find the optimal value of the regularization parameter lambda;
- `glmnet` function to fit the model with the optimal lambda value.

**LASSO**:
```{r}
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)
lasso <- train(Churn ~ ., data = data.train.balanced, method = "glmnet", metric = "Accuracy", trControl = ctrl, tuneGrid = expand.grid(alpha = 1, lambda = seq(0, 0.15, length = 30)))
max(lasso$results$Accuracy)
lasso$bestTune

lasso.predict <- predict(lasso, data.val)
lasso.cm <- table(lasso.predict, data.val$Churn)
lasso.metrics <- get.metrics(lasso.cm)
```
```{r, echo=FALSE}
lasso.plot <- lasso %>% 
  ggplot(aes(x = lambda, y = Accuracy)) + 
  geom_line() + 
  geom_point() +
  geom_text(aes(label = sprintf("%.3f", Accuracy)), check_overlap = TRUE, vjust = -0.5, size = 2.5) + 
  scale_x_continuous(limits = c(0, 0.10)) + 
  labs(x = TeX("Lambda ($\\lambda$)"), y = "Accuracy", title = "Accuracy vs. Lambda for Lasso Regularization") +
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold")
  )
```
**RIDGE**:
```{r}
set.seed(1)
ridge <- train(Churn ~ ., data = data.train.balanced, method = "glmnet", metric = "Accuracy", trControl = ctrl, tuneGrid = expand.grid(alpha = 0, lambda = seq(0, 0.15, length = 30)))
max(ridge$results$Accuracy)
ridge$bestTune

ridge.predict <- predict(ridge, data.val)
ridge.cm <- table(ridge.predict, data.val$Churn)
ridge.metrics <- get.metrics(ridge.cm)
```
```{r, echo=FALSE, message=FALSE, warning=FALSE}

ridge.plot <- ridge %>% 
  ggplot(aes(x = lambda, y = Accuracy)) + 
  geom_line() + 
  geom_point() +
  geom_text(aes(label = sprintf("%.3f", Accuracy)), check_overlap = TRUE, vjust = -0.5, size = 2.5) +
  labs(x = TeX("Lambda ($\\lambda$)"), y = "Accuracy", title = "Accuracy vs. Lambda for Ridge Regularization") +
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold")
  )
```
```{r, echo=FALSE, warning=FALSE, message=FALSE}
par(mfrow = c(1,2))

auc.lasso <- roc(data.val$Churn, as.numeric(lasso.predict), 
                 plot = TRUE, main = "ROC Curve Lasso Model", col = "purple", lwd = 3, 
                 auc.polygon = TRUE, print.auc = TRUE)

auc.ridge <- roc(data.val$Churn, as.numeric(ridge.predict), 
                 plot = TRUE, main = "ROC Curve Ridge Model", col = "black", lwd = 3, 
                 auc.polygon = TRUE, print.auc = TRUE)
```
```{r, echo=FALSE, warning=FALSE, message=FALSE}
grid.arrange(lasso.plot, ridge.plot, top = "Penalized Approaches for Logistic Regression")

lasso_df <- data.frame(Model = "Logistic Lasso",
                       Accuracy = lasso.metrics$Accuracy,
                       Precision = lasso.metrics$Precision,
                       Recall = lasso.metrics$Recall,
                       F1_Score = lasso.metrics$F1,
                       AUC = as.numeric(auc.lasso$auc))

ridge_df <- data.frame(Model = "Logistic Ridge",
                       Accuracy = ridge.metrics$Accuracy,
                       Precision = ridge.metrics$Precision,
                       Recall = ridge.metrics$Recall,
                       F1_Score = ridge.metrics$F1,
                       AUC = as.numeric(auc.ridge$auc))
```
Now let's compare the metrics obtained with the different models:
```{r, echo=FALSE}
comparison_df <- rbind(comparison_df, ridge_df, lasso_df)
comparison_df
```
Our baseline logistic regression model demonstrates moderate performance across various metrics:
- following analyses under different approaches, accuracy ranges from 78.54% to 83.69%;
- precision consistently hovers around 76%, indicating a high percentage of correctly predicted positive cases;
- recall is lower, ranging from 36% to 43%, suggesting room for improvement in capturing all positive cases;
- the F1 score, a balanced metric, varies from 0.49 to 0.50. AUC values, indicating model discrimination, range from 0.73 to 0.83.
Overall, the model performs reasonably well but could benefit from increased recall to capture more positive cases.

### Main conclusions

## Task 2 {.tabset}

Cluster customers according to their behavior

### Feature Engineering

### Kmeans

### Hierarchical Clustering
