---
title: "Data Analysis for Business - Final Project"
output:
  html_document: default
---

# Standard Deviated Group {.tabset .tabset-fade}

Group members:

-   Edoardo Cocciò - 282401
-   Alexandra Tabarani - 282091
-   Marta Torella - 284091
-   Simone Filosofi - 284531

## Task 1 {.tabset .tabset-fade}

The dataset we are going to analyze is the "Telecom Churn" dataset, which contains information about US Telecom. For this first task, our objective is to predict customer churn, enabling the company to implement preventative measures to retain clients; as a matter of fact, by identifying customers at high risk of churn, the company can proactively offer targeted promotions to improve retention rates.

```{r message=FALSE, warning=FALSE, include=FALSE}
library(ggplot2)
library(corrplot)
library(mapproj)
library(dplyr)
library(tidyr)
library(skimr)
library(treemapify)
library(stringr)
library(glue)
library(dummy)
library(caret)
library(car)
library(pROC)
library(glmnet)
library(rpart)
library(rpart.plot) 
library(randomForest)
library(pROC)
library(cluster)
library(factoextra)
library(sf)
library(latex2exp)
library(gridExtra)
library(tree)
library(randomForest)
library(xgboost)
library(dendextend)
library(purrr)
library(ROSE)

data <- read.csv("TelecomChurn.csv")
```

### EDA

We start by performing an exploratory data analysis (EDA) to understand the structure of the dataset and the relationships between the variables. Starting with a general overview of the dataset, we can see that it contains 3333 observations and 20 variables. The variables are a mix of numerical and categorical variable; we proceed by factoring the categorical variables in order to better deal with them later.

```{r echo=FALSE}
names(data)

data$State = as.factor(data$State)
data$International.plan = as.factor(data$International.plan)
data$Voice.mail.plan = as.factor(data$Voice.mail.plan)
data$Churn = as.factor(data$Churn)

continuous_vars <- data[, sapply(data, is.numeric)]
categorical_vars <- data[, sapply(data, is.factor) & !names(data) %in% "Churn"]
```

#### Missing Values and Duplicates

```{r}
skim(data)
```

```{r, echo=FALSE, results='hide'}
anyDuplicated(data)
```

A crucial step in the data analysis process is to check for missing values and duplicates in the dataset to ensure that the data is clean and ready for analysis; in our case we have no missing values and no duplicates. After ensuring dataset integrity, we proceed with the analysis.


#### Univariate and Bivariate Analysis

We proceed by conducting an univariate analysis of the variables in our dataset: our first step is to check the distribution of the target variable 'Churn'; indeed, we identified the variable 'Churn' as the target of our analysis since, as previously stated, the goal of our first task is to build a model to predict customer churn. As shown by the below pie chart,the distribution of the target variable is unbalanced, with 85.5% of the customers not churning and 14.5% churning.

```{r echo=FALSE, fig.align='center', out.width="50%"}
churn_percent <- data %>%
  group_by(Churn) %>%
  summarise(Count = n(), .groups = "drop") %>%
  mutate(Percent = Count / sum(Count) * 100)
```

```{r echo=FALSE, fig.show ="hold", out.width="50%"}
churn_percent <- data %>%
  group_by(Churn) %>%
  summarise(Count = n(), .groups = "drop") %>%
  mutate(Percent = Count / sum(Count) * 100)
ggplot(churn_percent, aes(x = "", y = Percent, fill = Churn)) +
  geom_col(width = 1, color = 1) +
  coord_polar("y", start = 0) +
  theme_void() +
  scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
  labs(title = "Percentage of Churn", fill = "Churn") +
  geom_text(aes(label = paste0(round(Percent, 1), "%")), position = position_stack(vjust = 0.5)) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 18))
long_data <- categorical_vars %>%
  pivot_longer(cols = c("International.plan", "Voice.mail.plan"), names_to = "Category", values_to = "Value")
ggplot(long_data, aes(x = Value, fill = Value)) +
  geom_bar(color = "black") +
  facet_wrap(~ Category, scales = "free_x", nrow = 2, ncol = 2) +
  labs(title = "Distribution of Categorical Variables", x = NULL, y = "Count") +
  theme_minimal() +
  scale_fill_manual(values = c("No" = "#ffe8cc", "Yes" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
  theme(
    strip.text = element_text(size = 12),
    axis.text.x = element_text(angle = 90, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    legend.position = "none"
  ) 
```

Another remarkable imbalance can be found in the distribution of both **International.plan** and **Voice.mail.plan**. The majority of customers do not have an international plan nor a voice mail plan; this might suggest that these services are not very popular among customers.

We now proceed with the univariate analysis of the numerical variables in the dataset. We will plot the distribution of the numerical variables by the target variable 'Churn' to see if there are any differences in the distribution of the numerical variables between the two levels of the target variable.

```{r, echo=FALSE, fig.align='center', fig.width=10, fig.height=10}

continuous <- data[, sapply(data, is.numeric) | names(data) == "Churn"]

continuous %>%
  pivot_longer(cols = -Churn, names_to = "metric", values_to = "value") %>%
  ggplot(aes(value, color = as.factor(Churn))) +
  geom_density(alpha = 0.3) +
  facet_wrap(vars(metric), scales = "free") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  scale_color_manual(name = "Churn", labels = c("No", "Yes"), values = c("#669bbc", "#ff8787")) +
  labs(title = "Numeric Features", subtitle = "Univariate Distribution by Churn") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 18, colour = "gray5"),
    plot.subtitle = element_text(hjust = 0.5, colour = "gray5"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    legend.position = "top"
  )
```

The above univariate analysis of the numerical variables shows that the variables distribution differs remarkably for the two levels of the target variable "Churn" in a couple of cases. Indeed, there's a noticeable difference in the distribution of **customer service calls**: customers who churned seem to make more customer service calls, which could indicate a relationship between customer satisfaction and churn. It also appears that customers who churned tend to have slightly higher **total day minutes** and **total day charges**; this might suggest that customers with higher usage during the day are more likely to churn. The other variables do not show a significant difference in distribution between the two levels of the target variable.

Now we decided to further investigate the relationship between the **Customer.service.calls** variable and the target variable **Churn**. The below graph seems to confirm our initial intuition: the churn rate increases as the number of customer service calls increases. The graph illustrates that most customers who do not churn typically make few customer service calls, peaking at 0-1 calls and then sharply decreasing. Conversely, churn rates are initially low and remain steady, even as the number of calls increases, suggesting that those who make more than five calls are less likely to churn. 

```{r, echo=FALSE, fig.align='center', fig.width=10, fig.height=5}
data %>%
  group_by(Customer.service.calls, Churn) %>%
  summarise(Count = n(), .groups = "drop") %>%
  ggplot(aes(x = Customer.service.calls, y = Count, color = Churn)) +
  geom_line() +
  geom_point() +
  labs(title = "Churn by Customer Service Calls", x = "Customer Service Calls", y = "Count", color = "Churn") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

To further prove our hypothesis, we decided to perform the hypothesis testing to determine if the number of customer service calls is associated with the churn rate:

- H0: the average number of customer service calls is the same for customers who churned and those who did not
- H1: the average number of customer service calls is different for customers who churned and those who did not

```{r, warning=FALSE, message=FALSE}
chisq.test(data$Customer.service.calls, data$Churn)
```
Since the p-value < 0.05, we reject the null hypothesis; therefore the average number of customer service calls is different for customers who churned and those who did not, which confirms our first assumption.

We now proceed with some more hypothesis testing: we explore the relationship between the **Voice.mail.plan** variable and the target variable **Churn**:

H0: the average churn rate is the same for customers who have a voice mail plan and those who do not

H1: the average churn rate is different for customers who have a voice mail plan and those who do not

```{r, warning=FALSE, message=FALSE}
chisq.test(data$Voice.mail.plan, data$Churn)
```

Since the p-value = 5.151e-09 < 0.05, we reject the null hypothesis; therefore the average churn rate is different for customers who have a voice mail plan and those who do not. Hence we reject the null hypothesis, confirming that the average churn rate is different for customers who have a voice mail plan compared to those who do not.

Lastly, we investigate the relationship between the **International.plan** variable and the target variable **Churn**:

H0: the average churn rate is the same for customers who have an international plan and those who do not

H1: the average churn rate is different for customers who have an international plan and those who do not

```{r}
chisq.test(data$International.plan, data$Churn)
```

The p-value is less than 2.2e-16, indicating a statistically significant association between having an international plan and customer churn. Therefore, we reject the null hypothesis, confirming that the average churn rate is different for customers who have an international plan compared to those who do not.

To conclude, for the the analysis of categorical variables, we also analysed the distribution of the **State** variable; the below plot shows the distribution of customers across different states in the US.



```{r message=FALSE, warning=FALSE, include=FALSE}
us_map <- map_data("state") %>% 
  as_tibble()
us_map %>%
  ggplot(aes(long, lat, map_id = region)) + 
  geom_map(
    map =us_map,
    color = "gray80",
    fill = "gray30",
    size = 0.3
  ) + 
  coord_map("ortho", orientation = c(39, -98, 0)) + 
  labs(
    title = "US Map",
    subtitle = "Based on Latitude and Longitude"
  ) + theme(plot.title = element_text(hjust=0.5))
```


```{r echo=FALSE, fig.align="center", fig.height=9, fig.width=9, message=FALSE, warning=FALSE}
state_names <- c(KS = "kansas", OH = "ohio", NJ = "new jersey", OK = "oklahoma",
                 AL = "alabama", MA = "massachusetts", MO = "missouri", LA = "louisiana",
                 WV = "west virginia", IN = "indiana", RI = "rhode island", IA = "iowa",
                 MT = "montana", NY = "new york", ID = "idaho", VT = "vermont",
                 VA = "virginia", TX = "texas", FL = "florida", CO = "colorado",
                 AZ = "arizona", SC = "south carolina", NE = "nebraska", WY = "wyoming",
                 HI = "hawaii", IL = "illinois", NH = "new hampshire", GA = "georgia",
                 AK = "alaska", MD = "maryland", AR = "arkansas", WI = "wisconsin",
                 OR = "oregon", MI = "michigan", DE = "delaware", UT = "utah",
                 CA = "california", MN = "minnesota", SD = "south dakota", NC = "north carolina",
                 WA = "washington", NM = "new mexico", NV = "nevada", DC = "district of columbia",
                 KY = "kentucky", ME = "maine", MS = "mississippi", TN = "tennessee",
                 PA = "pennsylvania", CT = "connecticut", ND = "north dakota")

churn.tbl <- data %>% select(State, Churn) %>%
  group_by(State) %>%
  summarize(
    total_customers = n(),
    churned_customers = sum(Churn == "True"),
    churn_rate = (churned_customers / total_customers),
    churn_rate_txt = scales::percent(churn_rate)
  ) %>%
  mutate(full_state = state_names[as.character(State)]) %>%
  ungroup() %>% 
  left_join(us_map, by=c("full_state" = "region"))

us_map_sf <- st_as_sf(us_map, coords = c("long", "lat"), crs = 4326, agr = "constant")

centroids <- us_map_sf %>%
  group_by(region) %>%
  summarise(geometry = st_centroid(st_union(geometry)))

centroids_df <- as.data.frame(st_coordinates(centroids))
centroids_df$region <- centroids$region

churn.tbl <- churn.tbl %>%
  left_join(centroids_df, by = c("full_state" = "region"))

churn.tbl %>%
  ggplot(aes(long, lat)) + 
  geom_map(
    aes(map_id = full_state),
    map = us_map,
    color = "gray86",
    fill = "gray30",
    size = 0.3
  ) + coord_map("ortho", orientation = c(39, -98, 0)) + geom_polygon(aes(group = group, fill = churn_rate), color="black") +
  scale_fill_gradient2("",low = "#18BC9C", mid = "white", high = "#E31A1C", midpoint = 0.10, labels = scales::percent) + 
  geom_text(aes(x = X, y = Y, label = State), stat = "unique", size = 3, inherit.aes = FALSE, fontface = "bold") +
  theme_void() + 
  theme(
    plot.title = element_text(size=18, face="bold", color = "#2C3E50"),
    legend.position = "right"
  ) + 
  labs(
    title = "Churn Rate Across US States",
    x = "",
    y = ""
  ) + 
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
  ) 
```

The above map displaying churn rates across US states reveals significant regional variations, with higher churn rates observed particularly in Western states like California and Arizona, as well as Southern states such as Texas and Florida. These patterns suggest potential market opportunities for businesses to implement targeted customer retention strategies or competitive market entries. Lower churn rates in other states could indicate effective localized strategies or stronger customer loyalty, serving as benchmarks for regions with higher churn rates. By understanding these geographic differences, companies can tailor their approaches to enhance customer satisfaction and retention, adjust their marketing strategies, and set realistic performance benchmarks tailored to regional market dynamics.

To further investigate the relationship between state and churn in a more rigorous way, we perform a chi-squared test of independence

- **H0**: State and Churn are independent
- **H1**: State and Churn are dependent

```{r echo=FALSE, message=FALSE, warning=FALSE}
contingency.table <- table(data$State, data$Churn)
chisq.test(contingency.table)
```
The p-value is significant, therefore we reject the null hypothesis and conclude that state influences the churn rate, as it was previously hinted by the chart

#### Correlation analysis

```{r, echo=FALSE, fig.align='center'}
cor_mat <-
  data %>% 
  select(where(is.numeric)) %>% 
  cor()

corrplot(
  main = "\n\nCorrelation Matrix",
  cor_mat,
  method = "color",
  order = "alphabet",
  type = "lower",
  diag = FALSE,
  number.cex = 0.6,
  tl.cex = 0.6,
  tl.srt = 45,
  cl.pos = "b",
  addgrid.col = "white",
  addCoef.col = "white",
  col = COL1("Purples"),
  bg="gray",
  tl.col = "grey50"
)
```

The correlation matrix shows that there is a **perfect correlation between the charge variables and minutes variables** so we proceed to drop the following variables to avoid multicollinearity issues in the next section: **Total.day.minutes, Total.eve.minutes, Total.night.minutes, Total.intl.minutes**.

```{r echo=FALSE}
data <- data %>%
  select(-c(Total.day.minutes, Total.eve.minutes, Total.night.minutes, Total.intl.minutes))
```

### Lower-dimensional models

Before proceeding with the full model we need to focus on some lower dimensional model in order to investigate some interesting relationships between the variables. We decided to proceed by implementing a logistic regression model to understand the relationship between the target variable **"Churn"** and the **Customer Service Calls** variables: indeed, we want to understand if the number of customer service calls influences the churn rate.

```{r}
service_calls <- data %>%
  filter(Customer.service.calls > 0)
```

We start by visualizing the proportion of churn by the number of customer service calls:

```{r, echo=FALSE, fig.align='center'}
churn_percent <- service_calls %>%
  group_by(Customer.service.calls, Churn) %>%
  summarise(Count = n(), .groups = "drop") %>%
  mutate(Percent = Count / sum(Count) * 100)

ggplot(churn_percent, aes(x = Customer.service.calls, y = Percent, fill = Churn)) +
  geom_col(width = 0.7, color = 1) +
  labs(title = "Churn by Customer Service Calls", x = "Customer Service Calls", y = "Percent", fill = "Churn") +
  theme_minimal() +
  scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "bottom"
  )
```

We can see that the churn rate increases as the number of customer service calls increases. Just by looking at the above plot we get the intuition that customers who make more service calls are more likely to churn.

Hence, we proceed by fitting the logistic regression model to understand the relationship between the number of customer service calls and the churn rate:

```{r}
set.seed(1)
data$Churn <- as.factor(data$Churn)
model <- glm(Churn ~ Customer.service.calls, family = binomial, data = data)
summary(model)
```

The output shows a logistic regression analysis where the likelihood of customer churn increases significantly with each additional customer service call, as indicated by the positive coefficient of 0.39617 (p \< 2e-16). The model fits the data reasonably with an AIC of 2631.2, suggesting that customer service calls are a relevant predictor of churn. Moreover, the intercept's p-value (\<2e-16) suggests that there is a statistically significant effect of having zero customer service calls on the likelihood of churn and that there is a statistically significant association between the number of customer service calls and an increased likelihood of churn.

```{r, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, results='hide'}
call_range <- seq(min(data$Customer.service.calls, na.rm = TRUE), 
                  max(data$Customer.service.calls, na.rm = TRUE), 
                  length.out = 100)
newdata <- data.frame(Customer.service.calls = call_range)
newdata$Probability <- predict(model, newdata = newdata, type = "response")
newdata$Odds <- newdata$Probability / (1 - newdata$Probability)

ggplot(newdata, aes(x = Customer.service.calls, y = Probability)) +
  geom_line() + 
  labs(title = "Effect of Customer Service Calls on the Probability of Churn",
       x = "Number of Customer Service Calls",
       y = "Probability of Churn") +
  scale_y_continuous(labels = scales::percent_format()) +  # Convert y-axis into percentage format
  theme_minimal() +  # Use a minimal theme
theme(
  plot.title = element_text(hjust = 0.5), 
  axis.text.x = element_text(angle = 45, hjust = 1), 
  axis.title.x = element_text(face = "bold"),  
  axis.title.y = element_text(face = "bold")  
)
```

By looking at the above graph, there is a clear positive trend showing that as the number of customer service calls increases, the probability of churn also rises significantly. For instance, the probability of churn exceeds 50% as the number of calls approaches approximately 5.5; indeed the graph highlights a critical area for operational improvement.

However, we are dealing with a dataset that is highly imbalanced, with 85.5% of the customers not churning and 14.5% churning. This imbalance can lead to biased models that predict the majority class more accurately than the minority class. To address this issue, we will use the undersampling technique to balance the dataset; that is we reduce the number of observations in the majority class (Non-Churn) to match the number of observations in the minority class (Churn).

```{r}
set.seed(1)
majority_indices <- which(data$Churn == 'False')
minority_count <- sum(data$Churn == 'True')
sampled_indices <- sample(majority_indices, size = minority_count, replace = FALSE)
balanced_data <- data[c(sampled_indices, which(data$Churn == 'True')), ]
balanced_data$Churn <- as.factor(balanced_data$Churn)
balanced_model <- glm(Churn ~ Customer.service.calls, family = binomial, data = balanced_data )
summary(balanced_model)
```

In this balanced model, the intercept is -0.54195, indicating the log-odds of churn when there are zero customer service calls. Each additional customer service call increases the probability of churn by 0.29344 with a highly significant p-value (1.73e-11), suggesting a strong influence of customer service interactions on churn likelihood. Compared to the initial model, which had a coefficient of 0.39617 for customer service calls, the coefficient in the balanced model is somewhat lower, which might indicate that the initial model overestimated the effect of customer service calls due to the imbalance in the dataset. Furthermore, the AIC (Akaike Information Criterion) has notably decreased from 2631.2 in the original model to 1293.9 in the balanced model, indicating a more parsimonious fit.

```{r, echo=FALSE, fig.align='center'}
call_range <- seq(min(data$Customer.service.calls, na.rm = TRUE), 
                  max(data$Customer.service.calls, na.rm = TRUE), 
                  length.out = 100)
newdata <- data.frame(Customer.service.calls = call_range)
newdata$Probability <- predict(balanced_model, newdata = newdata, type = "response")
newdata$Odds <- newdata$Probability / (1 - newdata$Probability)
ggplot(newdata, aes(x = Customer.service.calls, y = Probability)) +
  geom_line() + 
  labs(title = "Effect of Customer Service Calls on the Probability of Churn (Balanced Data)",
       x = "Number of Customer Service Calls",
       y = "Probability of Churn") +
  scale_y_continuous(labels = scales::percent_format()) +  # Convert y-axis into percentage format
  theme_minimal() +  # Use a minimal theme
  theme(
    plot.title = element_text(hjust = 0.5), 
    axis.text.x = element_text(angle = 45, hjust = 1), 
    axis.title.x = element_text(face = "bold"),  
    axis.title.y = element_text(face = "bold")  
  )
```

The above curve shows a steady increase in the probability of churn as the number of customer service calls rises, beginning from below 50% and reaching towards 90% as the number approaches 7.5 calls; this graph clearly demonstrates how increased customer service interactions are strongly correlated with higher churn rates, emphasizing the need for addressing issues that lead to repeated customer service engagements.

Finally we compare the two confusion matrices of the balanced and unbalanced models:

```{r, echo=FALSE}
confusion.mat <- function(data, model, target, threshold) {
  predicted_Y <- ifelse(predict(model, type = "response", newdata = data) > threshold, 'True', 'False')
  actual_Y <- data[[target]]
  True.positive <- sum(predicted_Y == 'True' & actual_Y == 'True')
  True.negative <- sum(predicted_Y == 'False' & actual_Y == 'False')
  False.positive <- sum(predicted_Y == 'True' & actual_Y == 'False')
  False.negative <- sum(predicted_Y == 'False' & actual_Y == 'True')
  Confusion.Matrix <- matrix(c(True.positive, False.negative, 
                               False.positive, True.negative),
                             nrow = 2, byrow = TRUE)
  rownames(Confusion.Matrix) <- c("Actual Positive", "Actual Negative")
  colnames(Confusion.Matrix) <- c("Predicted Positive", "Predicted Negative")
  print(Confusion.Matrix)
}

confusion.mat(data, model, "Churn", 0.5) 
confusion.mat(balanced_data,balanced_model, "Churn", 0.5) 
```

In the unbalanced dataset, the model struggles to correctly identify positive cases, predicting only 8 as positive (true positives) and incorrectly labeling 5 negatives as positive (false positives). In contrast, the balanced model demonstrates better performance in identifying positive cases (269 true positives) but at the cost of more false positives (203). Indeed, this comparison underscores the impact of using balanced data on model performance, particularly in improving the detection of positive cases, which is critical in scenarios like customer churn prediction.

We further proceeded in exploring the interactions between other variables such as the **International.plan** and **Voice.mail.plan** with the target variable **Churn**.

```{r}
set.seed(1)
data$Churn <- as.factor(data$Churn)
model <- glm(Churn ~ International.plan*Voice.mail.plan, family = binomial, data = data)
summary(model)
```

The intercept is very significant with a p-value \< 2e-16, indicating a strong effect when both plans are absent. Moreover, both International.planYes and Voice.mail.planYes are highly significant. For the interaction term, while international plans are associated with higher churn, voice mail plans seem to mitigate churn risk. However, customers who have both plans are particularly at risk of churning, possibly due to the higher costs or complexities associated with managing multiple service features.

The last association we investigated was the relationship between the **Customer.service.calls** and **International.plan** variables with the target variable **Churn**.

```{r}
set.seed(1)
data$Churn <- as.factor(data$Churn)
model <- glm(Churn ~ Customer.service.calls*International.plan, family = binomial, data = data)
summary(model)
```

Both International.planYes and Customer.service.calls are highly significant. The interaction term shows that customers with more customer service interactions generally have higher churn rates. However, the negative interaction indicates that the churn-increasing effect of service calls might be mitigated among customers with an international plan, possibly due to different expectations or experiences.

**CONCLUSION UP TO NOW:**

-   we dropped the *minutes variables* because they are *perfectly correlated with the charge variables*;

-   we saw that the *churn rate* is *higher* for customers that have an *international plan*;

-   we saw that the churn rate is *higher* for customers that do *not have a voice mail plan*;

-   we saw that the churn rate is *higher* for customers that *have a higher number of customer service calls*;

-   we saw that the churn rate is *higher* for customers that *have higher total day minutes and total day charge*;

-   we saw that the churn rate is *higher* for customers that *have higher total eve charge, night charge, and intl charge*;

-   we saw that *State* also *influences the churn rate*;

### Preprocessing and feature engineering

In proceeding with our analysis we should first deal with the 51 states: we can group them by region; mid west, north east, south east, south west, west.
This is to **avoid high-cardinality** encoded columns. 

```{r}
map_region <- function(state) {
  west.regions <- c("WA", "OR", "ID", "MT", "WY", "CA", "NV", "UT", "CO", "AK", "HI")
  southwest.regions <- c("AZ", "NM", "TX", "OK")
  midwest.regions <- c("ND", "SD", "NE", "KS", "MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH")
  southeast.regions <- c("AR", "LA", "MS", "AL", "TN", "KY", "WV", "VA", "NC", "SC", "GA", "FL")
  northeast.regions <- c("ME", "NH", "VT", "MA", "RI", "CT", "NY", "PA", "NJ", "DE", "MD", "DC")
  
  if (state %in% west.regions) {
    return("West")
  } else if (state %in% midwest.regions) {
    return("Mid West")
  } else if (state %in% southeast.regions) {
    return("South East")
  } else if (state %in% northeast.regions) {
    return("North East")
  } else {
    return("South West")  # in case there are any states not covered
  }
}

data$Region <- sapply(data$State, map_region)
data$Region <- as.factor(data$Region)
```

We'll use the regions column for the first models to avoid having high-cardinality encoded columns
```{r}
state.column <- data$State
data$State <- NULL
```

Now let's re-do the chi square:

```{r}
contingency.table <- table(data$Region, data$Churn)
chisq.test(contingency.table)
```

H0: Region and Churn are independent, H1: Region and Churn are dependent. 
Since p-value \> 0.05, we fail to reject the null hypothesis, therefore the region does not influence the churn rate as the State variable did. 
Let's see the distribution of churn by region:

```{r, echo=FALSE, fig.align='center'}
ggplot(data, aes(x = Region, fill = Churn)) + 
  geom_bar(width = 0.7, color = 1) +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust = 0, colour = "gray29", size = 10)) +
  labs(title = "Churn by Region", x = "Region", y = "Count", fill = "Churn", subtitle = "Is Churn rate influenced by the Region?") +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank(),
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5)
  ) +
  scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) 
```

We can see, also from the plot, that the churn rate is not influenced by the region.

#### Splitting the data and scaling

```{r}
set.seed(1)
ids.train <- sample(1:nrow(data), size = 0.75 * nrow(data), replace = FALSE)
data.train <- data[ids.train,]
data.val <- data[-ids.train,] 
```

In this section we oversample the minority class to balance the dataset, and as a result now we have 4260 total observations.
We experimented both undersampling and oversampling, but we found that the latter yielded significantly better results.

```{r}
data.train.balanced <- ovun.sample(Churn ~ ., data = data.train, method = "over", N=4260)$data
table(data.train.balanced$Churn)
```

In order to to prevent certain columns from dominating the model, we scale our data:

```{r}
cols_to_scale <- c("Account.length", "Area.code", "Number.vmail.messages", "Total.day.calls", "Total.day.charge", "Total.eve.calls", "Total.eve.charge", "Total.night.calls", "Total.night.charge", "Total.intl.calls", "Total.intl.charge", "Customer.service.calls")

train.mean <- apply(data.train.balanced[, cols_to_scale], MARGIN = 2, FUN = mean)
train.sd <- apply(data.train.balanced[, cols_to_scale], MARGIN = 2, FUN = sd)

data.train.balanced[, cols_to_scale] <- scale(data.train.balanced[, cols_to_scale], center = train.mean, scale = train.sd)
data.val[, cols_to_scale] <- scale(data.val[, cols_to_scale], center = train.mean, scale = train.sd)
```
### Models Comparison
#### Baseline Logistic Regression Model

It is finally time to start the analysis with our first full model. We'll begin by employing a logistic regression model as our baseline. This approach is straightforward and effective for predicting binary outcomes, which makes it an ideal starting point. By using logistic regression as our benchmark, we can clearly assess the performance improvements offered by more complex models. As we proceed, we'll compare this baseline model's metrics against more sophisticated algorithms to determine whether the increased complexity justifies the potential gains in prediction. This step-by-step comparison helps us identify which models enhance our analytical capabilities and which may not be worth the additional complexity.

```{r}
logistic.baseline <- glm(Churn ~ ., data = data.train.balanced, family = "binomial")
```

Our baseline logistic regression model effectively captures the main trends affecting customer churn, highlighting the significant roles played by factors like international plans, voicemail services, and customer service calls. The model’s performance is pretty decent with an accuracy of 83.69%, although its precision at 59.69% and recall at 43.08% suggest there’s room for improvement, especially in correctly identifying all actual cases of churn. The F1 score of 0.5 indicates that the balance between precision and recall could be better. This is normal since we still have to perform feature selection and regularization to improve the model.

```{r, echo=FALSE}
baseline.pred <- ifelse(predict(logistic.baseline, newdata = data.val) > 0.5, 1, 0)
(baseline.cm <- table(baseline.pred, data.val$Churn))

get.metrics<- function(conf.mat) {
  true.positives <- conf.mat[2,2]
  true.negatives <- conf.mat[1,1]
  false.positives <- conf.mat[1,2]
  false.negatives <- conf.mat[2,1]
  num.observations <- true.positives + true.negatives + false.positives + false.negatives
  
  accuracy <- (true.positives + true.negatives) / num.observations
  precision <- (true.positives) / (true.positives + false.positives)
  recall <- true.positives / (true.positives + false.negatives)
  f1 <- 2 * ((precision * recall) / (precision + recall))
  
  metrics <- data.frame(t(c(accuracy, precision, recall, f1)))
  columns <- c("Accuracy", "Precision", "Recall", "F1")
  colnames(metrics) <- columns
  
  return(metrics)
}

(baseline.metrics <- get.metrics(baseline.cm))
```

#### AIC and BIC Model Selection
We'll now proceed with further refining our model by conducting AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) analysis. 

We performed the **Akaike Information Criterion** (AIC) stepwise selection to identify the most important predictors in the model:

```{r, results='hide'}
akaike.fw <- step(glm(Churn ~ 1, family = "binomial", data = data.train.balanced), scope = formula(logistic.baseline), direction = "forward")
akaike.back <- step(logistic.baseline, direction = "backward")
akaike.both <- step(logistic.baseline, direction = "both")
```

The AIC analysis results highlight how each variable affects the model's complexity and fit. For example, variables like `Total.day.charge` and `International.plan` show a substantial increase in the AIC when removed, indicating they are highly significant to the model's accuracy. Conversely, variables like `Account.length` and `Area.code` have minimal impact on AIC, suggesting they contribute less to model performance. 

**Bayesian Information criterion** (BIC) is another criterion to select the best model. We will use the BIC to compare the models selected by the AIC:

```{r, results='hide'}
bayesian.fw <- step(glm(Churn ~ 1, family = "binomial", data = data.train.balanced), scope = formula(logistic.baseline), direction = "forward", k = log(nrow(data.train.balanced)))
bayesian.back <- step(logistic.baseline, direction = "backward", k = log(nrow(data.train.balanced)))
bayesian.both <- step(logistic.baseline, direction = "both", k = log(nrow(data.train.balanced)))
```

Considering bidirectional elimination of both methods, BIC is more stringent than AIC: it removes all region-related variables, favoring a simpler model. This stricter approach by BIC contrasts with AIC's tendency to keep more variables for a better fit. 

After that we compare the validation set results:

The results show that the model selected by the AIC method performs slightly better than the one selected by the BIC method. The AIC model has a higher accuracy, precision, recall, and F1 score: maybe dropping all the region features as BIC does is too big of a loss of informations.

**AIC**:
Features included in the model:
```{r}
formula(akaike.both)
```


```{r, echo=FALSE}
akaike.preds <- ifelse(predict(akaike.both, data.val) > 0.5, 1, 0)
(akaike.cm <- table(akaike.preds, data.val$Churn))
(akaike.metrics <- get.metrics(akaike.cm))
```

**BIC**:
Features included in the model:
```{r}
formula(bayesian.both)
```

```{r, echo=FALSE}
bayesian.preds <- ifelse(predict(bayesian.both, data.val) > 0.5, 1, 0)
(bayesian.cm <- table(bayesian.preds, data.val$Churn))
(bayesian.metrics <- get.metrics(bayesian.cm))
```

#### ROC Curves

The AUC-ROC curve is a graphical representation of the model's ability to discriminate between positive and negative classes. We plot the ROC curves for the full model, the AIC model, and the BIC model to compare their performance:

-   The full model has an AUC of 0.726 and it's clearly the model with the lowest discrimination ability;

-   The AIC model (AUC of 0.825) and the BIC model (AUC of 0.834) have two very similar AUCs, but the BIC model has a slightly better performance.

```{r echo=FALSE, fig.align='center', fig.height=4, fig.width=10, message=FALSE, warning=FALSE, results="hide"}
par(mfrow = c(1, 3))

roc_full <- capture.output(roc(data.val$Churn, as.numeric(baseline.pred), 
                plot = TRUE, main = "ROC Curve Full Model", col = "purple", lwd = 3, 
                auc.polygon = TRUE, print.auc = TRUE), type = "message")

roc_akaike <- capture.output(roc(data.val$Churn, predict(akaike.both, newdata = data.val, type = "response"), 
                  plot = TRUE, main = "AIC Model", col = "blue", lwd = 3, 
                  auc.polygon = TRUE, print.auc = TRUE), type = "message")

roc_bayesian <- capture.output(roc(data.val$Churn, predict(bayesian.both, newdata = data.val, type = "response"), 
                    plot = TRUE, main = "BIC Model", col = "red", lwd = 3, 
                    auc.polygon = TRUE, print.auc = TRUE), type = "message")

par(mfrow = c(1, 1))
```


```{r, echo=FALSE, results='hide'}
auc_akaike <- as.numeric(auc(roc(data.val$Churn, predict(akaike.both, newdata = data.val, type = "response"))))
auc_bayesian <- as.numeric(auc(roc(data.val$Churn, predict(bayesian.both, newdata = data.val, type = "response"))))
auc_full <- as.numeric(auc(roc(data.val$Churn, baseline.pred)))

```

```{r, echo=FALSE, results='hide'}
# comparison (for now) between full, aic, bic 
akaike_df <- data.frame(Model = "Akaike",
                        Accuracy = akaike.metrics$Accuracy,
                        Precision = akaike.metrics$Precision,
                        Recall = akaike.metrics$Recall,
                        F1_Score = akaike.metrics$F1,
                        AUC = auc_akaike)

bayesian_df <- data.frame(Model = "Bayesian",
                          Accuracy = bayesian.metrics$Accuracy,
                          Precision = bayesian.metrics$Precision,
                          Recall = bayesian.metrics$Recall,
                          F1_Score = bayesian.metrics$F1,
                          AUC = auc_bayesian)

full_df <- data.frame(Model = "Full Logistic",
                      Accuracy = baseline.metrics$Accuracy,
                      Precision = baseline.metrics$Precision,
                      Recall = baseline.metrics$Recall,
                      F1_Score = baseline.metrics$F1,
                      AUC = auc_full)

comparison_df <- bind_rows(akaike_df, bayesian_df, full_df)
comparison_df
```

#### LASSO and RIDGE REGULARIZATION

We are now exploring Lasso and Ridge regression techniques. Lasso (Least Absolute Shrinkage and Selection Operator) helps in enhancing model accuracy and interpretability by reducing less important features to zero. Ridge regression, on the other hand, minimizes the influence of less important features through shrinkage but keeps all variables in the model. 

We decided to leverage both LASSO and Ridge regularization methods to try to fine-tune our logistic regression model, aiming to enhance its predictive capabilities.

```{r}
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)
lasso <- train(Churn ~ ., data = data.train.balanced, method = "glmnet", metric = "Accuracy", trControl = ctrl, tuneGrid = expand.grid(alpha = 1, lambda = seq(0, 0.15, length = 30)))
max(lasso$results$Accuracy)
lasso$bestTune
```

```{r include=FALSE}
lasso.predict <- predict(lasso, data.val)
lasso.cm <- table(lasso.predict, data.val$Churn)
lasso.metrics <- get.metrics(lasso.cm)
```

After tuning over a grid of parameters, the highest achieved accuracy is 77.42%. The optimal regularization parameters selected are alpha = 1 and lambda = 0, indicating LASSO regularization without penalty.

```{r, echo=FALSE}
lasso.plot <- lasso %>% 
  ggplot(aes(x = lambda, y = Accuracy)) + 
  geom_line() + 
  geom_point() +
  geom_text(aes(label = sprintf("%.3f", Accuracy)), check_overlap = TRUE, vjust = -0.5, size = 2.5) + 
  scale_x_continuous(limits = c(0, 0.10)) + 
  labs(x = TeX("Lambda ($\\lambda$)"), y = "Accuracy", title = "Accuracy vs. Lambda for Lasso Regularization") +
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold")
  )
```

**RIDGE**:


```{r}
set.seed(1)
ridge <- train(Churn ~ ., data = data.train.balanced, method = "glmnet", metric = "Accuracy", trControl = ctrl, tuneGrid = expand.grid(alpha = 0, lambda = seq(0, 0.15, length = 30)))
max(ridge$results$Accuracy)
ridge$bestTune
```

```{r include=FALSE}
ridge.predict <- predict(ridge, data.val)
ridge.cm <- table(ridge.predict, data.val$Churn)
ridge.metrics <- get.metrics(ridge.cm)
```


Using the Ridge regularization technique the best-performing parameters selected are alpha = 0 and lambda = 0.01034, indicating Ridge regularization almost without penalty.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

ridge.plot <- ridge %>% 
  ggplot(aes(x = lambda, y = Accuracy)) + 
  geom_line() + 
  geom_point() +
  geom_text(aes(label = sprintf("%.3f", Accuracy)), check_overlap = TRUE, vjust = -0.5, size = 2.5) +
  labs(x = TeX("Lambda ($\\lambda$)"), y = "Accuracy", title = "Accuracy vs. Lambda for Ridge Regularization") +
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold")
  )
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center" , fig.height=4, fig.width=10}
par(mfrow = c(1,2))

auc.lasso <- roc(data.val$Churn, as.numeric(lasso.predict), 
                 plot = TRUE, main = "ROC Curve Lasso Model", col = "purple", lwd = 3, 
                 auc.polygon = TRUE, print.auc = TRUE)

auc.ridge <- roc(data.val$Churn, as.numeric(ridge.predict), 
                 plot = TRUE, main = "ROC Curve Ridge Model", col = "black", lwd = 3, 
                 auc.polygon = TRUE, print.auc = TRUE)
```

As shown in the above plots, the AUC for the Lasso model is 0.778, while the AUC for the Ridge model is 0.776. Both models provide little improvement as they apply little to none regularization to our baseline model.


#### TREES

From now on we are studying other models and comparing the metrics with the logistic regression we chose as baseline model:

-   **Decision trees**

-   **Random Forest**

-   **XGBOOST**

#### Decision Trees

First of all we split again to undo the scaling and to make trees more interpretable for us. Then we oversampled the minority class in order to have a total of 4260 observations as in the previous case.

```{r include=FALSE}
set.seed(1)
ids.train <- sample(1:nrow(data), size = 0.75 * nrow(data), replace = FALSE)
data.train <- data[ids.train,]
data.val <- data[-ids.train,] 

data.train.balanced <- ovun.sample(Churn ~ ., data = data.train, method = "over", N=4260)$data
table(data.train.balanced$Churn)
```

```{r echo=FALSE}
set.seed(1) # otherwise results might be inconsistent due to ties
tree.full <- tree(Churn ~ ., data = data.train.balanced)
par(mar=c(5,5,2,2))  # Adjust margin sizes to avoid the "Error in plot.new() : figure margins too large"
plot(tree.full)
text(tree.full, pretty = 0, cex = 0.7)
```

Computing the confusion matrix we can notice that the model accurately predicts 651 'False' instances and 100 'True' instances, but misclassifies 14 'False' instances and 69 'True' instances. It achieves an accuracy of 90.05%, with a precision of 87.72% and recall of 59.17%.

```{r echo=FALSE}
tree.pred <- predict(tree.full, data.val, type = "class")
table(tree.pred, data.val$Churn)
get.metrics(table(tree.pred, data.val$Churn))
```

##### Pruning the tree

In order to improve the performance and generalization ability of decision trees (to prevent overfitting) we tried a Pruning approach with cross-validation:

```{r echo=FALSE, fig.align = "center"}
cv.trees <- cv.tree(tree.full, FUN = prune.misclass)
#(optimal.size <- cv.trees$size[which.min(cv.trees$dev)])
par(mfrow = c(1,2))
plot(cv.trees$size, cv.trees$dev/nrow(data.val), type = "b", xlab = "Size", ylab = "Error")
plot(cv.trees$k, cv.trees$dev/nrow(data.val), type = "b", col = 3, xlab = "K", ylab = "Error")
```
The left plot shows a sharp decrease in error as the size of the tree increases from 2 to about 5 or 6. Beyond this point, the error stabilizes and remains relatively flat indicating a good tradeoff point between underfitting and overfitting. 
The right plot indicates there is an optimal value of K that minimizes error before the penalty becomes counterproductive due to excessive simplification. We note that the optimal K is 0, so no penalization is applied, we will simply limit the size of the tree.

After pruning, the resulting tree is the following (there are some minor differences with respect to the previous tree):

```{r echo=FALSE}
pruned.tree <- prune.misclass(tree.full, k = 0, best = optimal.size)
plot(pruned.tree)
text(pruned.tree, pretty = 0, cex = 0.7)
```

The predictive performance of the pruned tree is evaluated on the validation dataset, giving as contingency table the below:

```{r echo=FALSE, message=FALSE, warning=FALSE}
pruned.pred <- predict(pruned.tree, data.val, type = "class")
table(pruned.pred, data.val$Churn)
trees.metrics <- get.metrics(table(pruned.pred, data.val$Churn))
trees.auc <- roc(data.val$Churn, as.numeric(pruned.pred))
```

```{r echo=FALSE}
cv_trees_df <- data.frame(Model = "CV Decision Trees",
                          Accuracy = trees.metrics$Accuracy,
                          Precision = trees.metrics$Precision,
                          Recall = trees.metrics$Recall,
                          F1_Score = trees.metrics$F1,
                          AUC = as.numeric(trees.auc$auc))
```

#### Random Forests

Now its the turn of the Random Forest model. Using again a seed set for reproducibility, we train the model on a balanced training dataset, configuring parameters such as the number of trees and variable selection for each split (500).

```{r}
set.seed(1)
fit.bag <- randomForest(Churn ~ ., data = data.train.balanced, mtry = ncol(data.train.balanced) - 1, ntree = 500, importance = T) 
```

```{r echo=FALSE, fig.align='center'}
layout(matrix(c(1,2),nrow=1),
       width=c(4,1)) 
par(mar=c(5,4,4,0)) # No margin right side
plot(fit.bag, log="y")
par(mar=c(5,0,4,2)) # No margin left side
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("bottom", colnames(fit.bag$err.rate),col=1:4,cex=0.8,fill=1:4)
```

In this section, we evaluate our bagged model's performance on the validation dataset. Our aim with this plot is to illustrate the importance of each predictor variable in the random forest model.

```{r echo=FALSE, fig.align='center', message=FALSE}
varImpPlot(fit.bag)
mean(fit.bag$err.rate[,1])
validation.preds <- predict(fit.bag, newdata = data.val)
(bag.missclassification <- mean(validation.preds != data.val$Churn)) 
table(validation.preds, data.val$Churn)
```

```{r echo=FALSE, message=FALSE}
rf.metrics <- get.metrics(table(validation.preds, data.val$Churn))
rf.auc <- roc(data.val$Churn, as.numeric(validation.preds))
```

We utilized two metrics for error estimation: **Out-of-Bag (OOB) test error**, which is approximately **1.34%**, and **true test erro**r, which is approximately **3.23%**. By comparing these figures, we observe that with a larger number of trees, the OOB test error estimation could potentially approach the true test error more closely. Additionally, we examine the contingency table of predicted versus actual outcomes, which illustrates the model's performance. For instance, out of 720 instances predicted as 'False', 709 are actually 'False', while out of 114 instances predicted as 'True', 94 are indeed 'True'.

```{r echo=FALSE}
rf_df <- data.frame(Model = "Random Forests",
                    Accuracy = rf.metrics$Accuracy,
                    Precision = rf.metrics$Precision,
                    Recall = rf.metrics$Recall,
                    F1_Score = rf.metrics$F1,
                    AUC = as.numeric(rf.auc$auc))
```

#### XGBOOST

We're now using XGBoost to build a churn prediction model. First, we'll prepare our data by encoding categorical variables and splitting it into training and validation sets, then, we'll train the XGBoost model with specified parameters.

```{r results='hide', message=FALSE, warning=FALSE}
X.tr <- model.matrix(Churn ~ ., data = data.train.balanced)[, -1]
y.tr <- as.numeric(data.train.balanced$Churn)-1
X.val <- model.matrix(Churn ~ ., data = data.val)[, -1]
y.val <- as.numeric(data.val$Churn)-1
fit.xg <- xgboost(as.matrix(X.tr), label = y.tr, nrounds = 50, objective = "binary:logistic", eval_metric = "error")
```

As shown from the results below, the **XGBoost** model achieves **strong performance** with a 97% of accuracy, a recall of 95% for 'Churn' instances and a precision of 85%. It exhibits low rates of false negatives (18 out of 114) and false positives (7 out of 731).

```{r echo=FALSE, message=FALSE, warning=FALSE}
xg.pred <- ifelse(predict(fit.xg, X.val)> 0.5, 1, 0)
table(xg.pred, data.val$Churn)
get.metrics(table(xg.pred, data.val$Churn))
```

##### Validation error and number of trees

```{r message=FALSE, warning=FALSE, include=FALSE}
train.errors <- fit.xg$evaluation_log$train_error
val.errors <- numeric(50)

for (i in 1:50) {
  pred_i = ifelse(predict(fit.xg, X.val, ntreelimit = i) > 0.5, 1, 0)
  val.errors[i] = mean(pred_i != y.val)
} 
```

```{r echo=FALSE, fig.align = "center"}
plot(1:50, val.errors, type="b", xlab="number of trees", ylab="error", col=3, ylim = c(0,0.3))
points(1:50, train.errors, type="b")
legend("topright", legend = c("Train", "Test"), col=c(1, 3), lty=1, lwd=2, cex = 1)
#which.min(val.errors)
abline(v = which.min(val.errors))
```


The vertical line highlights the number of trees that minimizes the validation error, helping identify the optimal number of trees for the model. In our case the value is around 42.

```{r echo=FALSE, results='hide', warning=FALSE, message=FALSE}
(feature.importance <- xgb.importance(model = fit.xg))
```

##### Feature Importance

The feature importance plot below illustrates the relative importance of each feature in the XGBoost mode:

```{r, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
xgb.plot.importance(importance_matrix = feature.importance)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
fitControl = trainControl(
  method = "cv",
  number = 10, 
  search="random")

tune_grid = 
  expand.grid(
    nrounds = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100),
    eta = 0.3,
    max_depth=5,
    subsample = 1,
    colsample_bytree = 1,
    min_child_weight = 5,
    gamma = c(0.1, 0.2, 0.5, 0.75, 1)
  )

set.seed(1)
fit_xg_cv = train(Churn ~ ., data = data.train.balanced, 
                  method = "xgbTree", 
                  trControl = fitControl,
                  verbose = FALSE, 
                  tuneGrid = tune_grid,
                  objective = "binary:logistic", 
                  eval_metric = "error")
```

Here we can see from the confusion matrix that the model has a good performance, but if compared to the matrix shown at the beginning of the section, we can see that the model improved in the prediction of true positives (96 before, 98 now), but has a slightly worse performance in predicting true negatives (713 before, 707 now).

```{r echo=FALSE, warning=FALSE, message=FALSE}
pred_xg_cv = predict(fit_xg_cv, data.val, type="raw")
#mean(pred_xg_cv != data.val$Churn)
table(pred_xg_cv, data.val$Churn)
xgb.metrics <- get.metrics(table(pred_xg_cv, data.val$Churn))

auc.xgb <- roc(data.val$Churn, as.numeric(pred_xg_cv))

xgb_df <- data.frame(Model = "CV XGB",
                     Accuracy = xgb.metrics$Accuracy,
                     Precision = xgb.metrics$Precision,
                     Recall = xgb.metrics$Recall,
                     F1_Score = xgb.metrics$F1,
                     AUC = as.numeric(auc.xgb$auc))
```

### Main conclusions

#### Main conclusions

To sum up, we have performed a comprehensive analysis of the churn dataset, including data exploration, preprocessing, and feature engineering. We have built a logistic regression model as a baseline and compared its performance with other models, such as decision trees, random forests, and gradient boosting.

```{r, echo=FALSE}
comparison_df <- rbind(comparison_df, cv_trees_df)
comparison_df <- rbind(comparison_df, rf_df)
comparison_df <- rbind(comparison_df, xgb_df)
comparison_df
```

Now to draw the main conclusions from our analysis let's compare the metrics obtained with the different models' final data:

-   **Random Forests**: Random Forests has an accuracy of 96.64%, precision of 82.4%, recall of 93%, F1-score of 87%, and area under the curve (AUC) of 90%. 

-   **Boosting models**: CV XGBoost shows an accuracy of 96%, precision of 86%, recall of 89%, F1-score of 87.5%, and AUC of 92%, demonstrating performance similar to Random Forests and superior to other classification models.

-   **Linear models**: Linear models (Akaike, Bayesian, Full Logistic, Logistic Ridge, Logistic Lasso) exhibit lower performance compared to boosting models and decision trees, with AUC ranging from 72.63% to 83.44%. However, Logistic Ridge and Logistic Lasso show notably higher precision compared to other linear models.

-   **Decision trees**: CV Decision Trees demonstrate good performance with an accuracy of 90.05%, precision of 87.72%, recall of 59.17%, F1-score of 70.67%, and AUC of 89.07%, but are surpassed by boosting models and Random Forests in terms of AUC.

In conclusion, we believe that **XGBoost is the most effective model** for churn prediction, with Random Forests showing slightly better performance in terms of AUC. It exhibits robust performance across all critical metrics. It combines high accuracy with excellent precision, recall, F1 Score, and the highest AUC, making it the most reliable and effective model for predicting customer churn. It balances the trade-offs between different types of prediction errors and shows the greatest ability to distinguish between the classes

## Task 2 {.tabset}

For this second task, we will focus on the **customer segmentation**. We will use clustering techniques (kmeans and hierarchical clustering) to group customers based on their characteristics and behaviour. The goal is to identify different customer segments that can help the company to better understand its customers and tailor its marketing strategies.

### Feature Engineering

Before proceeding with the clustering, we need to preprocess the data and engineer some features. Indeed, we drop the columns that are not useful for the clustering analysis, we create three new features that can help to better identify the customer segments, and lastly, we scale the numerical variables.

```{r, echo=FALSE}
clustering_data <- read.csv("TelecomChurn.csv")
```

```{r, echo=FALSE}
clustering_data <- clustering_data[, !(names(clustering_data) %in% c("Churn", "Area.code", "International.plan", "Voice.mail.plan", "Customer.service.calls", "State", "Account.length"))]
# total_minute
clustering_data$Total.minutes <- clustering_data$Total.day.minutes + clustering_data$Total.eve.minutes + clustering_data$Total.night.minutes + clustering_data$Total.intl.minutes
clustering_data <- clustering_data[, !(names(clustering_data) %in% c("Total.day.minutes", "Total.eve.minutes", "Total.night.minutes", "Total.intl.minutes"))]
# total_charge
clustering_data$Total.charge <- clustering_data$Total.day.charge + clustering_data$Total.eve.charge + clustering_data$Total.night.charge + clustering_data$Total.intl.charge
clustering_data <- clustering_data[, !(names(clustering_data) %in% c("Total.day.charge", "Total.eve.charge", "Total.night.charge", "Total.intl.charge"))]
# total call
clustering_data$Total.calls <- clustering_data$Total.day.calls + clustering_data$Total.eve.calls + clustering_data$Total.night.calls + clustering_data$Total.intl.calls
clustering_data <- clustering_data[, !(names(clustering_data) %in% c("Total.day.calls", "Total.eve.calls", "Total.night.calls", "Total.intl.calls"))]
```

```{r, echo=FALSE}
numerical_vars <- sapply(clustering_data, is.numeric)
clustering_data[numerical_vars] <- scale(clustering_data[numerical_vars])
```

```{r, echo=FALSE, result='show'}
head(clustering_data)
```

As shown by the above table, we decided to consider the following features for the clustering analysis: **Number.vmail.messages, Total.minutes, Total.charge, Total.calls**. These features were chosen to capture a comprehensive view of customer behaviors, focusing on voicemail usage, overall call time, total expenditure, and call frequency. This selection aims to segment the customer base effectively, revealing patterns that can inform targeted marketing strategies, service optimizations, and customer retention efforts. By analyzing both behavioral and financial dimensions, the company can better understand diverse customer needs and tailor its approaches to enhance satisfaction and loyalty.

### Kmeans

Once we have preprocessed the data, we can proceed with the clustering; the first method we will use is the **kmeans algorithm**. We start by determining the optimal number of clusters using the **elbow method** and the **silhouette score**. As shown in the plots below, both the elbow method and the silhouette score suggest that the optimal number of clusters is 3; we will then proceed with 3 clusters, as suggested by both the methods.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=5}
fviz_nbclust(clustering_data, kmeans, method = "wss") +
  labs(subtitle = "WSS - Elbow method")
fviz_nbclust(clustering_data, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")
```

After feeding the data into the kmeans algorithm, we can visualize the clusters thanks to the **Principal Component Analysis (PCA)** that helps us in reducing the dimensionality of the data and visualizing the clusters in a 2D space. Hence, the plot below shows the customer segments identified by the kmeans algorithm.

```{r}
K <- 3
set.seed(1)  
km_result <- kmeans(clustering_data, centers = K, nstart = 25, iter.max = 100)
```

```{r, echo=FALSE, fig.width=10, fig.height=5}
pca <- prcomp(clustering_data)
plot_data <- data.frame(pca$x[, 1:2])
plot_data$cluster <- factor(km_result$cluster)
ggplot(plot_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.5) +
  labs(title = "Cluster Visualization with PCA",
       x = "Principal Component 1",
       y = "Principal Component 2") +
  theme_minimal() +
  scale_color_brewer(palette = "Set1")
```

The above scatter plot shows the customer segments identified by the kmeans algorithm. Each color represents a different cluster, and the points are plotted in a 2D space using the first two principal components. While the blue and the red clusters are well separated, indicating that the kmeans algorithm has successfully grouped customers based on their characteristics and behavior, the clusters exhibit considerable overlap, especially between the red and green clusters, indicating some shared characteristics among them.

```{r, echo=FALSE, fig.width=10, fig.height=5}
silhouette <- silhouette(km_result$cluster, dist(clustering_data))
mean(silhouette[, 3])
fviz_silhouette(silhouette) +
  theme_minimal()
```

The silhouette plot for three clusters reveals varied cluster cohesion: Cluster 2 (Green) displays high silhouette widths, indicating strong cluster integrity and good separation from other clusters. In contrast, Clusters 1 (Red) and 3 (Blue) show lower and more varied silhouette widths, suggesting these clusters might be less well-defined and contain points that are close to the boundaries of other clusters. The average silhouette width across all clusters is 0.3, highlighting an overall moderate separation quality between the clusters. This suggests potential areas for improving the clustering approach or further investigation into the data characteristics.

### Hierarchical Clustering

The second clustering method we will use is **hierarchical clustering**; we decided to use the same variables as for the kmeans algorithm in order to compare the results obtained with the two methods. We use the correlation-based method for hierarchical clustering which uses correlation coefficients as a measure of similarity between observations. We also consider different linkage methods to build the dendrogram and identify the optimal number of clusters: complete linkage, single linkage, average linkage, centroid linkage, and ward linkage.

```{r}
dist_matrix <- dist(clustering_data)
set.seed(1)
h_complete <- hclust(dist_matrix, method = "complete")
h_single <- hclust(dist_matrix, method = "single")
h_average <- hclust(dist_matrix, method = "average")
h_centroid <- hclust(dist_matrix, method = "centroid")
h_ward <- hclust(dist_matrix, method = "ward.D2")
```

By plotting the dendorgrams for the different linkage methods, we can identify the optimal number of clusters. The Complete Linkage shows a more balanced structure, suggesting uniformity in cluster compactness. Single Linkage results in elongated clusters, prone to chaining effects, highlighting susceptibility to noise. Average Linkage offers a moderate clustering structure, more natural than Single or Complete Linkage. Centroid Linkage shows variations in cluster sizes, influenced by centroid positions which may shift due to outliers. Ward's Method demonstrates very compact and well-defined clusters, effectively minimizing within-cluster variance, suggesting it as the most robust method for distinct and cohesive grouping in this dataset. As shown in the plots below, the **ward linkage method** in particular suggests that the optimal number of clusters is 3, which is consistent with the results obtained with the kmeans algorithm.

```{r, echo=FALSE, fig.align = "center", fig.width=13, fig.height=5}
par(mfrow=c(1, 5))
plot(hclust(dist_matrix, method = "complete"), main = "Complete Linkage")
plot(hclust(dist_matrix, method = "single"), main = "Single Linkage")
plot(hclust(dist_matrix, method = "average"), main = "Average Linkage")
plot(hclust(dist_matrix, method = "centroid"), main = "Centroid Linkage")
plot(hclust(dist_matrix, method = "ward.D2"), main = "Ward's Method")
par(mfrow=c(1, 1))
```

After identifying the optimal number of clusters, we can proceed by also checking the silouhette values achieved by the different linkage methods. When analyzed alongside dendrograms, high silhouette scores confirm the visual separation and compactness seen in methods like Ward's, suggesting effective clustering. Conversely, low scores, particularly with methods like Single Linkage which shows elongated clusters, indicate issues such as chaining effects or potential misassignments. Moderate scores for Average or Complete Linkage reflect better clustering than Single but possibly with some overlaps.

```{r, echo=FALSE}
sil_width <- function(method) {
  cluster <- hclust(dist_matrix, method = method)
  silhouette_stats <- silhouette(cutree(cluster, k = 3), dist_matrix)
  mean(silhouette_stats[, "sil_width"])
}
methods <- c("complete", "single", "average", "centroid", "ward.D2")
sapply(methods, sil_width)
best_method <- "ward.D2"
```

Finally, we proceed with the hierarchical clustering using the **ward linkage method** and 3 clusters, as suggested by the silhouette scores. We then visualize the clusters using PCA, as we did for the kmeans algorithm. The PCA-based cluster visualization using Ward's method clearly identifies three distinct customer groups in the dataset: a densely packed central cluster (blue), a broadly distributed cluster with some overlap (red), and a smaller, more distinct cluster (green). These clusters, represented on two principal components that likely capture the most significant variances in customer behavior such as usage, suggest specific strategies for targeted marketing, customer service enhancements, and product development. The graph validates the effectiveness of Ward's method in hierarchical clustering by showing well-defined group separations, providing robust insights for strategic business decisions focused on tailored customer engagement and service offerings.

```{r, echo=FALSE, fig.width=10, fig.height=5}
hc_best <- hclust(dist_matrix, method = best_method)
clusters <- cutree(hc_best, k = 3)
pca <- prcomp(clustering_data, scale. = TRUE)  # Ensure data is scaled
plot_data <- data.frame(pca$x[, 1:2])  # Using the first two principal components
plot_data$cluster <- factor(clusters)
ggplot(plot_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.5) +  # Points with some transparency
  labs(title = "Cluster Visualization with PCA",
       subtitle = paste("Hierarchical Clustering with", best_method, "Method"),
       x = "Principal Component 1",
       y = "Principal Component 2") +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") 
```

The comparison between the PCA plots of k-means clustering and hierarchical clustering using Ward's method reveals distinct differences in clustering outcomes. K-means clustering exhibits greater overlap and less defined separation between clusters, particularly between the blue and red clusters, suggesting a less effective partitioning of the dataset. In contrast, hierarchical clustering demonstrates clearer and more distinct cluster boundaries, with notably more compact and homogeneous clusters. This indicates that hierarchical clustering with Ward’s method provides more precise and well-defined groupings, enhancing the interpretability and utility of the clustering results. Consequently, hierarchical clustering appears more suitable for this dataset, offering robust insights for targeted strategies and data-driven decisions.
