---
title: "Data Analysis for Business - Midterm Project"
author: Edoardo Cocciò, Alexandra Tabarani, Marta Torella, Simone Filosofi
output:
  html_document: default
---

# Standard Deviated Group {.tabset .tabset-fade}

Group members:

-   Edoardo Cocciò - 282401
-   Alexandra Tabarani - 282091
-   Marta Torella - 284091
-   Simone Filosofi - 284531

## Task 1 {.tabset .tabset-fade}

The dataset we are going to analyze is the "Telecom Churn" dataset, which is a dataset that contains information about a telecommunications company that is trying to understand why its customers are leaving the company. Our role is to build a classification model to help Telecom in targeting the willing-to-churn customers before it is too late.


```{r message=FALSE, warning=FALSE, include=FALSE}
library(ggplot2)
library(corrplot)
library(dplyr)
library(tidyr)
library(skimr)
library(treemapify)
library(stringr)
library(glue)
library(dummy)
library(caret)
library(car)
library(pROC)
library(glmnet)
library(rpart)
library(rpart.plot) 
library(randomForest)
library(pROC)
library(cluster)
library(factoextra)
library(sf)
library(latex2exp)
library(gridExtra)
library(tree)
library(randomForest)
library(xgboost)
library(dendextend)
library(purrr)
library(ROSE)

data <- read.csv("TelecomChurn.csv")
```


### EDA

```{r}
names(data)

data$State = as.factor(data$State)
data$International.plan = as.factor(data$International.plan)
data$Voice.mail.plan = as.factor(data$Voice.mail.plan)
data$Churn = as.factor(data$Churn)
```
#### Missing values and duplicates
```{r}
sum(is.na(data))
anyDuplicated(data)
```
We checked for missing values in the dataset and found that there are no missing values in the dataset. We also checked for duplicates and found that there are no duplicates either.

```{r}
continuous_vars <- data[, sapply(data, is.numeric)]
categorical_vars <- data[, sapply(data, is.factor) & !names(data) %in% "Churn"]
```


#### Univariate analysis for numerical variables
```{r, echo=FALSE, fig.align='center', fig.width=10, fig.height=10}
continuous <- data[, sapply(data, is.numeric) | names(data) == "Churn"]

continuous %>%
  pivot_longer(cols = -Churn, names_to = "metric", values_to = "value") %>%
  ggplot(aes(value, color = as.factor(Churn))) +
  geom_density(alpha = 0.3) +
  facet_wrap(vars(metric), scales = "free") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  scale_color_manual(name = "Churn", labels = c("No", "Yes"), values = c("#15aabf", "#ff8787")) +
  labs(title = "Numeric Features", subtitle = "Univariate Distribution by Churn") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 18, colour = "gray5"),
    plot.subtitle = element_text(hjust = 0.5, colour = "gray5"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    legend.position = "top"
  )
```
The univariate analysis of the numerical variables shows that the distribution of the numerical variables is different for the two levels of the target variable "Churn". 
Foe example there's a noticeable difference in the distribution of *customer service calls*: customers who churned seem to make more customer service calls, which could indicate a relationship between customer satisfaction and churn. It appears that customers who churned tend to have slightly higher day minutes and charges. This might suggest that customers with higher usage during the day are more likely to churn. Something similar happens with *total.day.charge* also. The distribution of the other variables is not as clear-cut as the ones mentioned above, but there are some differences in the distributions of the variables between the two levels of the target variable.

```{r include=FALSE}
continuous_vars <- data[, sapply(data, is.numeric)]
categorical_vars <- data[, sapply(data, is.factor) & !names(data) %in% "Churn"]
```

#### Distribution of Target and Categorical Variables
```{r echo=FALSE, fig.show ="hold", out.width="50%"}
churn_percent <- data %>%
  group_by(Churn) %>%
  summarise(Count = n(), .groups = "drop") %>%
  mutate(Percent = Count / sum(Count) * 100)

ggplot(churn_percent, aes(x = "", y = Percent, fill = Churn)) +
  geom_col(width = 1, color = 1) +
  coord_polar("y", start = 0) +
  theme_void() +
  scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
  labs(title = "Percentage of Churn", fill = "Churn") +
  geom_text(aes(label = paste0(round(Percent, 1), "%")), position = position_stack(vjust = 0.5)) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 18))

long_data <- categorical_vars %>%
  pivot_longer(cols = c("International.plan", "Voice.mail.plan"), names_to = "Category", values_to = "Value")

ggplot(long_data, aes(x = Value, fill = Value)) +
  geom_bar(color = "black") +
  facet_wrap(~ Category, scales = "free_x", nrow = 2, ncol = 2) +
  labs(title = "Distribution of Categorical Variables", x = NULL, y = "Count") +
  theme_minimal() +
  scale_fill_manual(values = c("No" = "#ffe8cc", "Yes" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
  theme(
    strip.text = element_text(size = 12),
    axis.text.x = element_text(angle = 90, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    legend.position = "none"
  ) 

```

```{r message=FALSE, warning=FALSE, include=FALSE}
us_map <- map_data("state") %>% 
  as_tibble()

us_map %>%
  ggplot(aes(long, lat, map_id = region)) + 
  geom_map(
    map =us_map,
    color = "gray80",
    fill = "gray30",
    size = 0.3
  ) + 
  coord_map("ortho", orientation = c(39, -98, 0)) + 
  labs(
    title = "US Map",
    subtitle = "Based on Latitude and Longitude"
  ) + theme(plot.title = element_text(hjust=0.5))
```


```{r echo=FALSE, fig.align="center", fig.height=9, fig.width=9, message=FALSE, warning=FALSE}
state_names <- c(KS = "kansas", OH = "ohio", NJ = "new jersey", OK = "oklahoma",
                 AL = "alabama", MA = "massachusetts", MO = "missouri", LA = "louisiana",
                 WV = "west virginia", IN = "indiana", RI = "rhode island", IA = "iowa",
                 MT = "montana", NY = "new york", ID = "idaho", VT = "vermont",
                 VA = "virginia", TX = "texas", FL = "florida", CO = "colorado",
                 AZ = "arizona", SC = "south carolina", NE = "nebraska", WY = "wyoming",
                 HI = "hawaii", IL = "illinois", NH = "new hampshire", GA = "georgia",
                 AK = "alaska", MD = "maryland", AR = "arkansas", WI = "wisconsin",
                 OR = "oregon", MI = "michigan", DE = "delaware", UT = "utah",
                 CA = "california", MN = "minnesota", SD = "south dakota", NC = "north carolina",
                 WA = "washington", NM = "new mexico", NV = "nevada", DC = "district of columbia",
                 KY = "kentucky", ME = "maine", MS = "mississippi", TN = "tennessee",
                 PA = "pennsylvania", CT = "connecticut", ND = "north dakota")

churn.tbl <- data %>% select(State, Churn) %>%
  group_by(State) %>%
  summarize(
    total_customers = n(),
    churned_customers = sum(Churn == "True"),
    churn_rate = (churned_customers / total_customers),
    churn_rate_txt = scales::percent(churn_rate)
  ) %>%
  mutate(full_state = state_names[as.character(State)]) %>%
  ungroup() %>% 
  left_join(us_map, by=c("full_state" = "region"))

us_map_sf <- st_as_sf(us_map, coords = c("long", "lat"), crs = 4326, agr = "constant")

centroids <- us_map_sf %>%
  group_by(region) %>%
  summarise(geometry = st_centroid(st_union(geometry)))

centroids_df <- as.data.frame(st_coordinates(centroids))
centroids_df$region <- centroids$region

churn.tbl <- churn.tbl %>%
  left_join(centroids_df, by = c("full_state" = "region"))

churn.tbl %>%
  ggplot(aes(long, lat)) + 
  geom_map(
    aes(map_id = full_state),
    map = us_map,
    color = "gray86",
    fill = "gray30",
    size = 0.3
  ) + coord_map("ortho", orientation = c(39, -98, 0)) + geom_polygon(aes(group = group, fill = churn_rate), color="black") +
  scale_fill_gradient2("",low = "#18BC9C", mid = "white", high = "#E31A1C", midpoint = 0.10, labels = scales::percent) + 
  geom_text(aes(x = X, y = Y, label = State), stat = "unique", size = 3, inherit.aes = FALSE, fontface = "bold") +
  theme_void() + 
  theme(
    plot.title = element_text(size=18, face="bold", color = "#2C3E50"),
    legend.position = "right"
  ) + 
  labs(
    title = "Churn Rate Across US States",
    x = "",
    y = ""
  ) + 
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
  ) 
```



#### Hypothesis testing



#### Correlation analysis
```{r, echo=FALSE, fig.align='center'}
cor_mat <-
  data %>% 
  select(where(is.numeric)) %>% 
  cor()

corrplot(
  main = "\n\nCorrelation Matrix",
  cor_mat,
  method = "color",
  order = "alphabet",
  type = "lower",
  diag = FALSE,
  number.cex = 0.6,
  tl.cex = 0.6,
  tl.srt = 45,
  cl.pos = "b",
  addgrid.col = "white",
  addCoef.col = "white",
  col = COL1("Purples"),
  bg="gray",
  tl.col = "grey50"
)
```
The correlation matrix shows that there is a *perfect correlation between the charge variables and minutes variables* so we can drop all the minutes one *we drop Total.day.minutes, Total.eve.minutes, Total.night.minutes, Total.intl.minutes*.


**N.B.** We computed more analysis, using plots and hypothesis testing, to understand the relationship between the variables and the target variable "Churn", but for limited space restrictions we will not show them here. In the next section we will report the main conclusions arising from this analysis.

**CONCLUSION UP TO NOW:**
- we dropped the *minutes variables* because they are *perfectly correlated with the charge variables*;
- we saw that the *churn rate* is *higher* for customers that have an *international plan*;
- we saw that the churn rate is *higher* for customers that do *not have a voice mail plan*;
- we saw that the churn rate is *higher* for customers that *have a higher number of customer service calls*;
- we saw that the churn rate is *higher* for customers that *have higher total day minutes and total day charge*;
- we saw that the churn rate is *higher* for customers that *have higher total eve charge, night charge, and intl charge*;
- we saw that *State* also *influences the churn rate*;

### Lower-dimensional models
Before proceeding with the full model we need to focus on some lower dimensional model in order to investigate some interesting relationships between the variables. 
We only include customers who have made a service call:



### Preprocessing and feature engineering
We should first deal with the 51 states: we can group them by region (north, south, east, west). 
```{r, echo=FALSE} 
map_region <- function(state) {
  west.regions <- c("WA", "OR", "ID", "MT", "WY", "CA", "NV", "UT", "CO", "AK", "HI")
  southwest.regions <- c("AZ", "NM", "TX", "OK")
  midwest.regions <- c("ND", "SD", "NE", "KS", "MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH")
  southeast.regions <- c("AR", "LA", "MS", "AL", "TN", "KY", "WV", "VA", "NC", "SC", "GA", "FL")
  northeast.regions <- c("ME", "NH", "VT", "MA", "RI", "CT", "NY", "PA", "NJ", "DE", "MD", "DC")
  
  if (state %in% west.regions) {
    return("West")
  } else if (state %in% midwest.regions) {
    return("Mid West")
  } else if (state %in% southeast.regions) {
    return("South East")
  } else if (state %in% northeast.regions) {
    return("North East")
  } else {
    return("South West")  # in case there are any states not covered
  }
}

data$Region <- sapply(data$State, map_region)
data$Region <- as.factor(data$Region)
```
We'll use the regions column for the first models to avoid having high-cardinality encoded columns: we store the variable in order to use it later with more robust models (tree-based).
```{r}
state.column <- data$State
data$State <- NULL
```

Now let's re-do the chi square:
```{r}
contingency.table <- table(data$Region, data$Churn)
chisq.test(contingency.table)
```
H0: Region and Churn are independent, H1: Region and Churn are dependent
Since p-value > 0.05, we fail to reject the null hypothesis, therefore the region does not influence the churn rate. Let's see the distribution of churn by region:
```{r, echo=FALSE}
ggplot(data, aes(x = Region, fill = Churn)) + 
  geom_bar(width = 0.7, color = 1) +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust = 0, colour = "gray29", size = 10)) +
  labs(title = "Churn by Region", x = "Region", y = "Count", fill = "Churn", subtitle = "Is Churn rate influenced by the Region?") +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank(),
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5)
  ) +
  scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) 
```
We can see that the churn rate is not influenced by the region. 

#### Splitting the data
```{r}
set.seed(1)
ids.train <- sample(1:nrow(data), size = 0.75 * nrow(data), replace = FALSE)
data.train <- data[ids.train,]
data.val <- data[-ids.train,] 
```
We  oversample the minority class to balance the dataset, now we have 4260 total observations.
```{r}
data.train.balanced <- ovun.sample(Churn ~ ., data = data.train, method = "over", N=4260)$data
table(data.train.balanced$Churn)
```


### Main conclusions

## Task 2 {.tabset}
Cluster customers according to their behavior

### Feature Engineering
 
### Kmeans

### Hierarchical Clustering



