minC <- 2
maxC <- 10
res <- sapply(minC:maxC,
function(nc, clustering_data) index.G1(clustering_data, kmeans(clustering_data,centers = nc)$cluster),
clustering_data = utilities_s)
library(GGally)
## Dataset dal package
library(pdataita)
install.packages('pdataita')
install.packages('utilities')
library(GGally)
## Dataset dal package
library(pdataita)
install.packages('pdataita')
require(clusterSim)
install.packages('clusterSim')
minC <- 2
maxC <- 10
res <- sapply(minC:maxC,
function(nc, clustering_data) index.G1(clustering_data, kmeans(clustering_data,centers = nc)$cluster),
clustering_data = clustering_data)
library(clusterSim)
minC <- 2
maxC <- 10
res <- sapply(minC:maxC,
function(nc, clustering_data) index.G1(clustering_data, kmeans(clustering_data,centers = nc)$cluster),
clustering_data = clustering_data)
ggp <- ggplot(clustering_data=data.frame(x=2:(length(res)+1), y= res), mapping = aes(x=x,y=y)) +
geom_point() +
geom_line() +
xlab("Numero di cluster") +
ylab("Statistica pseudo-F di Calinski-Harabasz")
print(ggp)
# Calinski-Harabasz plot
calinski_plot <- fviz_nbclust(clustering_data, kmeans, method = "ch") +
labs(subtitle = "Calinski-Harabasz method")
# Calinski-Harabasz plot
calinski_plot <- fviz_nbclust(clustering_data, kmeans, method = "ch") +
labs(subtitle = "Calinski-Harabasz method")
# avg silhouette plot
fviz_nbclust(clustering_data, kmeans, method = "silhouette") +
labs(subtitle = "Silhouette method")
# Calinski-Harabasz plot
calinski_plot <- fviz_nbclust(clustering_data, kmeans, method = "gap_stat") +
labs(subtitle = "Calinski-Harabasz method")
# K-means clustering
set.seed(123)
kmeans_model <- kmeans(clustering_data, centers = 2, nstart = 25)
kmeans_model
# silhouette results
silhouette <- silhouette(kmeans_model$cluster, dist(clustering_data))
mean(silhouette[, 3])
# plot silhouette
fviz_silhouette(silhouette) +
theme_minimal()
# visualization
fviz_cluster(kmeans_model, data = clustering_data, geom = "point", stand = FALSE, ellipse.type = "convex") +
labs(title = "K-means Clustering") +
theme_minimal()
pca <- prcomp(clustering_data, scale = TRUE)
summary(pca)
plot(pca, type = "l")
abline(h = 1, col = "red", lty = 2)
biplot(pca, scale = 0)
biplot(pca)
ggcorr(cbind(clustering_data, pca$scores), label = TRUE, cex = 2.5)
set.seed(123)
kmeans_model <- kmeans(clustering_data, centers = 7, nstart = 25)
kmeans_model
silhouette <- silhouette(kmeans_model$cluster, dist(clustering_data))
mean(silhouette[, 3])
# plot the first two dimensions
fviz_pca_var(pca, col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE) +
theme_minimal()
set.seed(123)
kmeans_pca <- kmeans(pca$x[, 1:7], centers = 2, nstart = 25)
kmeans_pca
silhouette_pca <- silhouette(kmeans_pca$cluster, dist(pca$x[, 1:7]))
mean(silhouette_pca[, 3])
# plot silhouette
fviz_silhouette(silhouette_pca) +
theme_minimal()
# visualization
fviz_cluster(kmeans_pca, data = pca$x[, 1:7], geom = "point", stand = FALSE, ellipse.type = "convex") +
labs(title = "K-means Clustering with PCA") +
theme_minimal()
# CLUSTERING PRIMA BOZZA
# preparing data
clustering_data <- read.csv("TelecomChurn.csv")
clustering_data <- clustering_data[, !(names(clustering_data) %in% c("Churn", "State"))]
# let s drop all the minutes one
clustering_data <- clustering_data[, !(names(clustering_data) %in% c("Total.day.minutes", "Total.eve.minutes", "Total.night.minutes", "Total.intl.minutes"))]
# Creating new features
clustering_data$Total.call.charge <- clustering_data$Total.day.charge + clustering_data$Total.eve.charge + clustering_data$Total.night.charge + clustering_data$Total.intl.charge
clustering_data$Total.calls <- clustering_data$Total.day.calls + clustering_data$Total.eve.calls + clustering_data$Total.night.calls + clustering_data$Total.intl.calls
# drop them
clustering_data <- clustering_data[, !(names(clustering_data) %in% c("Total.day.charge", "Total.eve.charge", "Total.night.charge", "Total.intl.charge"))]
clustering_data <- clustering_data[, !(names(clustering_data) %in% c("Total.day.calls", "Total.eve.calls", "Total.night.calls", "Total.intl.calls"))]
numerical_vars <- sapply(clustering_data, is.numeric)
clustering_data[numerical_vars] <- scale(clustering_data[numerical_vars])
convert_yes_no <- function(column) {
ifelse(column == "Yes", 1, 0)
}
clustering_data$International.plan <- convert_yes_no(clustering_data$International.plan)
clustering_data$Voice.mail.plan <- convert_yes_no(clustering_data$Voice.mail.plan)
head(clustering_data)
# number of K
# elbow rule plot
fviz_nbclust(clustering_data, kmeans, method = "wss") +
labs(subtitle = "WSS - Elbow method")
# avg silhouette plot
fviz_nbclust(clustering_data, kmeans, method = "silhouette") +
labs(subtitle = "Silhouette method")
# K-means clustering
set.seed(123)
kmeans_model <- kmeans(clustering_data, centers = 2, nstart = 25)
kmeans_model
# silhouette results
silhouette <- silhouette(kmeans_model$cluster, dist(clustering_data))
mean(silhouette[, 3])
# plot silhouette
fviz_silhouette(silhouette) +
theme_minimal()
# visualization
fviz_cluster(kmeans_model, data = clustering_data, geom = "point", stand = FALSE, ellipse.type = "convex") +
labs(title = "K-means Clustering") +
theme_minimal()
# pca
pca <- prcomp(clustering_data, scale = TRUE)
summary(pca)
plot(pca, type = "l")
abline(h = 1, col = "red", lty = 2) # it suggests 7
fviz_pca_var(pca, col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE) +
theme_minimal()
# k-means with pca
set.seed(123)
kmeans_pca <- kmeans(pca$x[, 1:7], centers = 2, nstart = 25)
kmeans_pca
silhouette_pca <- silhouette(kmeans_pca$cluster, dist(pca$x[, 1:7]))
mean(silhouette_pca[, 3])
# visualization
fviz_cluster(kmeans_pca, data = pca$x[, 1:7], geom = "point", stand = FALSE, ellipse.type = "convex") +
labs(title = "K-means Clustering with PCA") +
theme_minimal()
# CLUSTERING PRIMA BOZZA
# preparing data
clustering_data <- read.csv("TelecomChurn.csv")
clustering_data <- clustering_data[, !(names(clustering_data) %in% c("Churn", "State"))]
# let s drop all the minutes one
clustering_data <- clustering_data[, !(names(clustering_data) %in% c("Total.day.minutes", "Total.eve.minutes", "Total.night.minutes", "Total.intl.minutes"))]
numerical_vars <- sapply(clustering_data, is.numeric)
clustering_data[numerical_vars] <- scale(clustering_data[numerical_vars])
convert_yes_no <- function(column) {
ifelse(column == "Yes", 1, 0)
}
clustering_data$International.plan <- convert_yes_no(clustering_data$International.plan)
clustering_data$Voice.mail.plan <- convert_yes_no(clustering_data$Voice.mail.plan)
head(clustering_data)
# number of K
# elbow rule plot
fviz_nbclust(clustering_data, kmeans, method = "wss") +
labs(subtitle = "WSS - Elbow method")
# avg silhouette plot
fviz_nbclust(clustering_data, kmeans, method = "silhouette") +
labs(subtitle = "Silhouette method")
# K-means clustering
set.seed(123)
kmeans_model <- kmeans(clustering_data, centers = 2, nstart = 25)
kmeans_model
# silhouette results
silhouette <- silhouette(kmeans_model$cluster, dist(clustering_data))
mean(silhouette[, 3])
# plot silhouette
fviz_silhouette(silhouette) +
theme_minimal()
# visualization
fviz_cluster(kmeans_model, data = clustering_data, geom = "point", stand = FALSE, ellipse.type = "convex") +
labs(title = "K-means Clustering") +
theme_minimal()
# pca
pca <- prcomp(clustering_data, scale = TRUE)
summary(pca)
plot(pca, type = "l")
abline(h = 1, col = "red", lty = 2) # it suggests 7
fviz_pca_var(pca, col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE) +
theme_minimal()
# k-means with pca
set.seed(123)
kmeans_pca <- kmeans(pca$x[, 1:7], centers = 2, nstart = 25)
kmeans_pca
silhouette_pca <- silhouette(kmeans_pca$cluster, dist(pca$x[, 1:7]))
mean(silhouette_pca[, 3])
# visualization
fviz_cluster(kmeans_pca, data = pca$x[, 1:7], geom = "point", stand = FALSE, ellipse.type = "convex") +
labs(title = "K-means Clustering with PCA") +
theme_minimal()
fviz_pca_var(pca, col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE) +
theme_minimal()
library(ggplot2)
library(corrplot)
library(dplyr)
library(tidyr)
library(skimr)
library(treemapify)
library(stringr)
library(glue)
library(dummy)
library(caret)
library(car)
library(pROC)
library(glmnet)
library(rpart)
library(rpart.plot)
library(randomForest)
library(pROC)
library(cluster)
library(factoextra)
library(sf)
library(latex2exp)
library(gridExtra)
library(tree)
library(randomForest)
data = read.csv("TelecomChurn.csv")
# 3333 rows and 20 columns
head(data)
str(data)
sum(is.na(data)) # no missing values
summary(data)
skim(data)
# Count of Distinct Features
for (col in names(data)) {
cat("Distinct values in", col, ":\n")
print(unique(data[[col]]))
cat("\n")
}
anyDuplicated(data) # no duplicates
# converting characters to factor
data$State = as.factor(data$State)
data$International.plan = as.factor(data$International.plan)
data$Voice.mail.plan = as.factor(data$Voice.mail.plan)
data$Churn = as.factor(data$Churn)
# checking the distribution of our target
# We will first calculate the proportions of Churn
churn_percent <- data %>%
group_by(Churn) %>%
summarise(Count = n(), .groups = "drop") %>%
mutate(Percent = Count / sum(Count) * 100)
#print(churn_percent)
ggplot(churn_percent, aes(x = "", y = Percent, fill = Churn)) +
geom_col(width = 1, color = 1) +
coord_polar("y", start = 0) +
theme_void() +
scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
labs(title = "Percentage of Churn", fill = "Churn") +
geom_text(aes(label = paste0(round(Percent, 1), "%")), position = position_stack(vjust = 0.5)) +
theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 18))
continuous_vars <- data[, sapply(data, is.numeric)]
#print(continuous_vars)
categorical_vars <- data[, sapply(data, is.factor) & !names(data) %in% "Churn"]
#print(categorical_vars)
# univariate analysis for categorical variables (State, International.plan, Voice.mail.plan)
long_data <- categorical_vars %>%
pivot_longer(cols = c("International.plan", "Voice.mail.plan"), names_to = "Category", values_to = "Value")
ggplot(long_data, aes(x = Value, fill = Value)) +
geom_bar(color = "black") +
facet_wrap(~ Category, scales = "free_x", nrow = 2, ncol = 2) +
labs(title = "Distribution of Categorical Variables", x = NULL, y = "Count") +
theme_minimal() +
theme(
strip.text = element_text(size = 12),
axis.text.x = element_text(angle = 90, hjust = 1, size = 10),
plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
legend.position = "none"
)
# voice mail and international plan are imbalanced
# also our target variable is imbalanced
# Treemap for state Distribution
state.count <- data %>% group_by(State) %>% summarise(count = n())
ggplot(state.count, aes(area = count, fill = count, label = glue("{State}\n{count}"))) +
geom_treemap() +
geom_treemap_text(colour = "white", place = "center", size = 13, grow = TRUE) +
labs(title = "Tree Map for State Distribution", fill = "State") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16, colour = "gray5"),
legend.position = "none",
plot.background = element_rect(fill = "#f6fff8")
) +
scale_fill_viridis_c()
# univariate analysis for numerical variables
### KDE plots
continuous <- data[, sapply(data, is.numeric) | names(data) == "Churn"]
continuous %>%
pivot_longer(cols = -Churn, names_to = "metric", values_to = "value") %>%
ggplot(aes(value, color = as.factor(Churn))) +
geom_density(alpha = 0.3) +
facet_wrap(vars(metric), scales = "free") +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
scale_color_manual(name = "Churn", labels = c("No", "Yes"), values = c("#15aabf", "#ff8787")) +
labs(title = "Numeric Features", subtitle = "Univariate Distribution by Churn") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 18, colour = "gray5"),
plot.subtitle = element_text(hjust = 0.5, colour = "gray5"),
axis.text.x = element_text(angle = 45, hjust = 1),
axis.title.x = element_text(face = "bold"),
axis.title.y = element_text(face = "bold"),
legend.position = "top"
)
#There's a noticeable difference in the distribution of customer service calls
# between those who churned and those who did not.
# Customers who churned seem to make more customer service calls, which could
# indicate a relationship between customer satisfaction and churn.
# It appears that customers who churned tend to have slightly higher day minutes
# and charges. This might suggest that customers with higher usage during the day are more likely to churn.
# something similar with total.day.charge
# the others follow more or less the same trend
# analysis of churn and customer service calls
data %>%
group_by(Customer.service.calls, Churn) %>%
summarise(Count = n(), .groups = "drop") %>%
ggplot(aes(x = Customer.service.calls, y = Count, color = Churn)) +
geom_line() +
geom_point() +
labs(title = "Churn by Customer Service Calls", x = "Customer Service Calls", y = "Count", color = "Churn") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5))
# customers who churned made more customer service calls
# let's double check it with hypothesis testing
# H0: the average number of customer service calls is the same for customers who churned and those who did not
# H1: the average number of customer service calls is different for customers who churned and those who did not
# chi square
chisq.test(data$Customer.service.calls, data$Churn)
# p-value < 0.05, we reject the null hypothesis therefore the average number of customer service calls is different for customers who churned and those who did not
# now hypo for: Average Charges for Churners vs. Non-Churners
# H0: the average charges are the same for customers who churned and those who did not
# H1: the average charges are different for customers who churned and those who did not
# t-test
t.test(data$Total.day.charge ~ data$Churn)
# p-value < 0.05, we reject the null hypothesis therefore the average charges are different for customers who churned and those who did not
# hypo for Voice Mail Plan and Churn:
# H0: the average churn rate is the same for customers who have a voice mail plan and those who do not
# H1: the average churn rate is different for customers who have a voice mail plan and those who do not
# chi square
chisq.test(data$Voice.mail.plan, data$Churn)
# p-value = 5.151e-09 < 0.05, we reject the null hypothesis therefore the average churn rate is different for customers who have a voice mail plan and those who do not
# analysis of churn and day minutes and day charge
# customers who churned have higher total day minutes
# customers who churned have higher total day charge
long_data <- data %>%
select(Churn, Total.day.minutes, Total.day.charge) %>%
pivot_longer(cols = c(Total.day.minutes, Total.day.charge),
names_to = "Measure",
values_to = "Value")
library(ggplot2)
library(corrplot)
library(dplyr)
library(tidyr)
library(skimr)
library(treemapify)
library(stringr)
library(glue)
library(dummy)
library(caret)
library(car)
library(pROC)
library(glmnet)
library(rpart)
library(rpart.plot)
library(randomForest)
library(pROC)
library(cluster)
library(factoextra)
library(sf)
library(latex2exp)
library(gridExtra)
library(tree)
library(randomForest)
data = read.csv("TelecomChurn.csv")
# 3333 rows and 20 columns
head(data)
str(data)
sum(is.na(data)) # no missing values
summary(data)
skim(data)
# Count of Distinct Features
for (col in names(data)) {
cat("Distinct values in", col, ":\n")
print(unique(data[[col]]))
cat("\n")
}
anyDuplicated(data) # no duplicates
# converting characters to factor
data$State = as.factor(data$State)
data$International.plan = as.factor(data$International.plan)
data$Voice.mail.plan = as.factor(data$Voice.mail.plan)
data$Churn = as.factor(data$Churn)
# checking the distribution of our target
# We will first calculate the proportions of Churn
churn_percent <- data %>%
group_by(Churn) %>%
summarise(Count = n(), .groups = "drop") %>%
mutate(Percent = Count / sum(Count) * 100)
#print(churn_percent)
ggplot(churn_percent, aes(x = "", y = Percent, fill = Churn)) +
geom_col(width = 1, color = 1) +
coord_polar("y", start = 0) +
theme_void() +
scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
labs(title = "Percentage of Churn", fill = "Churn") +
geom_text(aes(label = paste0(round(Percent, 1), "%")), position = position_stack(vjust = 0.5)) +
theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 18))
continuous_vars <- data[, sapply(data, is.numeric)]
#print(continuous_vars)
categorical_vars <- data[, sapply(data, is.factor) & !names(data) %in% "Churn"]
#print(categorical_vars)
# univariate analysis for categorical variables (State, International.plan, Voice.mail.plan)
long_data <- categorical_vars %>%
pivot_longer(cols = c("International.plan", "Voice.mail.plan"), names_to = "Category", values_to = "Value")
ggplot(long_data, aes(x = Value, fill = Value)) +
geom_bar(color = "black") +
facet_wrap(~ Category, scales = "free_x", nrow = 2, ncol = 2) +
labs(title = "Distribution of Categorical Variables", x = NULL, y = "Count") +
theme_minimal() +
theme(
strip.text = element_text(size = 12),
axis.text.x = element_text(angle = 90, hjust = 1, size = 10),
plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
legend.position = "none"
)
# voice mail and international plan are imbalanced
# also our target variable is imbalanced
# Treemap for state Distribution
state.count <- data %>% group_by(State) %>% summarise(count = n())
ggplot(state.count, aes(area = count, fill = count, label = glue("{State}\n{count}"))) +
geom_treemap() +
geom_treemap_text(colour = "white", place = "center", size = 13, grow = TRUE) +
labs(title = "Tree Map for State Distribution", fill = "State") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16, colour = "gray5"),
legend.position = "none",
plot.background = element_rect(fill = "#f6fff8")
) +
scale_fill_viridis_c()
# univariate analysis for numerical variables
### KDE plots
continuous <- data[, sapply(data, is.numeric) | names(data) == "Churn"]
continuous %>%
pivot_longer(cols = -Churn, names_to = "metric", values_to = "value") %>%
ggplot(aes(value, color = as.factor(Churn))) +
geom_density(alpha = 0.3) +
facet_wrap(vars(metric), scales = "free") +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
scale_color_manual(name = "Churn", labels = c("No", "Yes"), values = c("#15aabf", "#ff8787")) +
labs(title = "Numeric Features", subtitle = "Univariate Distribution by Churn") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 18, colour = "gray5"),
plot.subtitle = element_text(hjust = 0.5, colour = "gray5"),
axis.text.x = element_text(angle = 45, hjust = 1),
axis.title.x = element_text(face = "bold"),
axis.title.y = element_text(face = "bold"),
legend.position = "top"
)
#There's a noticeable difference in the distribution of customer service calls
# between those who churned and those who did not.
# Customers who churned seem to make more customer service calls, which could
# indicate a relationship between customer satisfaction and churn.
# It appears that customers who churned tend to have slightly higher day minutes
# and charges. This might suggest that customers with higher usage during the day are more likely to churn.
# something similar with total.day.charge
# the others follow more or less the same trend
# analysis of churn and customer service calls
data %>%
group_by(Customer.service.calls, Churn) %>%
summarise(Count = n(), .groups = "drop") %>%
ggplot(aes(x = Customer.service.calls, y = Count, color = Churn)) +
geom_line() +
geom_point() +
labs(title = "Churn by Customer Service Calls", x = "Customer Service Calls", y = "Count", color = "Churn") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5))
# customers who churned made more customer service calls
# let's double check it with hypothesis testing
# H0: the average number of customer service calls is the same for customers who churned and those who did not
# H1: the average number of customer service calls is different for customers who churned and those who did not
# chi square
chisq.test(data$Customer.service.calls, data$Churn)
# p-value < 0.05, we reject the null hypothesis therefore the average number of customer service calls is different for customers who churned and those who did not
# now hypo for: Average Charges for Churners vs. Non-Churners
# H0: the average charges are the same for customers who churned and those who did not
# H1: the average charges are different for customers who churned and those who did not
# t-test
t.test(data$Total.day.charge ~ data$Churn)
# p-value < 0.05, we reject the null hypothesis therefore the average charges are different for customers who churned and those who did not
# hypo for Voice Mail Plan and Churn:
# H0: the average churn rate is the same for customers who have a voice mail plan and those who do not
# H1: the average churn rate is different for customers who have a voice mail plan and those who do not
# chi square
chisq.test(data$Voice.mail.plan, data$Churn)
# p-value = 5.151e-09 < 0.05, we reject the null hypothesis therefore the average churn rate is different for customers who have a voice mail plan and those who do not
# analysis of churn and day minutes and day charge
# customers who churned have higher total day minutes
# customers who churned have higher total day charge
long_data <- data %>%
select(Churn, Total.day.minutes, Total.day.charge) %>%
pivot_longer(cols = c(Total.day.minutes, Total.day.charge),
names_to = "Measure",
values_to = "Value")
