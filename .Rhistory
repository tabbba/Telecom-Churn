mutate(perc = (counts / sum(counts)) * 100) %>%
arrange(desc(counts)) %>%
ggplot(aes("", counts)) +
geom_col(
position = "fill",
color = "black",
width = 1,
aes(fill = factor(var_2_ex))
) +
geom_text(
aes(label = str_c(round(perc,1), "%"),
group = factor(var_1_ex)),
position = position_fill(vjust = 0.5),
color = "white",
size = 5,
show.legend = FALSE,
fontface = "bold"
) +
coord_polar(theta = "y") +
scale_fill_manual (values = c("#193964", "#026AA3")) +
theme_void() +
facet_wrap(vars(str_c(var_1_ex, "\n", for_title_1)))+
labs(
title = glue(for_title_2, " proportion by ", for_title_1),
subtitle = " ",
fill =NULL
) +
theme(plot.title = element_text(hjust = 0.5),
legend.position = "bottom",
strip.text = element_text(
colour = "black",
size = 12,
face = "bold"
))
}
group_plt(Voice.mail.plan)
group_plt(International.plan)
group_plt(State)
ggplot(data, aes(x = State, fill = Churn)) +
geom_bar(width = 0.7, color = 1) +
theme_minimal() +
theme(axis.text.x = element_text(hjust = 0, colour = "gray29", size = 10)) +
labs(title = "Churn by State", x = "State", y = "Count", fill = "Churn", subtitle = "Is Churn rate influenced by State?") +
theme(
axis.text = element_blank(),
axis.title = element_blank(),
legend.position = "bottom",
plot.title = element_text(hjust = 0.5, face = "bold"),
plot.subtitle = element_text(hjust = 0.5)
) +
scale_y_continuous(limits = c(-35, 100)) +
scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
coord_polar(start = 0)
us_map <- map_data("state") %>%
as_tibble()
us_map <- map_data("state") %>%
as_tibble()
us_map %>%
ggplot(aes(long, lat, map_id = region)) +
geom_map(
map =us_map,
color = "gray80",
fill = "gray30",
size = 0.3
) +
coord_map("ortho", orientation = c(39, -98, 0)) +
labs(
title = "US Map",
subtitle = "Based on Latitude and Longitude"
) + theme(plot.title = element_text(hjust=0.5))
us_map <- map_data("state") %>%
as_tibble()
us_map %>%
ggplot(aes(long, lat, map_id = region)) +
geom_map(
map =us_map,
color = "gray80",
fill = "gray30",
size = 0.3
) +
coord_map("ortho", orientation = c(39, -98, 0)) +
labs(
title = "US Map",
subtitle = "Based on Latitude and Longitude"
) + theme(plot.title = element_text(hjust=0.5))
state_names <- c(KS = "kansas", OH = "ohio", NJ = "new jersey", OK = "oklahoma",
AL = "alabama", MA = "massachusetts", MO = "missouri", LA = "louisiana",
WV = "west virginia", IN = "indiana", RI = "rhode island", IA = "iowa",
MT = "montana", NY = "new york", ID = "idaho", VT = "vermont",
VA = "virginia", TX = "texas", FL = "florida", CO = "colorado",
AZ = "arizona", SC = "south carolina", NE = "nebraska", WY = "wyoming",
HI = "hawaii", IL = "illinois", NH = "new hampshire", GA = "georgia",
AK = "alaska", MD = "maryland", AR = "arkansas", WI = "wisconsin",
OR = "oregon", MI = "michigan", DE = "delaware", UT = "utah",
CA = "california", MN = "minnesota", SD = "south dakota", NC = "north carolina",
WA = "washington", NM = "new mexico", NV = "nevada", DC = "district of columbia",
KY = "kentucky", ME = "maine", MS = "mississippi", TN = "tennessee",
PA = "pennsylvania", CT = "connecticut", ND = "north dakota")
churn.tbl <- data %>% select(State, Churn) %>%
group_by(State) %>%
summarize(
total_customers = n(),
churned_customers = sum(Churn == "True"),
churn_rate = (churned_customers / total_customers),
churn_rate_txt = scales::percent(churn_rate)
) %>%
mutate(full_state = state_names[as.character(State)]) %>%
ungroup() %>%
left_join(us_map, by=c("full_state" = "region"))
us_map_sf <- st_as_sf(us_map, coords = c("long", "lat"), crs = 4326, agr = "constant")
centroids <- us_map_sf %>%
group_by(region) %>%
summarise(geometry = st_centroid(st_union(geometry)))
centroids_df <- as.data.frame(st_coordinates(centroids))
centroids_df$region <- centroids$region
churn.tbl <- churn.tbl %>%
left_join(centroids_df, by = c("full_state" = "region"))
churn.tbl %>%
ggplot(aes(long, lat)) +
geom_map(
aes(map_id = full_state),
map = us_map,
color = "gray86",
fill = "gray30",
size = 0.3
) + coord_map("ortho", orientation = c(39, -98, 0)) + geom_polygon(aes(group = group, fill = churn_rate), color="black") +
scale_fill_gradient2("",low = "#18BC9C", mid = "white", high = "#E31A1C", midpoint = 0.10, labels = scales::percent) +
geom_text(aes(x = X, y = Y, label = State), stat = "unique", size = 3, inherit.aes = FALSE, fontface = "bold") +
theme_void() +
theme(
plot.title = element_text(size=18, face="bold", color = "#2C3E50"),
legend.position = "right"
) +
labs(
title = "Churn Rate Across US States",
x = "",
y = ""
) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),
)
# Investigate the relationship between state and churn with a chi-squared test of independence
# H0: State and Churn are independent
# H1: State and Churn are dependent
contingency.table <- table(data$State, data$Churn)
chisq.test(contingency.table)
# p-value is significant, we reject the null hypothesis and conclude that state influences the churn rate
contingency.table <- table(data$Total.night.calls, data$Churn)
chisq.test(contingency.table)
# p-value is not significant, we fail to reject the null hypothesis and conclude that Total night calls does not influence the churn rate
service_calls <- data %>%
filter(Customer.service.calls > 0)
# The proportion of customers who churned after making a service call:
churn_percent <- service_calls %>%
group_by(Customer.service.calls, Churn) %>%
summarise(Count = n(), .groups = "drop") %>%
mutate(Percent = Count / sum(Count) * 100)
ggplot(churn_percent, aes(x = Customer.service.calls, y = Percent, fill = Churn)) +
geom_col(width = 0.7, color = 1) +
labs(title = "Churn by Customer Service Calls", x = "Customer Service Calls", y = "Percent", fill = "Churn") +
theme_minimal() +
scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),
legend.position = "bottom"
)
set.seed(1)
data$Churn <- as.factor(data$Churn)
model <- glm(Churn ~ Customer.service.calls, family = binomial, data = data)
summary(model)
call_range <- seq(min(data$Customer.service.calls, na.rm = TRUE),
max(data$Customer.service.calls, na.rm = TRUE),
length.out = 100)
newdata <- data.frame(Customer.service.calls = call_range)
newdata$Probability <- predict(model, newdata = newdata, type = "response")
newdata$Odds <- newdata$Probability / (1 - newdata$Probability)
head(newdata)
# plotting the probability of churn based on customer service calls
ggplot(newdata, aes(x = Customer.service.calls, y = Probability)) +
geom_line() +
labs(title = "Effect of Customer Service Calls on the Probability of Churn",
x = "Number of Customer Service Calls",
y = "Probability of Churn") +
scale_y_continuous(labels = scales::percent_format()) +  # Convert y-axis into percentage format
theme_minimal() +  # Use a minimal theme
geom_hline(yintercept = 0.5, colour = "red", linetype = "dashed")
theme(
plot.title = element_text(hjust = 0.5),
axis.text.x = element_text(angle = 45, hjust = 1),
axis.title.x = element_text(face = "bold"),
axis.title.y = element_text(face = "bold")
)
# since we are dealing with an imbalanced dataset we undersample the majority class (no churn) to balance the dataset.
set.seed(1)
majority_indices <- which(data$Churn == 'False')
minority_count <- sum(data$Churn == 'True')
sampled_indices <- sample(majority_indices, size = minority_count, replace = FALSE)
balanced_data <- data[c(sampled_indices, which(data$Churn == 'True')), ]
levels(data$Churn) # original levels --> i needed this step since i was getting some errors due to the factorization
balanced_data$Churn <- as.factor(balanced_data$Churn)
levels(balanced_data$Churn)
table(balanced_data$Churn)
balanced_model <- glm(Churn ~ Customer.service.calls, family = binomial, data = balanced_data )
summary(balanced_model)
# plot for balanced data
call_range <- seq(min(data$Customer.service.calls, na.rm = TRUE),
max(data$Customer.service.calls, na.rm = TRUE),
length.out = 100)
newdata <- data.frame(Customer.service.calls = call_range)
newdata$Probability <- predict(balanced_model, newdata = newdata, type = "response")
newdata$Odds <- newdata$Probability / (1 - newdata$Probability)
ggplot(newdata, aes(x = Customer.service.calls, y = Probability)) +
geom_line() +
labs(title = "Effect of Customer Service Calls on the Probability of Churn (Balanced Data)",
x = "Number of Customer Service Calls",
y = "Probability of Churn") +
scale_y_continuous(labels = scales::percent_format()) +  # Convert y-axis into percentage format
theme_minimal() +  # Use a minimal theme
theme(
plot.title = element_text(hjust = 0.5),
axis.text.x = element_text(angle = 45, hjust = 1),
axis.title.x = element_text(face = "bold"),
axis.title.y = element_text(face = "bold")
)
# plotting the confusion matrix for comparison
# 0 --> False, 1 --> True
# Prediction on the unbalanced data
confusion.mat <- function(data, model, target, threshold) {
predicted_Y <- ifelse(predict(model, type = "response", newdata = data) > threshold, 'True', 'False')
actual_Y <- data[[target]]
True.positive <- sum(predicted_Y == 'True' & actual_Y == 'True')
True.negative <- sum(predicted_Y == 'False' & actual_Y == 'False')
False.positive <- sum(predicted_Y == 'True' & actual_Y == 'False')
False.negative <- sum(predicted_Y == 'False' & actual_Y == 'True')
Confusion.Matrix <- matrix(c(True.positive, False.negative,
False.positive, True.negative),
nrow = 2, byrow = TRUE)
rownames(Confusion.Matrix) <- c("Actual Positive", "Actual Negative")
colnames(Confusion.Matrix) <- c("Predicted Positive", "Predicted Negative")
print(Confusion.Matrix)
}
confusion.mat(data, model, "Churn", 0.5) #Predictions on unbalanced data
confusion.mat(balanced_data,balanced_model, "Churn", 0.5) # Predictions on balanced data
# print all the states
unique(data$State)
map_region <- function(state) {
west.regions <- c("WA", "OR", "ID", "MT", "WY", "CA", "NV", "UT", "CO", "AK", "HI")
southwest.regions <- c("AZ", "NM", "TX", "OK")
midwest.regions <- c("ND", "SD", "NE", "KS", "MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH")
southeast.regions <- c("AR", "LA", "MS", "AL", "TN", "KY", "WV", "VA", "NC", "SC", "GA", "FL")
northeast.regions <- c("ME", "NH", "VT", "MA", "RI", "CT", "NY", "PA", "NJ", "DE", "MD", "DC")
if (state %in% west.regions) {
return("West")
} else if (state %in% midwest.regions) {
return("Mid West")
} else if (state %in% southeast.regions) {
return("South East")
} else if (state %in% northeast.regions) {
return("North East")
} else {
return("South West")  # in case there are any states not covered
}
}
data$Region <- sapply(data$State, map_region)
# we'll use the regions column for the first models to avoid having high-cardinality encoded columns.
# we store the variable in order to use it later with more robust models (tree-based).
state.column <- data$State
data$State <- NULL
head(data)
# let's see the distribution of churn by region
group_plt(Region)
# now let's re-do the chi square
contingency.table <- table(data$Region, data$Churn)
chisq.test(contingency.table)
# H0: Region and Churn are independent
# H1: Region and Churn are dependent
#p-value > 0.05, we fail to reject the null hypothesis, therefore the region does not influence the churn rate
# let's see the distribution of churn by region
ggplot(data, aes(x = Region, fill = Churn)) +
geom_bar(width = 0.7, color = 1) +
theme_minimal() +
theme(axis.text.x = element_text(hjust = 0, colour = "gray29", size = 10)) +
labs(title = "Churn by Region", x = "Region", y = "Count", fill = "Churn", subtitle = "Is Churn rate influenced by the Region?") +
theme(
axis.text = element_blank(),
axis.title = element_blank(),
legend.position = "bottom",
plot.title = element_text(hjust = 0.5, face = "bold"),
plot.subtitle = element_text(hjust = 0.5)
) +
scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes"))
# POINT 5
# data preprocessing
# - split the data into training and testing sets
# Set seed for reproducibility
set.seed(1)
ids.train <- sample(1:nrow(data), size = 0.75 * nrow(data), replace = FALSE)
data.train <- data[ids.train,]
data.val <- data[-ids.train,]
# SCALING
cols_to_scale <- c("Account.length", "Area.code", "Number.vmail.messages", "Total.day.calls", "Total.day.charge", "Total.eve.calls", "Total.eve.charge", "Total.night.calls", "Total.night.charge", "Total.intl.calls", "Total.intl.charge", "Customer.service.calls")
train.mean <- apply(data.train[, cols_to_scale], MARGIN = 2, FUN = mean)
train.sd <- apply(data.train[, cols_to_scale], MARGIN = 2, FUN = sd)
data.train[, cols_to_scale] <- scale(data.train[, cols_to_scale], center = train.mean, scale = train.sd)
# Scale validation data using training data's parameters
data.val[, cols_to_scale] <- scale(data.val[, cols_to_scale], center = train.mean, scale = train.sd)
# Baseline Logistic Regression Model
logistic.baseline <- glm(Churn ~ ., data = data.train, family = "binomial")
summary(logistic.baseline)
par(mfrow = c(2,2))
plot(logistic.baseline)
vif(logistic.baseline) # all good
# Stepwise Model Selection
# Akaike Information Criterion
akaike.fw <- step(glm(Churn ~ 1, family = "binomial", data = data.train), scope = formula(logistic.baseline), direction = "forward")
akaike.back <- step(logistic.baseline, direction = "backward")
akaike.both <- step(logistic.baseline, direction = "both")
# Bayesian Information criterion
bayesian.fw <- step(glm(Churn ~ 1, family = "binomial", data = data.train), scope = formula(logistic.baseline), direction = "forward", k = log(nrow(data.train)))
bayesian.back <- step(logistic.baseline, direction = "backward", k = log(nrow(data.train)))
bayesian.both <- step(logistic.baseline, direction = "both", k = log(nrow(data.train)))
# Considering bidirectional elimination of both methods, BIC is more strict as it removes all the covariates representing the regions
get.metrics = function(conf.mat) {
true.positives <- conf.mat[1,1]
true.negatives <- conf.mat[2,2]
false.positives <- conf.mat[1,2]
false.negatives <- conf.mat[2,1]
num.observations <- true.positives + true.negatives + false.positives + false.negatives
accuracy <- (true.positives + true.negatives) / num.observations
precision <- (true.positives) / (true.positives + false.positives)
recall <- true.positives / (true.positives + false.negatives)
f1 <- 2 * ((precision * recall) / (precision + recall))
metrics <- data.frame(t(c(accuracy, precision, recall, f1)))
columns <- c("Accuracy", "Precision", "Recall", "F1")
colnames(metrics) <- columns
return(metrics)
}
# Validation set results
akaike.mat <- confusion.mat(data.val, akaike.both, "Churn", 0.5)
akaike.metrics <- get.metrics(akaike.mat)
akaike.metrics
bayesian.mat <- confusion.mat(data.val, bayesian.both, "Churn", 0.5)
bayesian.metrics <- get.metrics(bayesian.mat)
bayesian.metrics
# Slightly better results with akaike, maybe dropping all the region features as BIC does is too big of a loss of informations
# full logistic model
full.mat <- confusion.mat(data.val, logistic.baseline, "Churn", 0.5)
full.metrics <- get.metrics(full.mat)
full.metrics
# ROC curves
par(mfrow = c(2, 2))
roc_full <- roc(data.val$Churn, predict(logistic.baseline, newdata = data.val, type = "response"),
plot = TRUE, main = "ROC Curve Full Model", col = "purple", lwd = 3,
auc.polygon = TRUE, print.auc = TRUE)
roc_akaike <- roc(data.val$Churn, predict(akaike.both, newdata = data.val, type = "response"),
plot = TRUE, main = "AIC Model", col = "blue", lwd = 3,
auc.polygon = TRUE, print.auc = TRUE)
roc_bayesian <- roc(data.val$Churn, predict(bayesian.both, newdata = data.val, type = "response"),
plot = TRUE, main = "BIC Model", col = "red", lwd = 3,
auc.polygon = TRUE, print.auc = TRUE)
par(mfrow = c(1, 1))
# AUC values
auc_akaike <- as.numeric(auc(roc(data.val$Churn, predict(akaike.both, newdata = data.val, type = "response"))))
auc_bayesian <- as.numeric(auc(roc(data.val$Churn, predict(bayesian.both, newdata = data.val, type = "response"))))
auc_full <- as.numeric(auc(roc(data.val$Churn, predict(logistic.baseline, newdata = data.val, type = "response"))))
# comparison (for now) between full, aic, bic
akaike_df <- data.frame(Model = "Akaike",
Accuracy = akaike.metrics$Accuracy,
Precision = akaike.metrics$Precision,
Recall = akaike.metrics$Recall,
F1_Score = akaike.metrics$F1,
AUC = auc_akaike)
bayesian_df <- data.frame(Model = "Bayesian",
Accuracy = bayesian.metrics$Accuracy,
Precision = bayesian.metrics$Precision,
Recall = bayesian.metrics$Recall,
F1_Score = bayesian.metrics$F1,
AUC = auc_bayesian)
full_df <- data.frame(Model = "Full Logistic",
Accuracy = full.metrics$Accuracy,
Precision = full.metrics$Precision,
Recall = full.metrics$Recall,
F1_Score = full.metrics$F1,
AUC = auc_full)
comparison_df <- bind_rows(akaike_df, bayesian_df, full_df)
comparison_df
# LASSO
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)
lasso <- train(Churn ~ ., data = data.train, method = "glmnet", metric = "Accuracy", trControl = ctrl, tuneGrid = expand.grid(alpha = 1, lambda = seq(0, 0.15, length = 30)))
max(lasso$results$Accuracy)
lasso$bestTune
lasso.plot <- lasso %>%
ggplot(aes(x = lambda, y = Accuracy)) +
geom_line() +
geom_point() +
geom_text(aes(label = sprintf("%.3f", Accuracy)), check_overlap = TRUE, vjust = -0.5, size = 2.5) +
scale_x_continuous(limits = c(0, 0.10)) +
labs(x = TeX("Lambda ($\\lambda$)"), y = "Accuracy", title = "Accuracy vs. Lambda for Lasso Regularization") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold")
)
lasso.plot <- lasso %>%
ggplot(aes(x = lambda, y = Accuracy)) +
geom_line() +
geom_point() +
geom_text(aes(label = sprintf("%.3f", Accuracy)), check_overlap = TRUE, vjust = -0.5, size = 2.5) +
scale_x_continuous(limits = c(0, 0.10)) +
labs(x = TeX("Lambda ($\\lambda$)"), y = "Accuracy", title = "Accuracy vs. Lambda for Lasso Regularization") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold")
)
#RIDGE
set.seed(1)
ridge <- train(Churn ~ ., data = data.train, method = "glmnet", metric = "Accuracy", trControl = ctrl, tuneGrid = expand.grid(alpha = 0, lambda = seq(0, 0.15, length = 30)))
max(ridge$results$Accuracy)
ridge$bestTune
ridge.plot <- ridge %>%
ggplot(aes(x = lambda, y = Accuracy)) +
geom_line() +
geom_point() +
geom_text(aes(label = sprintf("%.3f", Accuracy)), check_overlap = TRUE, vjust = -0.5, size = 2.5) +
labs(x = TeX("Lambda ($\\lambda$)"), y = "Accuracy", title = "Accuracy vs. Lambda for Ridge Regularization") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold")
)
grid.arrange(lasso.plot, ridge.plot, top = "Penalized Approaches for Logistic Regression")
# CLUSTERING PRIMA BOZZA
# preparing data
clustering_data <- read.csv("TelecomChurn.csv")
#keep only numeric
clustering_data <- clustering_data[, sapply(clustering_data, is.numeric)]
# scale data
clustering_data <- scale(clustering_data)
# number of K
# elbow rule plot
fviz_nbclust(clustering_data, kmeans, method = "wss") +
labs(subtitle = "WSS - Elbow method")
# avg silhouette plot
fviz_nbclust(clustering_data, kmeans, method = "silhouette") +
labs(subtitle = "Silhouette method")
# K-means clustering
set.seed(123)
kmeans_model <- kmeans(clustering_data, centers = 2, nstart = 25)
kmeans_model
# silhouette results
silhouette <- silhouette(kmeans_model$cluster, dist(clustering_data))
mean(silhouette[, 3])
# visualization
fviz_cluster(kmeans_model, data = clustering_data, geom = "point", stand = FALSE, ellipse.type = "convex") +
labs(title = "K-means Clustering") +
theme_minimal()
# re add the categorical
clustering_data <- cbind(clustering_data, categorical.vars)
# re add the categorical
clustering_data <- cbind(clustering_data, qualitative.vars)
# re add the categorical
clustering_data <- cbind(clustering_data, categorical_vars )
# CLUSTERING PRIMA BOZZA
# preparing data
clustering_data <- read.csv("TelecomChurn.csv")
#keep only numeric
clustering_data <- clustering_data[, sapply(clustering_data, is.numeric)]
# scale data
clustering_data <- scale(clustering_data)
# re add the categorical international plan, voice mail plan, and region
clustering_data <- cbind(clustering_data, data$International.plan, data$Voice.mail.plan)
# re add the categorical international plan, voice mail plan, and region
clustering_data <- cbind(clustering_data, clustering_data$International.plan, clustering_data$Voice.mail.plan)
# re add the categorical variable
clustering_data <- cbind(clustering_data, categorical_vars)
# re add the categorical variable
clustering_data <- cbind(clustering_data, categorical_vars)
data = read.csv("TelecomChurn.csv")
anyDuplicated(data) # no duplicates
# converting characters to factor
data$State = as.factor(data$State)
data$International.plan = as.factor(data$International.plan)
data$Voice.mail.plan = as.factor(data$Voice.mail.plan)
data$Churn = as.factor(data$Churn)
# checking the distribution of our target
# We will first calculate the p
continuous_vars <- data[, sapply(data, is.numeric)]
#print(continuous_vars)
categorical_vars <- data[, sapply(data, is.factor) & !names(data) %in% "Churn"]
#print(categorical_vars)
# univariate analysis for categorical variables (State, International.plan, Voice.mail.plan)
# re add the categorical variable
clustering_data <- cbind(clustering_data, categorical_vars)
head(clustering_data)
# re add the categorical variable
clustering_data <- cbind(clustering_data, categorical_vars)
head(clustering_data)
clustering_data$International.plan <- ifelse(clustering_data$International.plan == "yes", 1, 0)
clustering_data$Voice.mail.plan <- ifelse(clustering_data$Voice.mail.plan == "yes", 1, 0)
clustering_data$State <- NULL
head(clustering_data)
clustering_data$International.plan <- ifelse(clustering_data$International.plan == "yes", 1, 0)
clustering_data$Voice.mail.plan <- ifelse(clustering_data$Voice.mail.plan == "yes", 1, 0)
head(clustering_data)
clustering_data$State <- NULL
head(clustering_data)
clustering_data$International.plan <- as.numeric(clustering_data$International.plan) - 1
clustering_data$Voice.mail.plan <- as.numeric(clustering_data$Voice.mail.plan) - 1
head(clustering_data)
# convert the binary international and voice in 0 and 1
clustering_data$International.plan <- as.numeric(clustering_data$International.plan)
clustering_data$Voice.mail.plan <- as.numeric(clustering_data$Voice.mail.plan)
head(clustering_data)
