library(ggplot2)
library(corrplot)
library(dplyr)
library(tidyr)
library(skimr)
library(treemapify)
library(stringr)
library(glue)
library(dummy)
library(caret)
library(car)
library(pROC)
library(glmnet)
library(rpart)
library(rpart.plot)
library(randomForest)
library(pROC)
library(cluster)
library(factoextra)
library(sf)
library(latex2exp)
library(gridExtra)
library(tree)
library(randomForest)
library(xgboost)
library(dendextend)
library(purrr)
library(ROSE)
data = read.csv("TelecomChurn.csv")
head(data)
skim(data)
data$State = as.factor(data$State)
data$International.plan = as.factor(data$International.plan)
data$Voice.mail.plan = as.factor(data$Voice.mail.plan)
data$Churn = as.factor(data$Churn)
churn_percent <- data %>%
group_by(Churn) %>%
summarise(Count = n(), .groups = "drop") %>%
mutate(Percent = Count / sum(Count) * 100)
ggplot(churn_percent, aes(x = "", y = Percent, fill = Churn)) +
geom_col(width = 1, color = 1) +
coord_polar("y", start = 0) +
theme_void() +
scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
labs(title = "Percentage of Churn", fill = "Churn") +
geom_text(aes(label = paste0(round(Percent, 1), "%")), position = position_stack(vjust = 0.5)) +
theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 18))
state.count <- data %>% group_by(State) %>% summarise(count = n())
ggplot(state.count, aes(area = count, fill = count, label = glue("{State}\n{count}"))) +
geom_treemap() +
geom_treemap_text(colour = "white", place = "center", size = 13, grow = TRUE) +
labs(title = "Tree Map for State Distribution", fill = "State") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16, colour = "gray5"),
legend.position = "none",
plot.background = element_rect(fill = "#f6fff8")
) +
scale_fill_viridis_c()
continuous <- data[, sapply(data, is.numeric) | names(data) == "Churn"]
continuous %>%
pivot_longer(cols = -Churn, names_to = "metric", values_to = "value") %>%
ggplot(aes(value, color = as.factor(Churn))) +
geom_density(alpha = 0.3) +
facet_wrap(vars(metric), scales = "free") +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
scale_color_manual(name = "Churn", labels = c("No", "Yes"), values = c("#15aabf", "#ff8787")) +
labs(title = "Numeric Features", subtitle = "Univariate Distribution by Churn") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 18, colour = "gray5"),
plot.subtitle = element_text(hjust = 0.5, colour = "gray5"),
axis.text.x = element_text(angle = 45, hjust = 1),
axis.title.x = element_text(face = "bold"),
axis.title.y = element_text(face = "bold"),
legend.position = "top"
)
data %>%
group_by(Customer.service.calls, Churn) %>%
summarise(Count = n(), .groups = "drop") %>%
ggplot(aes(x = Customer.service.calls, y = Count, color = Churn)) +
geom_line() +
geom_point() +
labs(title = "Churn by Customer Service Calls", x = "Customer Service Calls", y = "Count", color = "Churn") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5))
long_data <- data %>%
select(Churn, Total.day.minutes, Total.day.charge) %>%
pivot_longer(cols = c(Total.day.minutes, Total.day.charge),
names_to = "Measure",
values_to = "Value")
ggplot(long_data, aes(x = Churn, y = Value, fill = Churn)) +
geom_boxplot() +
facet_wrap(~ Measure, scales = "free_y") +
labs(title = "Churn by Total Day Minutes and Total Day Charge",
x = "Churn",
y = "Value",
fill = "Churn") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5),
strip.background = element_blank(),
strip.text.x = element_text(size = 12)
)
service_calls <- data %>%
filter(Customer.service.calls > 0)
churn_percent <- service_calls %>%
group_by(Customer.service.calls, Churn) %>%
summarise(Count = n(), .groups = "drop") %>%
mutate(Percent = Count / sum(Count) * 100)
ggplot(churn_percent, aes(x = Customer.service.calls, y = Percent, fill = Churn)) +
geom_col(width = 0.7, color = 1) +
labs(title = "Churn by Customer Service Calls", x = "Customer Service Calls", y = "Percent", fill = "Churn") +
theme_minimal() +
scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes")) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),
legend.position = "bottom"
)
set.seed(1)
data$Churn <- as.factor(data$Churn)
model <- glm(Churn ~ Customer.service.calls, family = binomial, data = data)
summary(model)
call_range <- seq(min(data$Customer.service.calls, na.rm = TRUE),
max(data$Customer.service.calls, na.rm = TRUE),
length.out = 100)
newdata <- data.frame(Customer.service.calls = call_range)
newdata$Probability <- predict(model, newdata = newdata, type = "response")
newdata$Odds <- newdata$Probability / (1 - newdata$Probability)
head(newdata)
ggplot(newdata, aes(x = Customer.service.calls, y = Probability)) +
geom_line() +
labs(title = "Effect of Customer Service Calls on the Probability of Churn",
x = "Number of Customer Service Calls",
y = "Probability of Churn") +
scale_y_continuous(labels = scales::percent_format()) +  # Convert y-axis into percentage format
theme_minimal() +  # Use a minimal theme
geom_hline(yintercept = 0.5, colour = "red", linetype = "dashed")
theme(
plot.title = element_text(hjust = 0.5),
axis.text.x = element_text(angle = 45, hjust = 1),
axis.title.x = element_text(face = "bold"),
axis.title.y = element_text(face = "bold")
)
set.seed(1)
majority_indices <- which(data$Churn == 'False')
minority_count <- sum(data$Churn == 'True')
sampled_indices <- sample(majority_indices, size = minority_count, replace = FALSE)
balanced_data <- data[c(sampled_indices, which(data$Churn == 'True')), ]
levels(data$Churn) # original levels --> i needed this step since i was getting some errors due to the factorization
balanced_data$Churn <- as.factor(balanced_data$Churn)
levels(balanced_data$Churn)
table(balanced_data$Churn)
balanced_model <- glm(Churn ~ Customer.service.calls, family = binomial, data = balanced_data )
summary(balanced_model)
call_range <- seq(min(data$Customer.service.calls, na.rm = TRUE),
max(data$Customer.service.calls, na.rm = TRUE),
length.out = 100)
newdata <- data.frame(Customer.service.calls = call_range)
newdata$Probability <- predict(balanced_model, newdata = newdata, type = "response")
newdata$Odds <- newdata$Probability / (1 - newdata$Probability)
ggplot(newdata, aes(x = Customer.service.calls, y = Probability)) +
geom_line() +
labs(title = "Effect of Customer Service Calls on the Probability of Churn (Balanced Data)",
x = "Number of Customer Service Calls",
y = "Probability of Churn") +
scale_y_continuous(labels = scales::percent_format()) +  # Convert y-axis into percentage format
theme_minimal() +  # Use a minimal theme
theme(
plot.title = element_text(hjust = 0.5),
axis.text.x = element_text(angle = 45, hjust = 1),
axis.title.x = element_text(face = "bold"),
axis.title.y = element_text(face = "bold")
)
confusion.mat <- function(data, model, target, threshold) {
predicted_Y <- ifelse(predict(model, type = "response", newdata = data) > threshold, 'True', 'False')
actual_Y <- data[[target]]
True.positive <- sum(predicted_Y == 'True' & actual_Y == 'True')
True.negative <- sum(predicted_Y == 'False' & actual_Y == 'False')
False.positive <- sum(predicted_Y == 'True' & actual_Y == 'False')
False.negative <- sum(predicted_Y == 'False' & actual_Y == 'True')
Confusion.Matrix <- matrix(c(True.positive, False.negative,
False.positive, True.negative),
nrow = 2, byrow = TRUE)
rownames(Confusion.Matrix) <- c("Actual Positive", "Actual Negative")
colnames(Confusion.Matrix) <- c("Predicted Positive", "Predicted Negative")
print(Confusion.Matrix)
}
confusion.mat(data, model, "Churn", 0.5) #Predictions on unbalanced data
confusion.mat(balanced_data,balanced_model, "Churn", 0.5) # Predictions on balanced data
unique(data$State)
map_region <- function(state) {
west.regions <- c("WA", "OR", "ID", "MT", "WY", "CA", "NV", "UT", "CO", "AK", "HI")
southwest.regions <- c("AZ", "NM", "TX", "OK")
midwest.regions <- c("ND", "SD", "NE", "KS", "MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH")
southeast.regions <- c("AR", "LA", "MS", "AL", "TN", "KY", "WV", "VA", "NC", "SC", "GA", "FL")
northeast.regions <- c("ME", "NH", "VT", "MA", "RI", "CT", "NY", "PA", "NJ", "DE", "MD", "DC")
if (state %in% west.regions) {
return("West")
} else if (state %in% midwest.regions) {
return("Mid West")
} else if (state %in% southeast.regions) {
return("South East")
} else if (state %in% northeast.regions) {
return("North East")
} else {
return("South West")  # in case there are any states not covered
}
}
data$Region <- sapply(data$State, map_region)
data$Region <- as.factor(data$Region)
state.column <- data$State
data$State <- NULL
head(data)
group_plt(Region)
contingency.table <- table(data$Region, data$Churn)
chisq.test(contingency.table)
ggplot(data, aes(x = Region, fill = Churn)) +
geom_bar(width = 0.7, color = 1) +
theme_minimal() +
theme(axis.text.x = element_text(hjust = 0, colour = "gray29", size = 10)) +
labs(title = "Churn by Region", x = "Region", y = "Count", fill = "Churn", subtitle = "Is Churn rate influenced by the Region?") +
theme(
axis.text = element_blank(),
axis.title = element_blank(),
legend.position = "bottom",
plot.title = element_text(hjust = 0.5, face = "bold"),
plot.subtitle = element_text(hjust = 0.5)
) +
scale_fill_manual(values = c("False" = "#ffe8cc", "True" = "#ff8787"), labels = c("False" = "No", "True" = "Yes"))
set.seed(1)
ids.train <- sample(1:nrow(data), size = 0.75 * nrow(data), replace = FALSE)
data.train <- data[ids.train,]
data.val <- data[-ids.train,]
# Oversampling minority class
data.train.balanced <- ovun.sample(Churn ~ ., data = data.train, method = "over", N=4260)$data
table(data.train.balanced$Churn)
cols_to_scale <- c("Account.length", "Area.code", "Number.vmail.messages", "Total.day.calls", "Total.day.charge", "Total.eve.calls", "Total.eve.charge", "Total.night.calls", "Total.night.charge", "Total.intl.calls", "Total.intl.charge", "Customer.service.calls")
train.mean <- apply(data.train.balanced[, cols_to_scale], MARGIN = 2, FUN = mean)
train.sd <- apply(data.train.balanced[, cols_to_scale], MARGIN = 2, FUN = sd)
data.train.balanced[, cols_to_scale] <- scale(data.train.balanced[, cols_to_scale], center = train.mean, scale = train.sd)
# Scale validation data using training data's parameters
data.val[, cols_to_scale] <- scale(data.val[, cols_to_scale], center = train.mean, scale = train.sd)
logistic.baseline <- glm(Churn ~ ., data = data.train.balanced, family = "binomial")
summary(logistic.baseline)
baseline.pred <- ifelse(predict(logistic.baseline, newdata = data.val) > 0.5, 1, 0)
(baseline.cm <- table(baseline.pred, data.val$Churn))
get.metrics<- function(conf.mat) {
true.positives <- conf.mat[2,2]
true.negatives <- conf.mat[1,1]
false.positives <- conf.mat[1,2]
false.negatives <- conf.mat[2,1]
num.observations <- true.positives + true.negatives + false.positives + false.negatives
accuracy <- (true.positives + true.negatives) / num.observations
precision <- (true.positives) / (true.positives + false.positives)
recall <- true.positives / (true.positives + false.negatives)
f1 <- 2 * ((precision * recall) / (precision + recall))
metrics <- data.frame(t(c(accuracy, precision, recall, f1)))
columns <- c("Accuracy", "Precision", "Recall", "F1")
colnames(metrics) <- columns
return(metrics)
}
(baseline.metrics <- get.metrics(baseline.cm))
akaike.fw <- step(glm(Churn ~ 1, family = "binomial", data = data.train.balanced), scope = formula(logistic.baseline), direction = "forward")
akaike.back <- step(logistic.baseline, direction = "backward")
akaike.both <- step(logistic.baseline, direction = "both")
bayesian.fw <- step(glm(Churn ~ 1, family = "binomial", data = data.train.balanced), scope = formula(logistic.baseline), direction = "forward", k = log(nrow(data.train.balanced)))
bayesian.back <- step(logistic.baseline, direction = "backward", k = log(nrow(data.train.balanced)))
bayesian.both <- step(logistic.baseline, direction = "both", k = log(nrow(data.train.balanced)))
akaike.preds <- ifelse(predict(akaike.both, data.val) > 0.5, 1, 0)
(akaike.cm <- table(akaike.preds, data.val$Churn))
(akaike.metrics <- get.metrics(akaike.cm))
bayesian.preds <- ifelse(predict(bayesian.both, data.val) > 0.5, 1, 0)
(bayesian.cm <- table(bayesian.preds, data.val$Churn))
(bayesian.metrics <- get.metrics(bayesian.cm))
par(mfrow = c(2, 2))
roc_full <- roc(data.val$Churn, as.numeric(baseline.pred),
plot = TRUE, main = "ROC Curve Full Model", col = "purple", lwd = 3,
auc.polygon = TRUE, print.auc = TRUE)
roc_akaike <- roc(data.val$Churn, predict(akaike.both, newdata = data.val, type = "response"),
plot = TRUE, main = "AIC Model", col = "blue", lwd = 3,
auc.polygon = TRUE, print.auc = TRUE)
roc_bayesian <- roc(data.val$Churn, predict(bayesian.both, newdata = data.val, type = "response"),
plot = TRUE, main = "BIC Model", col = "red", lwd = 3,
auc.polygon = TRUE, print.auc = TRUE)
par(mfrow = c(1, 1))
auc_akaike <- as.numeric(auc(roc(data.val$Churn, predict(akaike.both, newdata = data.val, type = "response"))))
auc_bayesian <- as.numeric(auc(roc(data.val$Churn, predict(bayesian.both, newdata = data.val, type = "response"))))
auc_full <- as.numeric(auc(roc(data.val$Churn, baseline.pred)))
akaike_df <- data.frame(Model = "Akaike",
Accuracy = akaike.metrics$Accuracy,
Precision = akaike.metrics$Precision,
Recall = akaike.metrics$Recall,
F1_Score = akaike.metrics$F1,
AUC = auc_akaike)
bayesian_df <- data.frame(Model = "Bayesian",
Accuracy = bayesian.metrics$Accuracy,
Precision = bayesian.metrics$Precision,
Recall = bayesian.metrics$Recall,
F1_Score = bayesian.metrics$F1,
AUC = auc_bayesian)
full_df <- data.frame(Model = "Full Logistic",
Accuracy = baseline.metrics$Accuracy,
Precision = baseline.metrics$Precision,
Recall = baseline.metrics$Recall,
F1_Score = baseline.metrics$F1,
AUC = auc_full)
comparison_df <- bind_rows(akaike_df, bayesian_df, full_df)
comparison_df
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)
lasso <- train(Churn ~ ., data = data.train.balanced, method = "glmnet", metric = "Accuracy", trControl = ctrl, tuneGrid = expand.grid(alpha = 1, lambda = seq(0, 0.15, length = 30)))
max(lasso$results$Accuracy)
lasso$bestTune
lasso.predict <- predict(lasso, data.val)
lasso.cm <- table(lasso.predict, data.val$Churn)
lasso.metrics <- get.metrics(lasso.cm)
lasso.plot <- lasso %>%
ggplot(aes(x = lambda, y = Accuracy)) +
geom_line() +
geom_point() +
geom_text(aes(label = sprintf("%.3f", Accuracy)), check_overlap = TRUE, vjust = -0.5, size = 2.5) +
scale_x_continuous(limits = c(0, 0.10)) +
labs(x = TeX("Lambda ($\\lambda$)"), y = "Accuracy", title = "Accuracy vs. Lambda for Lasso Regularization") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold")
)
set.seed(1)
ridge <- train(Churn ~ ., data = data.train.balanced, method = "glmnet", metric = "Accuracy", trControl = ctrl, tuneGrid = expand.grid(alpha = 0, lambda = seq(0, 0.15, length = 30)))
max(ridge$results$Accuracy)
ridge$bestTune
ridge.plot <- ridge %>%
ggplot(aes(x = lambda, y = Accuracy)) +
geom_line() +
geom_point() +
geom_text(aes(label = sprintf("%.3f", Accuracy)), check_overlap = TRUE, vjust = -0.5, size = 2.5) +
labs(x = TeX("Lambda ($\\lambda$)"), y = "Accuracy", title = "Accuracy vs. Lambda for Ridge Regularization") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold")
)
ridge.predict <- predict(ridge, data.val)
ridge.cm <- table(ridge.predict, data.val$Churn)
ridge.metrics <- get.metrics(ridge.cm)
par(mfrow = c(1,2))
auc.lasso <- roc(data.val$Churn, as.numeric(lasso.predict),
plot = TRUE, main = "ROC Curve Lasso Model", col = "purple", lwd = 3,
auc.polygon = TRUE, print.auc = TRUE)
auc.ridge <- roc(data.val$Churn, as.numeric(ridge.predict),
plot = TRUE, main = "ROC Curve Ridge Model", col = "black", lwd = 3,
auc.polygon = TRUE, print.auc = TRUE)
grid.arrange(lasso.plot, ridge.plot, top = "Penalized Approaches for Logistic Regression")
lasso_df <- data.frame(Model = "Logistic Lasso",
Accuracy = lasso.metrics$Accuracy,
Precision = lasso.metrics$Precision,
Recall = lasso.metrics$Recall,
F1_Score = lasso.metrics$F1,
AUC = as.numeric(auc.lasso$auc))
ridge_df <- data.frame(Model = "Logistic Ridge",
Accuracy = ridge.metrics$Accuracy,
Precision = ridge.metrics$Precision,
Recall = ridge.metrics$Recall,
F1_Score = ridge.metrics$F1,
AUC = as.numeric(auc.ridge$auc))
comparison_df <- rbind(comparison_df, ridge_df, lasso_df)
comparison_df
set.seed(1)
ids.train <- sample(1:nrow(data), size = 0.75 * nrow(data), replace = FALSE)
data.train <- data[ids.train,]
data.val <- data[-ids.train,]
data.train.balanced <- ovun.sample(Churn ~ ., data = data.train, method = "over", N=4260)$data
table(data.train.balanced$Churn)
set.seed(1) # otherwise results might be inconsistent due to ties
tree.full <- tree(Churn ~ ., data = data.train.balanced)
plot(tree.full)
text(tree.full, pretty = 0, cex = 0.7)
summary(tree.full)
tree.pred <- predict(tree.full, data.val, type = "class")
table(tree.pred, data.val$Churn)
get.metrics(table(tree.pred, data.val$Churn))
cv.trees <- cv.tree(tree.full, FUN = prune.misclass)
(optimal.size <- cv.trees$size[which.min(cv.trees$dev)])
par(mfrow = c(1,2))
plot(cv.trees$size, cv.trees$dev/nrow(data.val), type = "b", xlab = "Size", ylab = "Error")
plot(cv.trees$k, cv.trees$dev/nrow(data.val), type = "b", col = 3, xlab = "K", ylab = "Error")
pruned.tree <- prune.misclass(tree.full, k = 0, best = optimal.size)
pruned.tree
plot(pruned.tree)
text(pruned.tree, pretty = 0, cex = 0.7)
pruned.pred <- predict(pruned.tree, data.val, type = "class")
table(pruned.pred, data.val$Churn)
trees.metrics <- get.metrics(table(pruned.pred, data.val$Churn))
trees.auc <- roc(data.val$Churn, as.numeric(pruned.pred))
cv_trees_df <- data.frame(Model = "CV Decision Trees",
Accuracy = trees.metrics$Accuracy,
Precision = trees.metrics$Precision,
Recall = trees.metrics$Recall,
F1_Score = trees.metrics$F1,
AUC = as.numeric(trees.auc$auc))
comparison_df <- rbind(comparison_df, cv_trees_df)
comparison_df
set.seed(1)
fit.bag <- randomForest(Churn ~ ., data = data.train.balanced, mtry = ncol(data.train.balanced) - 1, ntree = 500, importance = T)
importance(fit.bag)
layout(matrix(c(1,2),nrow=1),
width=c(4,1))
par(mar=c(5,4,4,0)) # No margin right side
plot(fit.bag, log="y")
par(mar=c(5,0,4,2)) # No margin left side
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("bottom", colnames(fit.bag$err.rate),col=1:4,cex=0.8,fill=1:4)
varImpPlot(fit.bag)
mean(fit.bag$err.rate[,1]) # OOB test error estimation
validation.preds <- predict(fit.bag, newdata = data.val)
(bag.missclassification <- mean(validation.preds != data.val$Churn)) # True test error estimation
# If we used more trees the OOB test error estimation would have been closer to the true test error
table(validation.preds, data.val$Churn)
rf.metrics <- get.metrics(table(validation.preds, data.val$Churn))
rf.auc <- roc(data.val$Churn, as.numeric(validation.preds))
rf_df <- data.frame(Model = "Random Forests",
Accuracy = rf.metrics$Accuracy,
Precision = rf.metrics$Precision,
Recall = rf.metrics$Recall,
F1_Score = rf.metrics$F1,
AUC = as.numeric(rf.auc$auc))
comparison_df <- rbind(comparison_df, rf_df)
comparison_df
X.tr <- model.matrix(Churn ~ ., data = data.train.balanced)[, -1]
y.tr <- as.numeric(data.train.balanced$Churn)-1
X.val <- model.matrix(Churn ~ ., data = data.val)[, -1]
y.val <- as.numeric(data.val$Churn)-1
fit.xg <- xgboost(as.matrix(X.tr), label = y.tr, nrounds = 50, objective = "binary:logistic", eval_metric = "error")
xg.pred <- ifelse(predict(fit.xg, X.val)> 0.5, 1, 0)
mean(xg.pred != y.val)
table(xg.pred, data.val$Churn)
get.metrics(table(xg.pred, data.val$Churn))
train.errors <- fit.xg$evaluation_log$train_error
val.errors <- numeric(50)
for (i in 1:50) {
pred_i = ifelse(predict(fit.xg, X.val, ntreelimit = i) > 0.5, 1, 0)
val.errors[i] = mean(pred_i != y.val)
}
plot(1:50, val.errors, type="b", xlab="number of trees", ylab="error", col=3, ylim = c(0,0.3))
points(1:50, train.errors, type="b")
legend("topright", legend = c("Train", "Test"), col=c(1, 3), lty=1, lwd=2, cex = 1)
which.min(val.errors)
abline(v = which.min(val.errors))
(feature.importance <- xgb.importance(model = fit.xg))
xgb.plot.importance(importance_matrix = feature.importance)
fitControl = trainControl(
method = "cv",
number = 10,
search="random")
tune_grid =
expand.grid(
nrounds = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100),
eta = 0.3,
max_depth=5,
subsample = 1,
colsample_bytree = 1,
min_child_weight = 5,
gamma = c(0.1, 0.2, 0.5, 0.75, 1)
)
set.seed(1)
fit_xg_cv = train(Churn ~ ., data = data.train.balanced,
method = "xgbTree",
trControl = fitControl,
verbose = FALSE,
tuneGrid = tune_grid,
objective = "binary:logistic",
eval_metric = "error")
pred_xg_cv = predict(fit_xg_cv, data.val, type="raw")
mean(pred_xg_cv != data.val$Churn)
table(pred_xg_cv, data.val$Churn)
xgb.metrics <- get.metrics(table(pred_xg_cv, data.val$Churn))
auc.xgb <- roc(data.val$Churn, as.numeric(pred_xg_cv))
xgb_df <- data.frame(Model = "CV XGB",
Accuracy = xgb.metrics$Accuracy,
Precision = xgb.metrics$Precision,
Recall = xgb.metrics$Recall,
F1_Score = xgb.metrics$F1,
AUC = as.numeric(auc.xgb$auc))
comparison_df <- rbind(comparison_df, xgb_df)
comparison_df
set.seed(1)
data$Churn <- as.factor(data$Churn)
model <- glm(Churn ~ Customer.service.calls, family = binomial, data = data)
summary(model)
head(data)
set.seed(1)
data$Churn <- as.factor(data$Churn)
model <- glm(Churn ~ Total.day.charge*Total.night.charge, family = binomial, data = data)
summary(model)
set.seed(1)
data$Churn <- as.factor(data$Churn)
model <- glm(Churn ~ Total.day.charge*Total.night.charge, family = binomial, data = data)
summary(model)
set.seed(1)
data$Churn <- as.factor(data$Churn)
model <- glm(Churn ~ International.plan*Voice.mail.plan, family = binomial, data = data)
summary(model)
set.seed(1)
data$Churn <- as.factor(data$Churn)
model <- glm(Churn ~ Customer.service.calls*International.plan, family = binomial, data = data)
summary(model)
source("~/Desktop/Telecom-Churn/telecom_churn.R")
