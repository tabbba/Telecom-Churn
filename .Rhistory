x_tr_num <- x_tr[ , -3]
x_te_num <- x_te[ , -3]
# Why standardization? Look at some plot
par(mfrow = c(2,2))
hist(x_tr_num$tax,
breaks = 10,
freq = T,
border = 0,
col = rgb(0,0,0.6,0.4),
ann = F,
axes = F,
ylim = c(0,100))
axis(1, cex.axis = .9, at = seq(100, 800, 50), line = -.3)
axis(2, las = 2 , cex.axis = .9)
mtext("Property tax", side = 3, cex = 1.5, line = 1)
abline(v = mean(x_tr_num$tax), lwd = 2)
text(mean(x_tr_num$tax)+40, 90, "mean", cex = 1)
abline(v = median(x_tr_num$tax), lwd = 2, col = 2)
text(median(x_tr_num$tax)-50, 90, "median", cex = 1, col = 2)
hist(x_tr_num$nox,
breaks = 10,
freq = T,
border = 0,
col = rgb(0,0,0.6,0.4),
ann = F,
axes = F,
ylim = c(0,100))
axis(1, cex.axis = .9, at = seq(0.3, 1, 0.05), line = -.3)
axis(2, las = 2 , cex.axis = .9)
mtext("Nitrogen oxides concentration", side = 3, cex = 1.5, line = 1)
abline(v = mean(x_tr_num$nox), lwd = 2)
text(mean(x_tr_num$nox)+0.05, 90, "mean", cex = 1)
abline(v = median(x_tr_num$nox), lwd = 2, col = 2)
text(median(x_tr_num$nox)-0.05, 90, "median", cex = 1, col = 2)
boxplot(x_tr_num$tax, col = rgb(0,0,0.6,0.4))
boxplot(x_tr_num$nox, col = rgb(0,0,0.6,0.4))
par(mfrow = c(1,1))
# Huge difference between the distributions:
# - Higher mean and variance for the property tax variable
# - Property tax is heavily right-skewed, resulting in a big
#   distance between median and third quartile and between median
#   and mean
# standardization
x_tr_num_s <- scale(x_tr_num)
train_m <- apply(x_tr_num,
MARGIN = 2,
FUN = mean)
train_s <- apply(x_tr_num,
MARGIN = 2,
FUN = sd)
x_te_num_s <- scale(x_te_num,
center = train_m,
scale = train_s)
# Remember to always scale the test set using mean and sd from the training.
# Re-append dummy variable 'chas'
x_tr_s <- cbind(chas = x_tr['chas'],
x_tr_num_s)
x_te_s <- cbind(chas = x_te['chas'],
x_te_num_s)
# High-dimensional setting ---------------------------------------------------------
# If the num. of observations is not much larger than
# the num. of variables, then there can be a lot of
# variability in the least squares fit -> overfitting
# In the extreme case of num. of variables bigger then num.
# of observations, there are infnitely many solutions with,
# typically, very poor performance on new data (test)
### Collinearity (not only for high-dimensional data) ###
# If one or more predictor variables are linearly related with each other,
# the data matrix is nearly singular -> unstable coefficient estimates.
### Common solutions ###
# - Subset Selection
# - Shrinkage/Regularization
# - PCA
# Stepwise subset selection ---------------------------------------------------------
# Classical least squares solution for a linear model that contains
# a subset of the predictors.
train_dat <- cbind(x_tr_s, crim = y_tr)
# full model
full_model <- lm(crim ~ ., data = train_dat)
summary(full_model)
# Perform stepwise variable selection using AIC
stepwise_model_aic <- step(full_model, direction = "both")
# Perform stepwise variable selection using BIC
stepwise_model_bic <- step(full_model, direction = "both",
k = log(nrow(train_dat)))
# selected variables
coefficients(full_model)
coefficients(stepwise_model_aic)
coefficients(stepwise_model_bic)
# bic favor simpler models
# once we have the model we can predict on the test data
pred_full <- predict(full_model, x_te_s)
pred_stepwise_aic <- predict(stepwise_model_aic, x_te_s)
pred_stepwise_bic <- predict(stepwise_model_bic, x_te_s)
# model performances
(mse_full <- mean((y_te - pred_full)^2))
(mse_aic <- mean((y_te - pred_stepwise_aic)^2))
(mse_bic <- mean((y_te - pred_stepwise_bic)^2))
# little difference in this case
# Shrinkage -----------------------------------------------------------
# Fit a model containing all the predictors constraining the
# coefficient estimates -> estimates are shrinked towards zero
# Shrinking can reduce variance of the estimates
# Common approaches:
# - LASSO
# - RIDGE
# - ELASTIC NET
######      Lasso      ######
# Function to fit a GLM with lasso or elastic-net regularization
?glmnet
# Among other arguments, alpha is the elastic-net mixing parameter.
# This means that alpha is in [0, 1], and:
# - if alpha = 0, we have ridge penalty;
# - if alpha = 1, we have lasso penalty.
# Standard fit
lasso_fit <- glmnet(x = x_tr_s,
y = y_tr,
alpha = 1)   # default value
lasso_fit
# For each lambda, a model is fitted.
# This also means that, for each lambda, we have a distinct vector of
# regression coefficients.
lasso_fit$beta
View(as.matrix(lasso_fit$beta))   # with names and zero values
# Plot coefficient values against the L1 norm of the coefficients
plot(lasso_fit,
label = T,       # to plot the variables' indices
xvar = 'norm')   # default value
# Plot coefficient values against the (log-)lambda sequence
plot(lasso_fit,
label = T,
xvar = "lambda")
# Plot coefficient values against the proportion of explained variance w.r.t
# the null model
plot(lasso_fit,
label = T,
xvar = 'dev')
# Single out lambda and beta
(lam <- lasso_fit$lambda)
(bet <- lasso_fit$beta)
# Fit for the last lambda
lam[length(lam)]                     #last lambda value considered
bet[ , length(lam)]                  #estimated coefficients
sum(abs(bet[ , length(lam)]))        #L1 norm
sqrt(sum(bet[ , length(lam)] ^ 2))   #L2 norm
# Fit for the first lambda
lam[1]                     #first lambda value considered
bet[ , 1]                  #estimated coefficients
sum(abs(bet[ , 1]))        #L1 norm
sqrt(sum(bet[ , 1] ^ 2))   #L2 norm
# Let's pick one in between: e.g., the 20th
lam[20]                     #20th lambda value considered
bet[ , 20]                  #estimated coefficients
sum(abs(bet[ , 20]))        #L1 norm
sqrt(sum(bet[ , 20] ^ 2))   #L2 norm
# Plot coefficients values against the L2 norm (useful for RIDGE)
(l2n <- apply(bet,
MARGIN = 2,
FUN = function(x) sqrt(sum(x^2))))
matplot(l2n,
t(bet),
type = 'l',
xlab = 'L2 Norm',
ylab = 'Coefficients')
# Predict on test
View(predict(lasso_fit,
newx = as.matrix(x_te_s)))
matplot(l2n,
t(bet),
type = 'l',
xlab = 'L2 Norm',
ylab = 'Coefficients')
# Predict on test
View(predict(lasso_fit,
newx = as.matrix(x_te_s)))
# We can obtain the prediction for a single value (i.e., the first) by using the
# optional argument 's'.
predict(lasso_fit,
newx = as.matrix(x_te_s),
s = lam[1])
table(predict(lasso_fit,
newx = as.matrix(x_te_s),
s = lam[1]))
# Why constant prediction? All 0 coefficients...
mean(y_tr) # the predictions are just the mean over the training set
# The optional argument 's' also allows to set lambda to a specific value.
predict(lasso_fit,
newx = as.matrix(x_te_s),
s = 0.809)
# Since the sequence of lambdas is estimated starting from the data, it is
# different for different folds. For the sake of comparison, let's build our own
# sequence.
my_lam <- seq(from = 0.001, to = 2, length.out = 100)
# Number of folds
nfolds <- 5
# Create folds by hand
set.seed(123)
folds_idx <- sample(1:nfolds,
size = n_tr,
replace = TRUE)
table(folds_idx)
# We could use more sophisticated methods to ensure that they
# are of equal size also for finite samples...
kk <- 1:nfolds
id_out <- sapply(kk, function(k) return(seq(ceiling(((n_tr/5)*(k-1))+1),
min(c(ceiling(n_tr/5)*k,n_tr)),
by = 1)))
set.seed(123)
folds_idx1 <- sapply(1:nfolds,
function(i)
return(sample(1:n_tr,
size = n_tr,
replace = F)[id_out[[i]]]))
unlist(lapply(folds_idx1, function(x)length(x)))
# Creating folds using caret
?caret::createFolds
folds_idx <- caret::createFolds(y = y_tr,
k = nfolds,
list = FALSE)
table(folds_idx)
# Using list = TRUE, it is easier to work with functions from the apply family.
folds <- caret::createFolds(y = y_tr,
k = nfolds,
list = TRUE)
folds
# Compute the CV scores (MSE) for each lambda and each fold
cv_scores <- sapply(folds,
function (f) {
# Lasso fit for this fold
lf_this <- glmnet(x = x_tr_s[-f, ],
y = y_tr[-f],
alpha = 1,
lambda = my_lam)
# Predictions
pred_this <- predict(lf_this,
newx = as.matrix(x_tr_s[f, ]),
s = my_lam)
# CV scores (in terms of MSE)
cv_scores_this <- colMeans((pred_this - y_tr[f]) ^ 2)
# For this fold, we get a distinct MSE for each set of
# predictions returned by using a lambda from my_lam.
return(cv_scores_this)
})
cv_scores
# Plot CV scores against lambda for each fold
matplot(x = my_lam,
y = cv_scores,
type = "l",
lty = 1,
ylab = "Error",
xlab = expression(lambda))
legend("topleft", paste('Fold', 1:nfolds), col = 1:nfolds, lty = 1, bty = "n")
# Improve readability
matplot(x = log(my_lam),
y = cv_scores,
type = "l",
lty = 1,
ylab = "Error",
xlab = expression(log(lambda)))
legend("topleft", paste('Fold', 1:nfolds), col = 1:nfolds, lty = 1, bty = "n")
# Average CV scores over the folds
rowMeans(cv_scores)
plot(x = my_lam,
y = rowMeans(cv_scores),
type = "l",
lty = 1,
ylab = "Error",
xlab = expression(lambda))
# using the logarithm
plot(x = log(my_lam),
y = rowMeans(cv_scores),
type = "l",
lty = 1,
ylab = "Error",
xlab = expression(log(lambda)))
# Lambda is typically chosen as the value minimizing the average MSE over the
# folds.
(lambda_star <- my_lam[which.min(rowMeans(cv_scores))])
abline(v = log(lambda_star), col = "red")
# Automatic CV
lasso_cv <- cv.glmnet(x = as.matrix(x_tr_s),
y = y_tr,
lambda = my_lam,
alpha = 1,
family = "gaussian")
# Plot the CV score against lambda
plot(lasso_cv$lambda,
lasso_cv$cvm,
type = "l",
lty = 1,
ylab = "Error",
xlab = expression(lambda))
plot(log(lasso_cv$lambda),
lasso_cv$cvm,
type = "l",
lty = 1,
ylab = "Error",
xlab = expression(log(lambda)))
# Directly plot the object returned by cv.glmnet()
plot(lasso_cv)
# The bands represent the upper and lower standard deviation curves.
?plot.cv.glmnet
# Best lambda (minimizing the average MSE over the folds)
lasso_cv$lambda.min
# Preferred best value: largest value of lambda such that error is within 1
# standard error of the minimum.
lasso_cv$lambda.1se
# In such a way, the regularization is stronger, but we are not so far from the
# global minimum.
(lambda_star <- lasso_cv$lambda.1se)
coef(lasso_cv,
s = lambda_star)
# Estimated MSE for the chosen lambda
lasso_cv$cvm[which(lasso_cv$lambda == lambda_star)]
# Predictions on the test set using the best lambda
pred_test <- predict(lasso_cv,
newx = as.matrix(x_te_s),
s = lambda_star)
# Error on the test set
mean((pred_test - y_te) ^ 2)
# CV with 5 folds overestimate a little bit the true test error...
# more folds?
library(boot)
library(glmnet)
n = nrow(Auto)
library(ISLR)
# load data auto
data(Auto)
n = nrow(Auto)
data = Auto[1:(n-1), ]
X_tr = scale(model.matrix(mpg~.-name-origin, data=data)[,-1])
X_te = (model.matrix(mpg~.-name-origin, data=Auto[n,])[,-1] - attr(X_tr,"scaled:center"))/attr(X_tr,"scaled:scale")
y_tr = data$mpg
y_te = Auto$mpg[n]
set.seed(1)
n
# Load necessary libraries
library(ISLR)
library(glmnet)
# Load the Auto dataset
data(Auto)
# Drop the 'name' and 'origin' variables
Auto <- subset(Auto, select = -c(name, origin))
# Set the seed for reproducibility
set.seed(1)
# Scale the numerical features (excluding the response variable 'mpg')
# Assuming all other variables except 'mpg' are numerical
predictors <- scale(Auto[, -which(names(Auto) == "mpg")])
# Prepare the response variable
mpg <- Auto$mpg
# Fit lasso regression with cross-validation to choose the optimal lambda
cv.lasso <- cv.glmnet(predictors, mpg, alpha = 1)
# Find the optimal lambda value
optimal_lambda <- cv.lasso$lambda.min
# Print the optimal lambda value
print(optimal_lambda)
# Fit the lasso model using the optimal lambda
lasso.model <- glmnet(predictors, mpg, alpha = 1, lambda = optimal_lambda)
# Determine the number of non-zero coefficients (non-discarded features)
non_zero_coef <- sum(coef(lasso.model) != 0)
# Print the number of non-zero coefficients
print(non_zero_coef)
# The number of discarded features is the total number of features minus the non-zero coefficients
discarded_features <- ncol(predictors) - non_zero_coef
# Print the number of discarded features
print(discarded_features)
setwd("C:/Users/alexa/OneDrive/Desktop/Telecom-Churn")
setwd("C:/Users/alexa/OneDrive/Desktop/Telecom-Churn")
library(ggplot2)
library(corrplot)
library(dplyr)
library(tidyr)
data = read.csv("TelecomChurn.csv")
dim(data) # 3333 rows and 20 columns
head(data)
str(data)
sum(is.na(data)) # no missing values
summary(data)
# Count of Distinct Features
for (col in names(data)) {
cat("Distinct values in", col, ":\n")
print(unique(data[[col]]))
cat("\n")
}
anyDuplicated(data) # no duplicates
# converting characters to factor
data$State = as.factor(data$State)
data$International.plan = as.factor(data$International.plan)
data$Voice.mail.plan = as.factor(data$Voice.mail.plan)
data$Churn = as.factor(data$Churn)
# checking the distribution of our target
# We will first calculate the proportions of Churn
churn_percent <- data %>%
group_by(Churn) %>%
summarise(Count = n(), .groups = "drop") %>%
mutate(Percent = Count / sum(Count) * 100)
ggplot(churn_percent, aes(x = "", y = Percent, fill = Churn)) +
geom_bar(stat = "identity", width = 1) +
coord_polar("y", start = 0) +
theme_void() +
scale_fill_manual(values = c("False" = "violet", "True" = "skyblue"), labels = c("False" = "No", "True" = "Yes")) +
labs(title = "Percentage of Churn", fill = "Churn") +
geom_text(aes(label = paste0(round(Percent, 1), "%")), position = position_stack(vjust = 0.5)) +
theme(plot.title = element_text(hjust = 0.5))
continuous_vars <- data[, sapply(data, is.numeric)]
categorical_vars <- data[, sapply(data, is.factor) & !names(data) %in% "Churn"]
# univariate analysis for categorical variables (State, International.plan, Voice.mail.plan)
long_data <- categorical_vars %>%
pivot_longer(cols = everything(), names_to = "Category", values_to = "Value")
ggplot(long_data, aes(x = Value)) +
geom_bar(fill = "skyblue", color = "black") +
facet_wrap(~ Category, scales = "free_x", nrow = 2, ncol = 2) +
labs(title = "Distribution of Categorical Variables", x = NULL, y = "Count") +
theme_minimal() +
theme(
strip.text = element_text(face = "bold", size = 12),
axis.text.x = element_text(angle = 90, hjust = 1, size = 10),
plot.title = element_text(hjust = 0.5, size = 16),
legend.position = "none"
)
# voice mail and international plan are imbalanced
# also our target variable is imbalanced
# univariate analysis for numerical variables
### KDE plots
continuous <- data[, sapply(data, is.numeric) | names(data) == "Churn"]
continuous %>%
pivot_longer(cols = -Churn, names_to = "metric", values_to = "value") %>%
ggplot(aes(value, color = as.factor(Churn))) +
geom_density(alpha = 0.3) +
facet_wrap(vars(metric), scales = "free") +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
scale_color_discrete(name = "Churn Status", labels = c("No", "Yes")) +
labs(title = "Numeric Features Univariate Distribution by Churn Status") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5),
axis.text.x = element_text(angle = 45, hjust = 1),
axis.title.x = element_text(face = "bold"),
axis.title.y = element_text(face = "bold")
)
#There's a noticeable difference in the distribution of customer service calls
# between those who churned and those who did not.
# Customers who churned seem to make more customer service calls, which could
# indicate a relationship between customer satisfaction and churn.
# It appears that customers who churned tend to have slightly higher day minutes
# and charges. This might suggest that customers with higher usage during the day are more likely to churn.
# something similar with total.day.charge
# the others follow more or less the same trend
# analysis of churn and customer service calls
data %>%
group_by(Customer.service.calls, Churn) %>%
summarise(Count = n(), .groups = "drop") %>%
ggplot(aes(x = Customer.service.calls, y = Count, color = Churn)) +
geom_line() +
geom_point() +
labs(title = "Churn by Customer Service Calls", x = "Customer Service Calls", y = "Count", color = "Churn") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5))
# customers who churned made more customer service calls
# let's double check it with hypothesis testing
# H0: the average number of customer service calls is the same for customers who churned and those who did not
# H1: the average number of customer service calls is different for customers who churned and those who did not
# chi square
chisq.test(data$Customer.service.calls, data$Churn)
# p-value < 0.05, we reject the null hypothesis therefore the average number of customer service calls is different for customers who churned and those who did not
# now hypo for: Average Charges for Churners vs. Non-Churners
# H0: the average charges are the same for customers who churned and those who did not
# H1: the average charges are different for customers who churned and those who did not
# t-test
t.test(data$Total.day.charge ~ data$Churn)
# p-value < 0.05, we reject the null hypothesis therefore the average charges are different for customers who churned and those who did not
# hypo for Voice Mail Plan and Churn:
# H0: the average churn rate is the same for customers who have a voice mail plan and those who do not
# H1: the average churn rate is different for customers who have a voice mail plan and those who do not
# chi square
chisq.test(data$Voice.mail.plan, data$Churn)
# p-value = 5.151e-09 < 0.05, we reject the null hypothesis therefore the average churn rate is different for customers who have a voice mail plan and those who do not
# analysis of churn and day minutes and day charge
# customers who churned have higher total day minutes
# customers who churned have higher total day charge
long_data <- data %>%
select(Churn, Total.day.minutes, Total.day.charge) %>%
pivot_longer(cols = c(Total.day.minutes, Total.day.charge),
names_to = "Measure",
values_to = "Value")
ggplot(long_data, aes(x = Churn, y = Value, fill = Churn)) +
geom_boxplot() +
facet_wrap(~ Measure, scales = "free_y") +
labs(title = "Churn by Total Day Minutes and Total Day Charge",
x = "Churn",
y = "Value",
fill = "Churn") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5),
strip.background = element_blank(),
strip.text.x = element_text(size = 12)
)
# churn by international plan
data %>%
group_by(International.plan, Churn) %>%
summarise(Count = n(), .groups = "drop") %>%
ggplot(aes(x = International.plan, y = Count, fill = Churn)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
geom_text(aes(label = Count),
position = position_dodge(width = 0.9),
vjust = -0.5,
size = 3.5) +
labs(title = "Churn by International Plan", x = "International Plan", y = "Count", fill = "Churn") +
theme_minimal()
# we see that almost all customers that have an international plan churned
